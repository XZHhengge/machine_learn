{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.metrics import average_precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Settings:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 299529\n",
    "        self.num_steps = 70\n",
    "        self.num_epochs = 300\n",
    "        self.num_classes = 12\n",
    "        self.gru_size = 340\n",
    "        self.keep_prob = 0.5\n",
    "        self.num_layers = 1\n",
    "        self.pos_size = 1\n",
    "        self.pos_num = 141\n",
    "        self.big_num = 50\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    def __init__(self, is_training, word_embeddings, settings):\n",
    "\n",
    "        self.num_steps = num_steps = settings.num_steps\n",
    "        self.vocab_size = vocab_size = settings.vocab_size\n",
    "        self.num_classes = num_classes = settings.num_classes\n",
    "        self.gru_size = settings.gru_size\n",
    "        self.big_num = big_num = settings.big_num\n",
    "        \n",
    "        self.input_word = tf.placeholder(dtype=tf.int32, shape=[None, num_steps], name='input_wod')\n",
    "        self.input_pos1 = tf.placeholder(dtype=tf.int32, shape=[None, num_steps], name='input_pos1')\n",
    "        self.input_pos2 = tf.placeholder(dtype=tf.int32, shape=[None, num_steps], name='input_pos2')\n",
    "        self.input_y = tf.placeholder(dtype=tf.float32, shape=[None, num_classes], name='input_y')\n",
    "        self.total_shape = tf.placeholder(dtype=tf.int32, shape=[big_num + 1], name='total_shape')\n",
    "        total_num = self.total_shape[-1]\n",
    "        \n",
    "        word_embedding = tf.get_variable(initializer=word_embeddings, name='word_embedding')\n",
    "        pos1_embedding = tf.get_variable('pos1_embedding', [settings.pos_num, settings.pos_size])\n",
    "        pos2_embedding = tf.get_variable('pos2_embedding', [settings.pos_num, settings.pos_size])\n",
    "        \n",
    "        atteintion_w = tf.get_variable('attention_omega', [self.gru_size, 1])\n",
    "        sen_a = tf.get_variable('attention_A', [self.gru_size])\n",
    "        sen_r = tf.get_variable('query_r', [self.gru_size, 1])\n",
    "        relation_embedding = tf.get_variable('relation_embedding', [self.num_classes, self.gru_size])\n",
    "        sen_d = tf.get_variable('bias_d', [self.num_classes])\n",
    "\n",
    "        gru_cell_forward = tf.contrib.rnn.GRUCell(self.gru_size)\n",
    "        gru_cell_backward = tf.contrib.rnn.GRUCell(self.gru_size)\n",
    "        # settings.keep_prob使用dropout降低过拟合的问题\n",
    "        if is_training and settings.keep_prob < 1:\n",
    "            gru_cell_forward = tf.contrib.rnn.DropoutWrapper(gru_cell_forward, output_keep_prob=settings.keep_prob)\n",
    "            gru_cell_backward = tf.contrib.rnn.DropoutWrapper(gru_cell_backward, output_keep_prob=settings.keep_prob)\n",
    "\n",
    "        cell_forward = tf.contrib.rnn.MultiRNNCell([gru_cell_forward] * settings.num_layers)\n",
    "        cell_backward = tf.contrib.rnn.MultiRNNCell([gru_cell_backward] * settings.num_layers)\n",
    "\n",
    "        sen_repre = []\n",
    "        sen_alpha = []\n",
    "        sen_s = []\n",
    "        sen_out = []\n",
    "        self.prob = []\n",
    "        self.predictions = []\n",
    "        self.loss = []\n",
    "        self.accuracy = []\n",
    "        self.total_loss = 0.0\n",
    "\n",
    "        self._initial_state_forward = cell_forward.zero_state(total_num, tf.float32)\n",
    "        self._initial_state_backward = cell_backward.zero_state(total_num, tf.float32)\n",
    "\n",
    "        # embedding layer\n",
    "        # 实际上tf.nn.embedding_lookup的作用就是找到要寻找的embedding data中的对应的行下的vector。\n",
    "        # 輸入的shape=[句子数，字数，200词向量+实体1位置编码+实体2位置编码]\n",
    "        inputs_forward = tf.concat(axis=2, values=[tf.nn.embedding_lookup(word_embedding, self.input_word),\n",
    "                                                   tf.nn.embedding_lookup(pos1_embedding, self.input_pos1),\n",
    "                                                   tf.nn.embedding_lookup(pos2_embedding, self.input_pos2)])\n",
    "        inputs_backward = tf.concat(axis=2,\n",
    "                                    values=[tf.nn.embedding_lookup(word_embedding, tf.reverse(self.input_word, [1])),\n",
    "                                            tf.nn.embedding_lookup(pos1_embedding, tf.reverse(self.input_pos1, [1])),\n",
    "                                            tf.nn.embedding_lookup(pos2_embedding, tf.reverse(self.input_pos2, [1]))])\n",
    "\n",
    "        outputs_forward = []\n",
    "\n",
    "        state_forward = self._initial_state_forward\n",
    "\n",
    "        # Bi-GRU layer\n",
    "        with tf.variable_scope('GRU_FORWARD') as scope:\n",
    "            for step in range(num_steps):\n",
    "                if step > 0:\n",
    "                    scope.reuse_variables()\n",
    "                (cell_output_forward, state_forward) = cell_forward(inputs_forward[:, step, :], state_forward)\n",
    "                outputs_forward.append(cell_output_forward)\n",
    "\n",
    "        outputs_backward = []\n",
    "\n",
    "        state_backward = self._initial_state_backward\n",
    "        with tf.variable_scope('GRU_BACKWARD') as scope:\n",
    "            for step in range(num_steps):\n",
    "                if step > 0:\n",
    "                    scope.reuse_variables()\n",
    "                (cell_output_backward, state_backward) = cell_backward(inputs_backward[:, step, :], state_backward)\n",
    "                outputs_backward.append(cell_output_backward)\n",
    "        # tf.concat(axis=1,  按列拼接\n",
    "        output_forward = tf.reshape(tf.concat(axis=1, values=outputs_forward), [total_num, num_steps, self.gru_size])\n",
    "        output_backward = tf.reverse(\n",
    "            tf.reshape(tf.concat(axis=1, values=outputs_backward), [total_num, num_steps, self.gru_size]),\n",
    "            [1])\n",
    "\n",
    "        output_h = tf.add(output_forward, output_backward)\n",
    "        output_h = tf.reshape(tf.tanh(output_h), [total_num * num_steps, self.gru_size])\n",
    "        \n",
    "        # sentence-level attention layer\n",
    "        for i in range(big_num):\n",
    "            sen_input_1 = output_h[self.total_shape[i]:self.total_shape[i + 1]]\n",
    "            sen_input_2 = tf.tanh(sen_input_1)\n",
    "            sen_repre.append(sen_input_2)\n",
    "\n",
    "            batch_size = self.total_shape[i + 1] - self.total_shape[i]\n",
    "\n",
    "            sen_alpha_1 = tf.matmul(sen_input_2, sen_r)\n",
    "            sen_alpha_2 = tf.reshape(sen_alpha_1, [batch_size])\n",
    "            sen_alpha_3 = tf.nn.softmax(sen_alpha_2)\n",
    "            sen_alpha_4 = tf.reshape(sen_alpha_3, [1, batch_size])\n",
    "            sen_alpha.append(sen_alpha_4)\n",
    "            # sen_alpha.append(\n",
    "            #     tf.reshape(tf.nn.softmax(tf.reshape(tf.matmul(tf.multiply(sen_repre[i], sen_a), sen_r), [batch_size])),\n",
    "            #                [1, batch_size]))\n",
    "\n",
    "            sen_s_1 = tf.matmul(sen_alpha[i], sen_repre[i])\n",
    "            sen_s_2 = tf.reshape(sen_s_1, [self.gru_size, 1])\n",
    "            sen_s.append(sen_s_2)\n",
    "            # sen_s.append(tf.reshape(tf.matmul(sen_alpha[i], sen_repre[i]), [self.gru_size, 1]))\n",
    "\n",
    "            sen_out_1 = tf.matmul(relation_embedding, sen_s[i])\n",
    "            sen_out_2 = tf.reshape(sen_out_1, [self.num_classes])\n",
    "            sen_out_3 = tf.add(sen_out_2, sen_d)\n",
    "            sen_out.append(sen_out_3)\n",
    "            # sen_out.append(tf.add(tf.reshape(tf.matmul(relation_embedding, sen_s[i]), [self.num_classes]), sen_d))\n",
    "\n",
    "            self.prob.append(tf.nn.softmax(sen_out[i]))\n",
    "\n",
    "            with tf.name_scope(\"output\"):\n",
    "                # tf.argmax 最大值的索引值\n",
    "                self.predictions.append(tf.argmax(self.prob[i], 0, name=\"predictions\"))\n",
    "\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                # sen_out[i] 输出结果，input_y[i]实际的分类标签\n",
    "                self.loss.append(\n",
    "                    tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=sen_out[i], labels=self.input_y[i])))\n",
    "                if i == 0:\n",
    "                    self.total_loss = self.loss[i]\n",
    "                else:\n",
    "                    self.total_loss += self.loss[i]\n",
    "\n",
    "            with tf.name_scope(\"accuracy\"):\n",
    "                self.accuracy.append(\n",
    "                    tf.reduce_mean(tf.cast(tf.equal(self.predictions[i], tf.argmax(self.input_y[i], 0)), \"float\"),\n",
    "                                   name=\"accuracy\"))\n",
    "\n",
    "        # tf.summary.scalar('loss',self.total_loss)\n",
    "        tf.summary.scalar('loss', self.total_loss)\n",
    "        # regularization 在损失函数上加上正则项是防止过拟合的一个重要方法\n",
    "        self.l2_loss = tf.contrib.layers.apply_regularization(regularizer=tf.contrib.layers.l2_regularizer(0.0001),\n",
    "                                                              weights_list=tf.trainable_variables())\n",
    "        \n",
    "        self.final_loss = self.total_loss + self.l2_loss\n",
    "        tf.summary.scalar('l2_loss', self.l2_loss)\n",
    "        tf.summary.scalar('final_loss', self.final_loss)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def main():\n",
    "    # the path to save models\n",
    "    save_path = './实体关系/jupyter_BGRU_2ATT/model/'\n",
    "\n",
    "    print('reading wordembedding')\n",
    "    wordembedding = np.load('./实体关系/jupyter_BGRU_2ATT/data/vec.npy')\n",
    "\n",
    "    print('reading training data')\n",
    "    train_y = np.load('./实体关系/jupyter_BGRU_2ATT/data/train_y.npy')\n",
    "    train_word = np.load('./实体关系/jupyter_BGRU_2ATT/data/train_word.npy')\n",
    "    train_pos1 = np.load('./实体关系/jupyter_BGRU_2ATT/data/train_pos1.npy')\n",
    "    train_pos2 = np.load('./实体关系/jupyter_BGRU_2ATT/data/train_pos2.npy')\n",
    "\n",
    "    settings = Settings()\n",
    "    settings.vocab_size = len(wordembedding)\n",
    "    settings.num_classes = len(train_y[0])\n",
    "\n",
    "    big_num = settings.big_num\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            # 初始化的权重参数\n",
    "            initializer = tf.contrib.layers.xavier_initializer()\n",
    "            with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "                m = GRU(is_training=True, word_embeddings=wordembedding, settings=settings)\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(0.0005)\n",
    "\n",
    "            train_op = optimizer.minimize(m.final_loss, global_step=global_step)\n",
    "            # 初始化模型的参数\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver(max_to_keep=None)\n",
    "\n",
    "            merged_summary = tf.summary.merge_all()\n",
    "            summary_writer = tf.summary.FileWriter('./train_loss', graph=sess.graph)\n",
    "\n",
    "            def train_step(word_batch, pos1_batch, pos2_batch, y_batch, big_num):\n",
    "\n",
    "                feed_dict = {}\n",
    "                total_shape = []\n",
    "                total_num = 0\n",
    "                total_word = []\n",
    "                total_pos1 = []\n",
    "                total_pos2 = []\n",
    "                for i in range(len(word_batch)):\n",
    "                    total_shape.append(total_num)\n",
    "                    total_num += len(word_batch[i])\n",
    "                    for word in word_batch[i]:\n",
    "                        total_word.append(word)\n",
    "                    for pos1 in pos1_batch[i]:\n",
    "                        total_pos1.append(pos1)\n",
    "                    for pos2 in pos2_batch[i]:\n",
    "                        total_pos2.append(pos2)\n",
    "                total_shape.append(total_num)\n",
    "                total_shape = np.array(total_shape)\n",
    "                total_word = np.array(total_word)\n",
    "                total_pos1 = np.array(total_pos1)\n",
    "                total_pos2 = np.array(total_pos2)\n",
    "\n",
    "                feed_dict[m.total_shape] = total_shape\n",
    "                feed_dict[m.input_word] = total_word\n",
    "                feed_dict[m.input_pos1] = total_pos1\n",
    "                feed_dict[m.input_pos2] = total_pos2\n",
    "                feed_dict[m.input_y] = y_batch\n",
    "\n",
    "                temp, step, loss, accuracy, summary, l2_loss, final_loss = sess.run(\n",
    "                    [train_op, global_step, m.total_loss, m.accuracy, merged_summary, m.l2_loss, m.final_loss],feed_dict)\n",
    "                # accuracy为50个句子能够正确预测的数据\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                accuracy = np.reshape(np.array(accuracy), (big_num))\n",
    "                acc = np.mean(accuracy)\n",
    "                summary_writer.add_summary(summary, step)\n",
    "\n",
    "                if step % 50 == 0:\n",
    "                    tempstr = \"{}: step {}, softmax_loss {:g}, acc {:g}\".format(time_str, step, loss, acc)\n",
    "                    print(tempstr)\n",
    "\n",
    "            for one_epoch in range(settings.num_epochs):\n",
    "                temp_order = list(range(len(train_word)))\n",
    "                np.random.shuffle(temp_order)\n",
    "                for i in range(int(len(temp_order) / float(settings.big_num))):\n",
    "\n",
    "                    temp_word = []\n",
    "                    temp_pos1 = []\n",
    "                    temp_pos2 = []\n",
    "                    temp_y = []\n",
    "\n",
    "                    temp_input = temp_order[i * settings.big_num:(i + 1) * settings.big_num]\n",
    "                    for k in temp_input:\n",
    "                        temp_word.append(train_word[k])\n",
    "                        temp_pos1.append(train_pos1[k])\n",
    "                        temp_pos2.append(train_pos2[k])\n",
    "                        temp_y.append(train_y[k])\n",
    "                    num = 0\n",
    "                    for single_word in temp_word:\n",
    "                        num += len(single_word)\n",
    "                        if len(single_word) != 1:\n",
    "                            test = single_word\n",
    "\n",
    "                    if num > 1500:\n",
    "                        print('out of range')\n",
    "                        continue\n",
    "\n",
    "                    temp_word = np.array(temp_word)\n",
    "                    temp_pos1 = np.array(temp_pos1)\n",
    "                    temp_pos2 = np.array(temp_pos2)\n",
    "                    temp_y = np.array(temp_y)\n",
    "\n",
    "                    train_step(temp_word, temp_pos1, temp_pos2, temp_y, settings.big_num)\n",
    "\n",
    "                    current_step = tf.train.global_step(sess, global_step)\n",
    "                    if current_step > 200 and current_step % 100 == 0:\n",
    "                        print('saving model')\n",
    "                        path = saver.save(sess, save_path + 'GRU_new_cw2vec_ATT_GRU_model', global_step=current_step)\n",
    "                        tempstr = 'have saved model to ' + path\n",
    "                        print(tempstr)\n",
    "                        return 0\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "reading wordembedding\n",
      "reading training data\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-3-db78e5889c30>:28: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-3-db78e5889c30>:35: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-3-db78e5889c30>:129: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "2019-12-12T21:41:23.054946: step 50, softmax_loss 102.297, acc 0.34\n",
      "2019-12-12T21:45:51.352347: step 100, softmax_loss 118.591, acc 0.22\n",
      "2019-12-12T21:50:22.578549: step 150, softmax_loss 101.286, acc 0.24\n",
      "2019-12-12T21:54:42.767551: step 200, softmax_loss 110.657, acc 0.2\n",
      "2019-12-12T21:59:16.116752: step 250, softmax_loss 107.897, acc 0.28\n",
      "2019-12-12T22:03:45.250154: step 300, softmax_loss 110.048, acc 0.2\n",
      "saving model\n",
      "INFO:tensorflow:./实体关系/jupyter_BGRU_2ATT/model/GRU_new_cw2vec_ATT_GRU_model-300 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "have saved model to ./实体关系/jupyter_BGRU_2ATT/model/GRU_new_cw2vec_ATT_GRU_model-300\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 7
    }
   ],
   "source": [
    "main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
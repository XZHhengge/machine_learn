{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-f19a686242c2>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease write your own downloading logic.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\n",
      "Extracting .\\deep_learn\\text2image\\MNIST_data\\train-images-idx3-ubyte.gz\nWARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\n",
      "Extracting .\\deep_learn\\text2image\\MNIST_data\\train-labels-idx1-ubyte.gz\nWARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.one_hot on tensors.\n",
      "Extracting .\\deep_learn\\text2image\\MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting .\\deep_learn\\text2image\\MNIST_data\\t10k-labels-idx1-ubyte.gz\nWARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\nInstructions for updating:\nUse `tf.global_variables_initializer` instead.\n",
      "0.168\n0.408",
      "\n0.461\n0.659\n0.622\n0.6412\n0.7128\n0.7227\n0.6788",
      "\n0.753",
      "\n0.7299",
      "\n0.8011\n0.7968\n0.7788\n0.759\n0.8118\n0.7711",
      "\n0.7895",
      "\n0.8158",
      "\n0.838\n0.8049\n0.8249\n0.8122\n0.8232\n0.8201",
      "\n0.8079",
      "\n0.8138",
      "\n0.8336\n0.8392\n0.8383\n0.8431\n0.8127\n0.8381",
      "\n0.8536",
      "\n0.8376\n0.7906\n0.7195\n0.8319\n0.8467\n0.8582",
      "\n0.8607",
      "\n0.8402\n0.8608\n0.8635\n0.8391\n0.8429\n0.8686\n0.8717",
      "\n0.8624",
      "\n0.8605\n0.8607\n0.8694\n0.8755\n0.8712\n0.8427\n0.8439",
      "\n0.8402",
      "\n0.8621\n0.851\n0.8661\n0.8585\n0.8637\n0.8628\n0.8563",
      "\n0.8249",
      "\n0.857\n0.8812\n0.8694\n0.8757\n0.8737\n0.8791\n0.8788",
      "\n0.8774",
      "\n0.878\n0.8641\n0.8781\n0.8848\n0.8831\n0.8803\n0.8806",
      "\n0.8734\n0.8678\n0.867\n0.8679\n0.848\n0.8756\n0.8764\n0.8615",
      "\n0.8631\n0.8757\n0.8597\n0.8745\n0.8808\n0.89\n0.8686\n0.8689",
      "\n0.8784\n0.8795\n0.878\n0.8722\n0.879\n0.8868\n0.8817",
      "\n0.8871",
      "\n0.8909\n0.8725\n0.8658",
      "\n0.8869",
      "\n0.8918\n0.8854\n0.8783\n0.8635\n0.8818\n0.8849",
      "\n0.8932",
      "\n0.8696\n0.849\n0.8913\n0.8934\n0.8753\n0.8777",
      "\n0.8898",
      "\n0.8689\n0.8795\n0.8868\n0.8871\n0.8879\n0.8892",
      "\n0.8744",
      "\n0.8898\n0.8862\n0.8836\n0.8912\n0.8926\n",
      "0.8894\n",
      "0.8994\n0.8841\n0.8947\n0.8829\n0.8965\n0.893\n0.8962",
      "\n",
      "0.8858\n0.8946\n0.8883\n0.896\n0.8852\n0.8801",
      "\n0.89\n",
      "0.8825\n0.8979\n0.8855\n0.8902\n0.8837\n",
      "0.8882\n0.8939\n",
      "0.8852\n0.8882\n0.8831\n0.8769\n0.8862",
      "\n0.8935\n0.8936",
      "\n0.8911\n0.882\n0.8873\n0.8953",
      "\n0.8954\n",
      "0.891\n0.8954\n0.8978\n0.8895\n",
      "0.8913\n0.8778\n0.9016",
      "\n0.8961\n0.8994\n0.8948\n0.8946",
      "\n0.8991\n",
      "0.8857\n0.8994\n0.8885\n0.9015\n0.8948",
      "\n0.9005\n",
      "0.9013\n0.8901\n0.8936\n0.8964\n0.8921",
      "\n0.8855\n",
      "0.8992\n0.8944\n0.8894\n0.9005\n0.8949\n",
      "0.8982\n",
      "0.8963\n0.8988\n0.8975\n0.9008\n0.8951\n",
      "0.8743",
      "\n0.8982\n0.8959\n0.8936\n0.8894\n0.9028\n",
      "0.8897\n0.9011\n0.892\n0.9016\n0.9004\n0.9012\n",
      "0.8896\n0.8989\n0.8825\n0.9019\n0.8994\n0.9052\n",
      "0.8818\n0.8942\n0.903\n0.9017\n0.9057\n0.8962",
      "\n0.8764\n0.8928\n0.9023\n0.8952\n0.8956",
      "\n0.8956\n0.8997\n0.9048\n0.903\n0.8742",
      "\n0.8939\n0.902\n0.9012\n0.9003\n0.8981",
      "\n0.8965\n0.8911\n0.9004\n0.8987\n0.8955",
      "\n0.896\n0.8975\n0.8955\n0.8876\n0.898",
      "\n0.8972\n0.9023\n0.9023\n0.8987\n0.8919",
      "\n0.9003\n0.9012\n0.9015\n0.9002\n0.9032",
      "\n0.9049\n0.8997\n0.9027\n0.8977\n0.8972",
      "\n0.9044\n0.9015\n0.9007\n0.8994\n0.8987",
      "\n0.902\n0.9034\n0.9008\n0.9054\n0.9048",
      "\n0.8977\n0.9009\n0.8911\n0.8942\n",
      "0.9003\n0.8935\n0.906\n0.9023\n0.9002\n0.9011",
      "\n0.9062\n0.906\n0.8989\n0.8979\n",
      "0.9002\n0.8945\n0.8945\n0.9016\n0.9068\n",
      "0.907\n0.9073\n0.9027\n0.903\n0.9078\n",
      "0.9043\n0.9014\n0.9073\n0.9089\n0.8956\n",
      "0.9068\n0.9058\n0.9093\n0.9045\n0.9026\n",
      "0.8988\n0.8968\n0.901\n0.9059\n0.9038",
      "\n0.9074\n0.9039\n0.8997\n0.897\n",
      "0.907\n0.9005\n0.906\n0.9051\n0.9032",
      "\n0.9008\n0.8942\n0.9052\n0.9058",
      "\n0.9042\n0.905\n0.9032\n0.897",
      "\n0.9046\n0.9093\n0.8994\n0.9042",
      "\n0.9009\n0.9078\n0.8984\n",
      "0.9059\n0.9026\n0.9065\n0.9068\n0.8774",
      "\n0.8927\n0.9051\n0.8968\n0.9024",
      "\n0.9039\n0.9021\n0.8926\n0.9056",
      "\n0.9019\n0.905\n0.8956\n",
      "0.9006\n0.8997\n0.8952\n0.9031\n",
      "0.9015\n0.9024\n0.9029\n0.9082\n",
      "0.9065\n0.891\n0.9055\n0.9033\n",
      "0.9052\n0.8998\n0.9041\n0.9058\n",
      "0.9026\n0.9009\n0.8978\n0.9056\n",
      "0.8979\n0.8879\n0.8964\n0.8853\n",
      "0.8966\n0.8881\n0.9008\n0.8977\n0.9021",
      "\n0.9008\n0.8981\n0.9098\n",
      "0.9086\n0.9096\n0.9065\n0.9087\n",
      "0.903\n0.9021\n0.9099\n0.9056\n",
      "0.9079\n0.9096\n0.9058\n0.909\n",
      "0.8924\n0.9081\n0.9078\n0.9045\n",
      "0.9099\n0.9063\n0.8842\n0.9024\n",
      "0.9079\n0.9057\n0.9038\n0.9051\n",
      "0.8862\n0.8936\n0.9011\n0.9002\n",
      "0.9078\n0.9062\n0.9052\n0.9022\n",
      "0.9061\n0.9016\n0.9082\n0.8991\n",
      "0.9066\n0.8984\n0.9087\n0.9078\n",
      "0.9057\n0.9052\n0.9068\n0.9071\n",
      "0.9067\n0.901\n0.9046\n0.9091\n",
      "0.9099\n0.9108\n0.9093\n0.9053",
      "\n0.9067\n0.9096\n0.9069\n",
      "0.9071\n0.9124\n0.9067\n0.9124\n",
      "0.9099\n0.9012\n0.9042\n0.9093\n",
      "0.8971\n0.9039\n0.9133\n0.909\n",
      "0.9079\n0.9044\n0.9088\n0.9064",
      "\n0.9059\n0.9031\n0.9033\n",
      "0.9071\n0.9106\n0.9089\n0.9067",
      "\n0.9086\n0.9114\n0.908",
      "\n0.911\n0.9104\n0.9095",
      "\n0.9107\n0.9066\n0.9068",
      "\n0.9004\n0.896\n0.9066",
      "\n0.9073\n0.9102\n0.9105\n",
      "0.9019\n0.9024\n0.9074\n0.9125",
      "\n0.9118\n0.9061\n0.9084",
      "\n0.9049\n0.9053\n0.9078",
      "\n0.9047\n0.9121\n0.9073",
      "\n0.9061\n0.9123\n0.9084",
      "\n0.9066\n0.9051\n0.9047",
      "\n0.8979\n0.8984\n",
      "0.9022\n0.9052\n0.9085\n0.908",
      "\n0.9024\n0.9082\n",
      "0.9067\n0.909\n0.903\n0.9062",
      "\n0.8965\n0.9097\n0.9095",
      "\n0.9073\n0.9004\n",
      "0.9025\n0.9091\n0.9086\n0.9121",
      "\n0.8985\n0.9031\n0.9095",
      "\n0.9114\n0.9037\n",
      "0.9036\n0.905\n0.9101\n",
      "0.9016\n0.9037\n0.9079\n",
      "0.9078\n0.9098\n0.9074\n",
      "0.9095\n0.9099\n0.9111\n",
      "0.9021\n0.9059\n0.9046\n",
      "0.9002\n0.9051\n0.8967\n0.9014",
      "\n0.9089\n0.9036\n",
      "0.8929\n0.8911\n0.897\n",
      "0.9123\n0.9053\n0.9126\n",
      "0.9088\n0.9127\n0.9102\n",
      "0.9113\n0.9136\n0.9126\n",
      "0.9115\n0.9012\n0.9118\n",
      "0.9104\n0.914\n0.9036\n",
      "0.9049\n0.9026\n0.9002\n",
      "0.8912\n0.8973\n0.9089\n",
      "0.9095\n0.9066\n0.9113\n",
      "0.909\n0.9077\n0.9102\n",
      "0.9113\n0.9109\n0.9041\n",
      "0.9103\n0.9086\n0.8965\n",
      "0.9043\n0.9059\n0.9045\n",
      "0.9086\n0.9052\n0.8991\n",
      "0.8952\n0.8952\n0.8983\n",
      "0.9024\n0.9045\n0.9044\n",
      "0.9094\n0.9074\n0.9103\n",
      "0.9143\n0.9039\n0.9111\n",
      "0.9122\n0.9108\n0.9083\n",
      "0.9144\n0.9119\n0.9132\n",
      "0.91\n0.9112\n0.9146\n",
      "0.9145\n0.9116\n0.9174\n",
      "0.9138\n0.912\n0.9154\n",
      "0.9151\n0.9094\n0.9104\n",
      "0.9076\n0.9126\n0.9076\n",
      "0.9152\n0.9154\n0.9154\n",
      "0.9153\n0.9154\n0.9139\n",
      "0.9105\n0.9086\n0.9122\n",
      "0.914\n0.9153\n0.917\n",
      "0.9085\n0.908\n0.9087\n",
      "0.9117\n0.9136\n0.9082\n",
      "0.9079\n0.9108\n0.9099\n",
      "0.9068\n0.9089\n0.9136\n",
      "0.9042\n0.9116\n0.9052\n",
      "0.9144\n0.9159\n0.9133\n",
      "0.9139\n0.9156\n0.9154\n",
      "0.9167\n0.9151\n0.9143\n",
      "0.9121\n0.9161\n0.9172\n",
      "0.9116\n0.9161\n0.9112\n",
      "0.9089\n0.9071\n0.9001\n",
      "0.9123\n0.9047\n0.9082\n",
      "0.9045\n0.9104\n0.9147\n",
      "0.9147\n0.9092\n0.915\n",
      "0.9148\n0.9172\n0.8992\n",
      "0.9126\n0.9103\n0.9112\n",
      "0.9163\n0.9175\n",
      "0.9137\n0.9175\n0.9177\n",
      "0.9141\n0.9139\n0.9094\n",
      "0.9096\n0.9122\n0.9121\n",
      "0.9098\n0.9129\n0.91\n",
      "0.9069\n0.9169\n0.915\n",
      "0.9131\n0.9127\n0.916\n",
      "0.9085\n0.9134\n0.9127\n",
      "0.9169\n0.917\n0.9167\n",
      "0.9065\n0.9109\n0.9133\n",
      "0.9129\n0.9128\n0.9122\n",
      "0.9117\n0.9106\n0.9103\n",
      "0.9059\n0.9075\n0.9126\n",
      "0.9136\n0.9124\n0.916",
      "\n0.9045\n0.9121\n",
      "0.9145\n0.908\n0.892\n",
      "0.9027\n0.9104\n0.9123\n",
      "0.9134\n0.9137\n0.9142\n",
      "0.9105\n0.9062\n0.9104\n",
      "0.9044\n0.9031\n0.9054",
      "\n0.9109\n0.9152\n",
      "0.9157\n0.9152\n0.9148\n",
      "0.9119\n0.9068\n0.9117\n",
      "0.9154\n0.9152\n0.9109",
      "\n0.912\n0.9119\n",
      "0.9113\n0.907\n0.9107",
      "\n0.9114\n0.9061\n",
      "0.909\n0.9061\n0.9069",
      "\n0.9073\n0.9091\n",
      "0.8936\n0.8974\n0.9138",
      "\n0.913\n0.9059",
      "\n0.9065\n0.9115\n",
      "0.9174\n0.9137\n0.9157",
      "\n0.9166\n0.9165",
      "\n0.9174\n0.9142\n",
      "0.915\n0.9142\n0.9165",
      "\n0.9148\n0.9171",
      "\n0.9115\n0.9114",
      "\n0.9117\n0.9092\n",
      "0.9072\n0.9147\n0.9175",
      "\n0.9137\n0.9173",
      "\n0.9118\n0.9104\n",
      "0.9095\n0.9144\n0.9114",
      "\n0.9098\n0.9172",
      "\n0.9156\n0.9167",
      "\n0.9152\n0.9148\n",
      "0.9139\n0.9181\n0.914",
      "\n0.9141\n0.9127",
      "\n0.9127\n0.9121",
      "\n0.9142\n0.9078",
      "\n0.9132\n0.9129",
      "\n0.9131\n0.9075",
      "\n0.9116\n0.9115",
      "\n0.9098\n0.9124",
      "\n0.9119\n0.9119",
      "\n0.9093\n0.9144",
      "\n0.9156\n0.9029",
      "\n0.9062\n0.9116",
      "\n0.9067\n0.9065",
      "\n0.9111\n",
      "0.9129\n0.9111\n0.9059",
      "\n0.9091\n0.9096",
      "\n0.911\n0.9096\n",
      "0.9124\n0.9094\n0.9087",
      "\n0.9086\n0.9101",
      "\n0.9091\n0.9129",
      "\n0.9102\n",
      "0.908\n0.9132\n0.9101",
      "\n0.9125\n0.9134",
      "\n0.911\n0.9101",
      "\n0.9148\n",
      "0.9142\n0.9125\n",
      "0.913\n0.9131\n",
      "0.9124\n0.917\n0.9162",
      "\n0.9142\n0.9144",
      "\n0.9166\n0.9158",
      "\n0.903\n0.9101",
      "\n0.9141\n0.9065",
      "\n0.9107\n0.9105",
      "\n0.9134\n0.9121",
      "\n0.914\n0.9083",
      "\n0.9129\n",
      "0.9139\n0.9122\n",
      "0.9106\n0.9091\n",
      "0.9098\n0.9125\n",
      "0.9145\n0.9132\n0.9118",
      "\n0.9103\n",
      "0.9117\n0.9103\n",
      "0.9116\n0.9062\n",
      "0.9105\n0.9119\n",
      "0.9109\n0.9091\n0.9047",
      "\n0.9106\n",
      "0.9077\n0.9117\n0.9046",
      "\n0.9077\n",
      "0.9062\n0.8994\n0.907",
      "\n0.9098\n",
      "0.9123\n0.9068\n",
      "0.9099\n0.9113\n",
      "0.906\n0.9121\n",
      "0.9005\n0.9109\n0.9102",
      "\n0.9135\n",
      "0.9134\n0.9106\n",
      "0.912\n0.9099\n",
      "0.9127\n0.9112\n",
      "0.9131\n0.9088\n",
      "0.9103\n0.9133\n",
      "0.9128\n0.9131\n",
      "0.9151\n0.9117\n",
      "0.9133\n0.9171\n",
      "0.9119\n0.9109\n",
      "0.91\n0.9069\n",
      "0.9122\n0.909\n",
      "0.9113\n0.9153\n",
      "0.9128\n0.9112\n",
      "0.9101\n0.9145\n",
      "0.909\n0.9129\n",
      "0.9092\n0.9131\n",
      "0.9102\n0.9112\n",
      "0.9087\n0.9122\n",
      "0.9091\n0.9138\n",
      "0.914\n0.9112\n",
      "0.9105\n0.9143\n",
      "0.9132\n0.9115\n",
      "0.9137\n0.9097\n",
      "0.9049\n0.9111\n",
      "0.9122\n0.9128\n",
      "0.9142\n0.9087\n",
      "0.9055\n0.9158\n",
      "0.9131\n0.9154\n",
      "0.9119\n0.9078\n",
      "0.9126\n0.9122\n",
      "0.9127\n0.9097\n",
      "0.9124\n0.912\n",
      "0.9098\n0.9146\n",
      "0.9135\n0.9105\n",
      "0.9122\n0.9114\n",
      "0.9058\n0.9119\n",
      "0.9121\n0.9086\n",
      "0.9153\n0.9159\n",
      "0.9154\n0.9151\n",
      "0.9112\n0.9145\n",
      "0.9161\n0.9189\n",
      "0.9158\n0.9165\n",
      "0.9116\n0.9115\n",
      "0.9163\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "minst = input_data.read_data_sets(r'.\\deep_learn\\text2image\\MNIST_data', one_hot=True)\n",
    "\n",
    "x = tf.placeholder(\"float\", shape=[None, 784])\n",
    "y_ = tf.placeholder(\"float\", shape=[None, 10])\n",
    "w = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(x, w) +b)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "for i in range(1000):\n",
    "    batch = minst.train.next_batch(50)\n",
    "    train_step.run(feed_dict={x:batch[0], y_:batch[1]})\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(accuracy.eval(feed_dict={x: minst.test.images, y_:minst.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-1a72ff16ae40>:52: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "step 0, training accuracy: 0.18000000715255737\n",
      "step 100, training accuracy: 0.800000011920929\n",
      "step 200, training accuracy: 0.9399999976158142\n",
      "step 300, training accuracy: 0.9800000190734863\n",
      "step 400, training accuracy: 0.9399999976158142\n",
      "step 500, training accuracy: 0.9599999785423279\n",
      "step 600, training accuracy: 0.8999999761581421\n",
      "step 700, training accuracy: 0.9599999785423279\n",
      "step 800, training accuracy: 0.9599999785423279\n",
      "step 900, training accuracy: 0.9800000190734863\n",
      "step 1000, training accuracy: 0.9800000190734863\n",
      "step 1100, training accuracy: 0.9399999976158142\n",
      "step 1200, training accuracy: 0.9399999976158142\n",
      "step 1300, training accuracy: 0.9800000190734863\n",
      "step 1400, training accuracy: 0.9599999785423279\n",
      "step 1500, training accuracy: 0.9399999976158142\n",
      "step 1600, training accuracy: 1.0\n",
      "step 1700, training accuracy: 1.0\n",
      "step 1800, training accuracy: 0.9599999785423279\n",
      "step 1900, training accuracy: 0.9399999976158142\n",
      "step 2000, training accuracy: 0.9800000190734863\n",
      "step 2100, training accuracy: 1.0\n",
      "step 2200, training accuracy: 0.9399999976158142\n",
      "step 2300, training accuracy: 1.0\n",
      "step 2400, training accuracy: 0.9800000190734863\n",
      "step 2500, training accuracy: 0.9800000190734863\n",
      "step 2600, training accuracy: 0.9599999785423279\n",
      "step 2700, training accuracy: 0.9800000190734863\n",
      "step 2800, training accuracy: 0.9800000190734863\n",
      "step 2900, training accuracy: 0.9800000190734863\n",
      "step 3000, training accuracy: 0.9599999785423279\n",
      "step 3100, training accuracy: 1.0\n",
      "step 3200, training accuracy: 0.9800000190734863\n",
      "step 3300, training accuracy: 0.9800000190734863\n",
      "step 3400, training accuracy: 0.9800000190734863\n",
      "step 3500, training accuracy: 0.9800000190734863\n",
      "step 3600, training accuracy: 0.9800000190734863\n",
      "step 3700, training accuracy: 1.0\n",
      "step 3800, training accuracy: 0.9599999785423279\n",
      "step 3900, training accuracy: 1.0\n",
      "step 4000, training accuracy: 0.9800000190734863\n",
      "step 4100, training accuracy: 0.9800000190734863\n",
      "step 4200, training accuracy: 1.0\n",
      "step 4300, training accuracy: 1.0\n",
      "step 4400, training accuracy: 1.0\n",
      "step 4500, training accuracy: 0.9800000190734863\n",
      "step 4600, training accuracy: 1.0\n",
      "step 4700, training accuracy: 1.0\n",
      "step 4800, training accuracy: 1.0\n",
      "step 4900, training accuracy: 0.9800000190734863\n",
      "step 5000, training accuracy: 0.9800000190734863\n",
      "step 5100, training accuracy: 0.9599999785423279\n",
      "step 5200, training accuracy: 0.9800000190734863\n",
      "step 5300, training accuracy: 0.9800000190734863\n",
      "step 5400, training accuracy: 1.0\n",
      "step 5500, training accuracy: 1.0\n",
      "step 5600, training accuracy: 1.0\n",
      "step 5700, training accuracy: 0.9599999785423279\n",
      "step 5800, training accuracy: 0.9800000190734863\n",
      "step 5900, training accuracy: 1.0\n",
      "step 6000, training accuracy: 1.0\n",
      "step 6100, training accuracy: 1.0\n",
      "step 6200, training accuracy: 0.9800000190734863\n",
      "step 6300, training accuracy: 1.0\n",
      "step 6400, training accuracy: 1.0\n",
      "step 6500, training accuracy: 1.0\n",
      "step 6600, training accuracy: 1.0\n",
      "step 6700, training accuracy: 1.0\n",
      "step 6800, training accuracy: 1.0\n",
      "step 6900, training accuracy: 1.0\n",
      "step 7000, training accuracy: 1.0\n",
      "step 7100, training accuracy: 1.0\n",
      "step 7200, training accuracy: 0.9800000190734863\n",
      "step 7300, training accuracy: 1.0\n",
      "step 7400, training accuracy: 1.0\n",
      "step 7500, training accuracy: 0.9800000190734863\n",
      "step 7600, training accuracy: 1.0\n",
      "step 7700, training accuracy: 1.0\n",
      "step 7800, training accuracy: 1.0\n",
      "step 7900, training accuracy: 1.0\n",
      "step 8000, training accuracy: 0.9800000190734863\n",
      "step 8100, training accuracy: 1.0\n",
      "step 8200, training accuracy: 1.0\n",
      "step 8300, training accuracy: 0.9800000190734863\n",
      "step 8400, training accuracy: 1.0\n",
      "step 8500, training accuracy: 1.0\n",
      "step 8600, training accuracy: 1.0\n",
      "step 8700, training accuracy: 1.0\n",
      "step 8800, training accuracy: 1.0\n",
      "step 8900, training accuracy: 1.0\n",
      "step 9000, training accuracy: 1.0\n",
      "step 9100, training accuracy: 1.0\n",
      "step 9200, training accuracy: 1.0\n",
      "step 9300, training accuracy: 1.0\n",
      "step 9400, training accuracy: 1.0\n",
      "step 9500, training accuracy: 1.0\n",
      "step 9600, training accuracy: 1.0\n",
      "step 9700, training accuracy: 1.0\n",
      "step 9800, training accuracy: 1.0\n",
      "step 9900, training accuracy: 1.0\n",
      "step 10000, training accuracy: 1.0\n",
      "step 10100, training accuracy: 1.0\n",
      "step 10200, training accuracy: 1.0\n",
      "step 10300, training accuracy: 1.0\n",
      "step 10400, training accuracy: 1.0\n",
      "step 10500, training accuracy: 1.0\n",
      "step 10600, training accuracy: 1.0\n",
      "step 10700, training accuracy: 1.0\n",
      "step 10800, training accuracy: 1.0\n",
      "step 10900, training accuracy: 1.0\n",
      "step 11000, training accuracy: 0.9800000190734863\n",
      "step 11100, training accuracy: 1.0\n",
      "step 11200, training accuracy: 1.0\n",
      "step 11300, training accuracy: 1.0\n",
      "step 11400, training accuracy: 1.0\n",
      "step 11500, training accuracy: 0.9800000190734863\n",
      "step 11600, training accuracy: 1.0\n",
      "step 11700, training accuracy: 1.0\n",
      "step 11800, training accuracy: 0.9800000190734863\n",
      "step 11900, training accuracy: 1.0\n",
      "step 12000, training accuracy: 1.0\n",
      "step 12100, training accuracy: 1.0\n",
      "step 12200, training accuracy: 1.0\n",
      "step 12300, training accuracy: 1.0\n",
      "step 12400, training accuracy: 0.9800000190734863\n",
      "step 12500, training accuracy: 1.0\n",
      "step 12600, training accuracy: 0.9800000190734863\n",
      "step 12700, training accuracy: 1.0\n",
      "step 12800, training accuracy: 1.0\n",
      "step 12900, training accuracy: 1.0\n",
      "step 13000, training accuracy: 1.0\n",
      "step 13100, training accuracy: 1.0\n",
      "step 13200, training accuracy: 1.0\n",
      "step 13300, training accuracy: 1.0\n",
      "step 13400, training accuracy: 1.0\n",
      "step 13500, training accuracy: 1.0\n",
      "step 13600, training accuracy: 1.0\n",
      "step 13700, training accuracy: 1.0\n",
      "step 13800, training accuracy: 1.0\n",
      "step 13900, training accuracy: 1.0\n",
      "step 14000, training accuracy: 1.0\n",
      "step 14100, training accuracy: 1.0\n",
      "step 14200, training accuracy: 1.0\n",
      "step 14300, training accuracy: 1.0\n",
      "step 14400, training accuracy: 0.9800000190734863\n",
      "step 14500, training accuracy: 0.9800000190734863\n",
      "step 14600, training accuracy: 1.0\n",
      "step 14700, training accuracy: 1.0\n",
      "step 14800, training accuracy: 1.0\n",
      "step 14900, training accuracy: 1.0\n",
      "step 15000, training accuracy: 1.0\n",
      "step 15100, training accuracy: 1.0\n",
      "step 15200, training accuracy: 1.0\n",
      "step 15300, training accuracy: 1.0\n",
      "step 15400, training accuracy: 1.0\n",
      "step 15500, training accuracy: 1.0\n",
      "step 15600, training accuracy: 1.0\n",
      "step 15700, training accuracy: 1.0\n",
      "step 15800, training accuracy: 1.0\n",
      "step 15900, training accuracy: 1.0\n",
      "step 16000, training accuracy: 1.0\n",
      "step 16100, training accuracy: 1.0\n",
      "step 16200, training accuracy: 1.0\n",
      "step 16300, training accuracy: 0.9800000190734863\n",
      "step 16400, training accuracy: 1.0\n",
      "step 16500, training accuracy: 1.0\n",
      "step 16600, training accuracy: 1.0\n",
      "step 16700, training accuracy: 1.0\n",
      "step 16800, training accuracy: 1.0\n",
      "step 16900, training accuracy: 1.0\n",
      "step 17000, training accuracy: 1.0\n",
      "step 17100, training accuracy: 1.0\n",
      "step 17200, training accuracy: 1.0\n",
      "step 17300, training accuracy: 1.0\n",
      "step 17400, training accuracy: 0.9800000190734863\n",
      "step 17500, training accuracy: 1.0\n",
      "step 17600, training accuracy: 1.0\n",
      "step 17700, training accuracy: 1.0\n",
      "step 17800, training accuracy: 1.0\n",
      "step 17900, training accuracy: 1.0\n",
      "step 18000, training accuracy: 1.0\n",
      "step 18100, training accuracy: 1.0\n",
      "step 18200, training accuracy: 1.0\n",
      "step 18300, training accuracy: 1.0\n",
      "step 18400, training accuracy: 0.9800000190734863\n",
      "step 18500, training accuracy: 1.0\n",
      "step 18600, training accuracy: 1.0\n",
      "step 18700, training accuracy: 1.0\n",
      "step 18800, training accuracy: 1.0\n",
      "step 18900, training accuracy: 1.0\n",
      "step 19000, training accuracy: 1.0\n",
      "step 19100, training accuracy: 1.0\n",
      "step 19200, training accuracy: 1.0\n",
      "step 19300, training accuracy: 1.0\n",
      "step 19400, training accuracy: 1.0\n",
      "step 19500, training accuracy: 1.0\n",
      "step 19600, training accuracy: 1.0\n",
      "step 19700, training accuracy: 1.0\n",
      "step 19800, training accuracy: 1.0\n",
      "step 19900, training accuracy: 1.0\n",
      "test accuracy: 0.9921000003814697\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 权重初始化\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# 卷积和池化\n",
    "def conv2d(x, w):\n",
    "    return tf.nn.conv2d(x, w, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1],\n",
    "                          strides=[1,2,2,1], padding='SAME')\n",
    "# 第一层卷积它由一个卷积接一个max pooling完成。\n",
    "# 卷积在每个5x5的patch中算出32个特征。\n",
    "# 卷积的权重张量形状是[5, 5, 1, 32]，前两个维度是patch的大小，接着是输入的通道数目，最后是输出的通道数目。\n",
    "# 而对于每一个输出通道都有一个对应的偏置量。\n",
    "W_conv1 = weight_variable([5,5,1,32])\n",
    "b_conv1 = bias_variable(([32]))\n",
    "#把x变成一个4d向量，其第2、第3维对应图片的宽、高，\n",
    "# 最后一维代表图片的颜色通道数(因为是灰度图所以这里的通道数为1，\n",
    "# 如果是rgb彩色图，则为3)。\n",
    "x_image = tf.reshape(x, [-1, 28,28,1])\n",
    "# 把x_image和权值向量进行卷积，\n",
    "# 加上偏置项，然后应用ReLU激活函数，最后进行max pooling。\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# 第二层卷积, 上一层输入为32\n",
    "W_conv2 = weight_variable([5,5,32,64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2)+ b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "# 密集连接层\n",
    "# 图片尺寸减小到7x7，\n",
    "# 我们加入一个有1024个神经元的全连接层，用于处理整个图片。\n",
    "# 我们把池化层输出的张量reshape成一些向量，乘上权重矩阵，加上偏置，然后对其使用ReLU。\n",
    "W_fc1 = weight_variable([7*7*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "# Dropout为了减少过拟合，我们在输出层之前加入dropout。\n",
    "# 我们用一个placeholder来代表一个神经元的输出在dropout中保持不变的概率。\n",
    "# 这样我们可以在训练过程中启用dropout，在测试过程中关闭dropout。 \n",
    "# TensorFlow的tf.nn.dropout操作除了可以屏蔽神经元的输出外，\n",
    "# 还会自动处理神经元输出值的scale。所以用dropout的时候可以不用考虑scale。\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# 输出层\n",
    "#最后，我们添加一个softmax层，就像前面的单层softmax regression一样。\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "# 为了进行训练和评估，我们使用与之前简单的单层SoftMax神经网络模型几乎相同的一套代码，\n",
    "# 只是我们会用更加复杂的ADAM优化器来做梯度最速下降，\n",
    "# 在feed_dict中加入额外的参数keep_prob来控制dropout比例。\n",
    "# 然后每100次迭代输出一次日志。\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(20000):\n",
    "    batch = minst.train.next_batch(50)\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x:batch[0], y_:batch[1], keep_prob:1.0\n",
    "        })\n",
    "        print(\"step {}, training accuracy: {}\".format(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x:batch[0], y_:batch[1], keep_prob:0.5})\n",
    "print(\"test accuracy: {}\".format(accuracy.eval(feed_dict={\n",
    "    x:minst.test.images, y_:minst.test.labels, keep_prob:1.0\n",
    "})))\n",
    "    \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
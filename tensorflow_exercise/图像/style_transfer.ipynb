{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import time\n",
    "# 原有模型的RGB通道的值\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "class VGGNet:\n",
    "    \"\"\"Builds VGG-16 net structure,\n",
    "        load parameters from pre-train models.    \n",
    "    \"\"\"\n",
    "    def __init__(self, data_dict):\n",
    "        self.data_dict = data_dict\n",
    "    # 将卷积分离出来\n",
    "    def get_conv_filter(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name='conv')\n",
    "    # 得到权重\n",
    "    def get_fc_weight(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name='fc')\n",
    "    # 得到偏置\n",
    "    def get_bias(self, name):\n",
    "        return tf.constant(self.data_dict[name][1], name='bias')\n",
    "    \n",
    "    # 卷积层\n",
    "    def conv_layer(self, x, name):\n",
    "        \"\"\"Builds convolution layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            conv_w = self.get_conv_filter(name)\n",
    "            conv_b = self.get_bias(name)\n",
    "            h = tf.nn.conv2d(x, conv_w, [1,1,1,1], padding='SAME')\n",
    "            h = tf.nn.bias_add(h, conv_b)\n",
    "            h = tf.nn.relu(h)\n",
    "            return h\n",
    "    # pooling层\n",
    "    def pooling_layer(self, x, name):\n",
    "        \"\"\"Builds pooling layer\"\"\"\n",
    "        return tf.nn.max_pool(x,\n",
    "                              ksize=[1,2,2,1],\n",
    "                              strides=[1,2,2,1],\n",
    "                              padding='SAME',\n",
    "                              name=name\n",
    "                              )\n",
    "    # 全连接层\n",
    "    def fc_layser(self, x, name, activation=tf.nn.relu):\n",
    "        \"\"\"Builds fuu \"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            fc_w = self.get_fc_weight(name)\n",
    "            fc_b = self.get_bias(name)\n",
    "            h = tf.matmul(x, fc_w)\n",
    "            h = tf.nn.bias_add(h, fc_b)\n",
    "            if activation is None:\n",
    "                return h\n",
    "            else:\n",
    "                return activation(h)\n",
    "    # 展平\n",
    "    def flatten_layer(self, x, name):\n",
    "        with tf.name_scope(name):\n",
    "            # x = [batch_size, image_width, image_height, channel]\n",
    "            x_shape = x.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for d in x_shape[1:]:\n",
    "                dim *= d\n",
    "            x = tf.reshape(x, [-1, dim])\n",
    "            return x   \n",
    "    def build(self, x_rgb):\n",
    "        \"\"\"\n",
    "        Build VGG16 network structure.\n",
    "        :param x_rgb: [1, 224, 224, 3]\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        print('building model ....')\n",
    "        r, g, b = tf.split(x_rgb, [1,1,1], axis=3)\n",
    "        x_bgr = tf.concat(\n",
    "            [b - VGG_MEAN[0],\n",
    "             g - VGG_MEAN[1],\n",
    "             r - VGG_MEAN[2],],\n",
    "            axis=3\n",
    "        )\n",
    "        \n",
    "        assert x_bgr.get_shape().as_list()[1:] == [224, 224, 3]\n",
    "        \n",
    "        self.conv1_1 = self.conv_layer(x_bgr, 'conv1_1')\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1, 'conv1_2')\n",
    "        self.pool1 = self.pooling_layer(self.conv1_2, 'pool1')\n",
    "        \n",
    "                \n",
    "        self.conv2_1 = self.conv_layer(self.pool1, 'conv2_1')\n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1, 'conv2_2')\n",
    "        self.pool2 = self.pooling_layer(self.conv2_2, 'pool2')\n",
    "        \n",
    "                \n",
    "        self.conv3_1 = self.conv_layer(self.pool2, 'conv3_1')\n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1, 'conv3_2')\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2, 'conv3_3')\n",
    "        self.pool3 = self.pooling_layer(self.conv3_3, 'pool3')        \n",
    "                \n",
    "        self.conv4_1 = self.conv_layer(self.pool3, 'conv4_1')\n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1, 'conv4_2')\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2, 'conv4_3')\n",
    "        self.pool4 = self.pooling_layer(self.conv4_3, 'pool4')        \n",
    "                \n",
    "        self.conv5_1 = self.conv_layer(self.pool4, 'conv5_1')\n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1, 'conv5_2')\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2, 'conv5_3')\n",
    "        self.pool5 = self.pooling_layer(self.conv5_3, 'pool5')\n",
    "        \n",
    "        '''\n",
    "        self.flatten5 = self.flatten_layer(self.pool5, 'flatten')\n",
    "        self.fc6 = self.fc_layser(self.flatten5, 'fc6')\n",
    "        self.fc7 = self.fc_layser(self.fc6, 'fc7')\n",
    "        self.fc8 = self.fc_layser(self.fc7, 'fc8', activation=None)\n",
    "        self.prob = tf.nn.softmax(self.fc8, name='prob')\n",
    "        '''\n",
    "        print('Building model finished:{}'.format(time.time()- start_time))\n",
    "# gugon = r'.\\deep_learn\\images\\gugong.jpg'\n",
    "# gugong = Image.open(gugon)\n",
    "# gugong_resize = gugong.resize((224, 224), Image.ANTIALIAS)\n",
    "# gugong_resize.save(r'.\\deep_learn\\images\\gugong_resize.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "vgg_npy_path = r'.\\deep_learn\\vgg16.npy'\n",
    "# data_dict = np.load(vgg_npy_path, encoding='latin1').item()\n",
    "# vgg16_for_result = VGGNet(data_dict)\n",
    "# content = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "# vgg16_for_result.build(content)\n",
    "style_img_path = r'.\\deep_learn\\images\\xingyueye_resize.jpeg'\n",
    "content_img_path = r'.\\deep_learn\\images\\gugong_resize.jpg'\n",
    "\n",
    "num_steps = 100\n",
    "learning_rate = 10\n",
    "lambda_c = 0.1\n",
    "lambda_s = 500\n",
    "\n",
    "output_dir = r'./run_style_trainsfer'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n",
      "(1, 224, 224, 3)\n(1, 224, 224, 3)\n",
      "building model ....\n",
      "Building model finished:0.2258002758026123\nbuilding model ....\n",
      "Building model finished:0.19900012016296387\nbuilding model ....\n",
      "Building model finished:0.2203998565673828\nWARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def initial_result(shape, mean, sdddev):\n",
    "    initail = tf.truncated_normal(shape, mean, sdddev)\n",
    "    return tf.Variable(initail)\n",
    "\n",
    "def read_img(img_name):\n",
    "    img = Image.open(img_name)\n",
    "    np_img = np.array(img)  # (224, 224, 3)\n",
    "    np_img = np.asarray([np_img], dtype=np.int32) # (1, 224,224, 3)\n",
    "    print(np_img.shape)\n",
    "    return np_img\n",
    "# 计算两张图片的相似度\n",
    "def gram_matrix(x):\n",
    "    \"\"\"Calulates gram matrix\n",
    "    :param x: features extracted from VGG Net. shape: [1, width, height, ch]\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    b, w, h, ch = x.get_shape().as_list()\n",
    "    features = tf.reshape(x, [b, h*w, ch]) \n",
    "    gram = tf.matmul(features, features, adjoint_a=True) \\\n",
    "        / tf.constant(ch * w * h, tf.float32)\n",
    "    return gram\n",
    "    \n",
    "    \n",
    "result = initial_result((1, 224, 224, 3), 127.5, 20)\n",
    "content_val = read_img(content_img_path)\n",
    "style_val = read_img(style_img_path)\n",
    "\n",
    "content = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "style = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "\n",
    "data_dict = np.load(vgg_npy_path, encoding='latin1').item()\n",
    "vgg_for_content = VGGNet(data_dict)\n",
    "vgg_for_style = VGGNet(data_dict)\n",
    "vgg_for_result = VGGNet(data_dict)\n",
    "\n",
    "vgg_for_content.build(content)\n",
    "vgg_for_style.build((style))\n",
    "vgg_for_result.build(result)\n",
    "\n",
    "# 提取VGGnet不同层次的卷积层， 不同层次的卷积对整体影响不同\n",
    "content_features = [\n",
    "    vgg_for_content.conv1_2,\n",
    "    vgg_for_content.conv2_2,\n",
    "    # vgg_for_content.conv3_3,\n",
    "    # vgg_for_content.conv4_3,\n",
    "    # vgg_for_content.conv5_3,\n",
    "]\n",
    "\n",
    "result_content_features = [\n",
    "    vgg_for_result.conv1_2,\n",
    "    vgg_for_result.conv2_2,\n",
    "    # vgg_for_result.conv3_3,\n",
    "    # vgg_for_result.conv4_3,\n",
    "    # vgg_for_result.conv5_3,\n",
    "    \n",
    "]\n",
    "style_features = [\n",
    "    # vgg_for_style.conv1_2,\n",
    "    # vgg_for_style.conv2_2,\n",
    "    # vgg_for_style.conv3_3,\n",
    "    vgg_for_style.conv4_3,\n",
    "    # vgg_for_style.conv5_3,\n",
    "]\n",
    "style_gram = [gram_matrix(feature) for feature in style_features]\n",
    "\n",
    "result_style_features = [\n",
    "    # vgg_for_result.conv1_2,\n",
    "    # vgg_for_result.conv2_2,\n",
    "    # vgg_for_result.conv3_3,\n",
    "    vgg_for_result.conv4_3,\n",
    "    # vgg_for_result.conv5_3,\n",
    "]\n",
    "result_style_gram = [gram_matrix(feature) for feature in result_style_features]\n",
    "# 内容loss\n",
    "content_loss = tf.zeros(1, tf.float32)\n",
    "for c, c_ in zip(content_features, result_content_features):\n",
    "    content_loss += tf.reduce_mean((c - c_) ** 2, [1, 2, 3])\n",
    "\n",
    "style_loss = tf.zeros(1, tf.float32)\n",
    "for s, s_ in zip(style_gram, result_style_gram):\n",
    "    style_loss += tf.reduce_mean((s - s_) ** 2, [1, 2])\n",
    "# (内容与风格)加权\n",
    "\n",
    "loss = content_loss * lambda_c + style_loss * lambda_s\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "step: 1, loss_value: 37377.484375, content_loss: 272564.5625 style_loss: 20.242050170898438\n",
      "step: 2, loss_value: 31028.888671875, content_loss: 214732.625 style_loss: 19.111249923706055\n",
      "step: 3, loss_value: 25611.21875, content_loss: 177966.921875 style_loss: 15.629048347473145\n",
      "step: 4, loss_value: 21562.537109375, content_loss: 157419.5 style_loss: 11.641173362731934\n",
      "step: 5, loss_value: 19160.98046875, content_loss: 143927.9375 style_loss: 9.536373138427734\n",
      "step: 6, loss_value: 17656.47265625, content_loss: 135240.390625 style_loss: 8.26486587524414\n",
      "step: 7, loss_value: 17056.435546875, content_loss: 129278.9140625 style_loss: 8.257086753845215\n",
      "step: 8, loss_value: 15572.9267578125, content_loss: 123620.21875 style_loss: 6.421808242797852\n",
      "step: 9, loss_value: 14856.857421875, content_loss: 117477.171875 style_loss: 6.218278884887695\n",
      "step: 10, loss_value: 14145.330078125, content_loss: 110605.984375 style_loss: 6.169464111328125\n",
      "step: 11, loss_value: 13078.2978515625, content_loss: 104118.40625 style_loss: 5.332913398742676\n",
      "step: 12, loss_value: 12361.33203125, content_loss: 97138.859375 style_loss: 5.294893264770508\n",
      "step: 13, loss_value: 11692.2861328125, content_loss: 89456.0703125 style_loss: 5.493357181549072\n",
      "step: 14, loss_value: 10775.853515625, content_loss: 82796.8984375 style_loss: 4.992325305938721\n",
      "step: 15, loss_value: 10155.41015625, content_loss: 76537.6484375 style_loss: 5.003291130065918\n",
      "step: 16, loss_value: 9734.2080078125, content_loss: 70545.546875 style_loss: 5.359306335449219\n",
      "step: 17, loss_value: 9048.841796875, content_loss: 66094.09375 style_loss: 4.878864288330078\n",
      "step: 18, loss_value: 8754.455078125, content_loss: 61789.32421875 style_loss: 5.151045322418213\n",
      "step: 19, loss_value: 8246.75, content_loss: 58730.46484375 style_loss: 4.747406959533691\n",
      "step: 20, loss_value: 7969.3720703125, content_loss: 56017.1484375 style_loss: 4.735313892364502\n",
      "step: 21, loss_value: 7609.1611328125, content_loss: 53554.10546875 style_loss: 4.507501602172852\n",
      "step: 22, loss_value: 7380.12841796875, content_loss: 51076.890625 style_loss: 4.544878959655762\n",
      "step: 23, loss_value: 7207.7919921875, content_loss: 49135.0390625 style_loss: 4.588576793670654\n",
      "step: 24, loss_value: 7612.54052734375, content_loss: 47043.875 style_loss: 5.816305637359619\n",
      "step: 25, loss_value: 6930.1328125, content_loss: 46705.484375 style_loss: 4.519168853759766\n",
      "step: 26, loss_value: 6897.72119140625, content_loss: 46166.66796875 style_loss: 4.562108516693115\n",
      "step: 27, loss_value: 6870.1611328125, content_loss: 44950.5234375 style_loss: 4.750218391418457\n",
      "step: 28, loss_value: 6551.462890625, content_loss: 44151.8203125 style_loss: 4.272561073303223\n",
      "step: 29, loss_value: 6403.3671875, content_loss: 42493.87109375 style_loss: 4.30795955657959\n",
      "step: 30, loss_value: 6150.05126953125, content_loss: 40882.984375 style_loss: 4.123505592346191\n",
      "step: 31, loss_value: 6002.341796875, content_loss: 39202.68359375 style_loss: 4.164146423339844\n",
      "step: 32, loss_value: 5831.40576171875, content_loss: 37834.33984375 style_loss: 4.095943450927734\n",
      "step: 33, loss_value: 5867.626953125, content_loss: 36133.09765625 style_loss: 4.508634567260742\n",
      "step: 34, loss_value: 5701.2763671875, content_loss: 35512.98046875 style_loss: 4.299956321716309\n",
      "step: 35, loss_value: 5830.591796875, content_loss: 34275.875 style_loss: 4.806008815765381\n",
      "step: 36, loss_value: 5504.451171875, content_loss: 34399.0546875 style_loss: 4.129091262817383\n",
      "step: 37, loss_value: 5461.626953125, content_loss: 34048.96484375 style_loss: 4.113461494445801\n",
      "step: 38, loss_value: 5374.66015625, content_loss: 33267.79296875 style_loss: 4.095761775970459\n",
      "step: 39, loss_value: 5235.626953125, content_loss: 32634.865234375 style_loss: 3.9442806243896484\n",
      "step: 40, loss_value: 5200.7138671875, content_loss: 31478.9296875 style_loss: 4.1056413650512695\n",
      "step: 41, loss_value: 5191.6845703125, content_loss: 31121.12890625 style_loss: 4.15914249420166\n",
      "step: 42, loss_value: 5625.896484375, content_loss: 30390.880859375 style_loss: 5.173616409301758\n",
      "step: 43, loss_value: 5223.54833984375, content_loss: 31404.91796875 style_loss: 4.166112899780273\n",
      "step: 44, loss_value: 5306.0625, content_loss: 31750.603515625 style_loss: 4.262004852294922\n",
      "step: 45, loss_value: 5193.3505859375, content_loss: 31891.724609375 style_loss: 4.008356094360352\n",
      "step: 46, loss_value: 5078.7822265625, content_loss: 31490.59765625 style_loss: 3.8594448566436768\n",
      "step: 47, loss_value: 5002.61328125, content_loss: 30528.83984375 style_loss: 3.899458885192871\n",
      "step: 48, loss_value: 4893.80078125, content_loss: 29799.89453125 style_loss: 3.827622413635254\n",
      "step: 49, loss_value: 4938.1953125, content_loss: 28741.361328125 style_loss: 4.128117561340332\n",
      "step: 50, loss_value: 4956.44921875, content_loss: 28894.103515625 style_loss: 4.134077072143555\n",
      "step: 51, loss_value: 5267.306640625, content_loss: 28288.69140625 style_loss: 4.8768744468688965\n",
      "step: 52, loss_value: 4918.57666015625, content_loss: 29201.37890625 style_loss: 3.9968771934509277\n",
      "step: 53, loss_value: 4964.27880859375, content_loss: 29667.451171875 style_loss: 3.9950671195983887\n",
      "step: 54, loss_value: 4953.20849609375, content_loss: 29655.755859375 style_loss: 3.9752659797668457\n",
      "step: 55, loss_value: 4888.66015625, content_loss: 29636.150390625 style_loss: 3.850090503692627\n",
      "step: 56, loss_value: 5100.08203125, content_loss: 28648.1875 style_loss: 4.470526695251465\n",
      "step: 57, loss_value: 4919.34423828125, content_loss: 29007.55859375 style_loss: 4.037176609039307\n",
      "step: 58, loss_value: 4989.21630859375, content_loss: 28676.80078125 style_loss: 4.243072509765625\n",
      "step: 59, loss_value: 4815.615234375, content_loss: 28959.103515625 style_loss: 3.839409589767456\n",
      "step: 60, loss_value: 4781.4521484375, content_loss: 28835.25 style_loss: 3.795854091644287\n",
      "step: 61, loss_value: 4692.5849609375, content_loss: 28254.142578125 style_loss: 3.734341621398926\n",
      "step: 62, loss_value: 4590.53466796875, content_loss: 27426.841796875 style_loss: 3.6957006454467773\n",
      "step: 63, loss_value: 4582.42578125, content_loss: 26492.556640625 style_loss: 3.8663406372070312\n",
      "step: 64, loss_value: 4614.04931640625, content_loss: 26350.34765625 style_loss: 3.958028793334961\n",
      "step: 65, loss_value: 5179.12890625, content_loss: 25867.658203125 style_loss: 5.184725761413574\n",
      "step: 66, loss_value: 4799.0830078125, content_loss: 27697.169921875 style_loss: 4.058732032775879\n",
      "step: 67, loss_value: 4863.67333984375, content_loss: 28636.4140625 style_loss: 4.000063896179199\n",
      "step: 68, loss_value: 4901.55908203125, content_loss: 29074.2109375 style_loss: 3.9882760047912598\n",
      "step: 69, loss_value: 4829.255859375, content_loss: 29330.390625 style_loss: 3.792433500289917\n",
      "step: 70, loss_value: 4925.0048828125, content_loss: 28424.435546875 style_loss: 4.1651225090026855\n",
      "step: 71, loss_value: 4685.826171875, content_loss: 28214.884765625 style_loss: 3.72867488861084\n",
      "step: 72, loss_value: 4685.837890625, content_loss: 27363.513671875 style_loss: 3.898972988128662\n",
      "step: 73, loss_value: 4560.865234375, content_loss: 26997.0625 style_loss: 3.722318172454834\n",
      "step: 74, loss_value: 4521.380859375, content_loss: 26517.095703125 style_loss: 3.739342212677002\n",
      "step: 75, loss_value: 4434.89111328125, content_loss: 26070.787109375 style_loss: 3.6556241512298584\n",
      "step: 76, loss_value: 4387.0625, content_loss: 25488.041015625 style_loss: 3.676516532897949\n",
      "step: 77, loss_value: 4352.2900390625, content_loss: 25204.361328125 style_loss: 3.6637072563171387\n",
      "step: 78, loss_value: 4690.390625, content_loss: 24346.431640625 style_loss: 4.511495113372803\n",
      "step: 79, loss_value: 4563.6708984375, content_loss: 25300.419921875 style_loss: 4.067257404327393\n",
      "step: 80, loss_value: 4699.0009765625, content_loss: 25650.4140625 style_loss: 4.267919063568115\n",
      "step: 81, loss_value: 4554.2333984375, content_loss: 26741.08203125 style_loss: 3.7602505683898926\n",
      "step: 82, loss_value: 4568.4951171875, content_loss: 27144.705078125 style_loss: 3.7080488204956055\n",
      "step: 83, loss_value: 4471.560546875, content_loss: 26864.80859375 style_loss: 3.5701589584350586\n",
      "step: 84, loss_value: 4442.134765625, content_loss: 25930.73046875 style_loss: 3.698122978210449\n",
      "step: 85, loss_value: 4340.5009765625, content_loss: 25303.1640625 style_loss: 3.6203689575195312\n",
      "step: 86, loss_value: 4459.90966796875, content_loss: 24470.896484375 style_loss: 4.025640487670898\n",
      "step: 87, loss_value: 4405.7041015625, content_loss: 24688.73046875 style_loss: 3.8736624717712402\n",
      "step: 88, loss_value: 4604.96484375, content_loss: 24219.376953125 style_loss: 4.366054058074951\n",
      "step: 89, loss_value: 4400.77783203125, content_loss: 25124.056640625 style_loss: 3.7767438888549805\n",
      "step: 90, loss_value: 4468.55517578125, content_loss: 25560.21875 style_loss: 3.825066566467285\n",
      "step: 91, loss_value: 4420.388671875, content_loss: 25523.0703125 style_loss: 3.7361631393432617\n",
      "step: 92, loss_value: 4391.96435546875, content_loss: 25540.9921875 style_loss: 3.6757302284240723\n",
      "step: 93, loss_value: 4594.4033203125, content_loss: 24802.30078125 style_loss: 4.228346824645996\n",
      "step: 94, loss_value: 4454.8359375, content_loss: 25363.86328125 style_loss: 3.836899518966675\n",
      "step: 95, loss_value: 4486.32421875, content_loss: 25271.576171875 style_loss: 3.9183335304260254\n",
      "step: 96, loss_value: 4407.99462890625, content_loss: 25503.515625 style_loss: 3.7152864933013916\n",
      "step: 97, loss_value: 4354.6640625, content_loss: 25604.236328125 style_loss: 3.5884809494018555\n",
      "step: 98, loss_value: 4419.39208984375, content_loss: 24833.814453125 style_loss: 3.872020959854126\n",
      "step: 99, loss_value: 4391.6318359375, content_loss: 24730.34765625 style_loss: 3.8371939659118652\n",
      "step: 100, loss_value: 4646.5439453125, content_loss: 24369.646484375 style_loss: 4.419158458709717\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for step in range(num_steps):\n",
    "        loss_value, content_loss_value, style_loss_value, _ = \\\n",
    "        sess.run([loss, content_loss, style_loss, train_op],\n",
    "                 feed_dict = {\n",
    "                     content:content_val,\n",
    "                     style:style_val\n",
    "                 })\n",
    "        print('step: {}, loss_value: {}, content_loss: {} style_loss: {}' \\\n",
    "              .format(step+1,\n",
    "                      loss_value[0],\n",
    "                      content_loss_value[0],\n",
    "                      style_loss_value[0],\n",
    "                      ))\n",
    "        result_img_path = os.path.join(\n",
    "            output_dir, 'result-{}.jpg'.format(step+1)\n",
    "        )\n",
    "        result_val = result.eval(sess)[0]\n",
    "        result_val = np.clip(result_val, 0, 255)\n",
    "        img_arr = np.asarray(result_val, np.uint8)\n",
    "        img = Image.fromarray(img_arr)\n",
    "        img.save(result_img_path)\n",
    "    # content = read_img(content_img_path)\n",
    "    # style = read_img(style_img_path)\n",
    "    \n",
    "        \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\nIf you depend on functionality not listed there, please file an issue.\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        num_embedding_size = 32,\n",
    "        num_timesteps = 120,\n",
    "        num_fc_nodes = 64,\n",
    "        num_filters=256,\n",
    "        num_kernel_size=3,\n",
    "        batch_size = 100,\n",
    "        # lstm梯度\n",
    "        clip_lstm_grads = 1.0,\n",
    "        learning_rate = 0.001,\n",
    "        # 词频的限定大小\n",
    "        num_word_threshold = 10,\n",
    "    )\n",
    "\n",
    "hps = get_default_params()\n",
    "\n",
    "train_file = r'.\\deep_learn\\jd_deep_learn\\train_data.tsv'\n",
    "test_file = r'.\\deep_learn\\jd_deep_learn\\test_data.tsv'\n",
    "\n",
    "seg_train_file = r'.\\deep_learn\\jd_deep_learn\\seg_train_data.txt'\n",
    "seg_test_file = r'.\\deep_learn\\jd_deep_learn\\seg_test_data.txt'\n",
    "\n",
    "vocab_file = r'.\\deep_learn\\jd_deep_learn\\jd_vocab.txt'\n",
    "category_file = r'.\\deep_learn\\jd_deep_learn\\jd_category.txt'\n",
    "output_dir = r'.\\deep_learn\\jd_deep_learn\\jd_cnn_runout'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "INFO:tensorflow:vocab_size: 4761\n",
      "category_to_id dict:{'5': 0, '1': 1, '2': 2, '3': 3}\nINFO:tensorflow:category_size: 4\n",
      "INFO:tensorflow:id:0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, filename, num_word_threshold):\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._num_word_threshold = num_word_threshold\n",
    "        self._read_dict(filename)\n",
    "        \n",
    "    def _read_dict(self, filename):\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, frequency = line.strip('\\r\\n').split('\\t')\n",
    "            frequency = int(frequency)\n",
    "            if frequency < self._num_word_threshold:\n",
    "                continue\n",
    "            idx = len(self._word_to_id)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            self._word_to_id[word] = idx\n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self._unk)\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "    \n",
    "    def sentence_to_id(self, sentence):\n",
    "        \n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence.split()]\n",
    "        return word_ids\n",
    "\n",
    "class CategoryDict:\n",
    "    def __init__(self, filename):\n",
    "        self._category_to_id = {}\n",
    "        with open(filename, 'r',encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            category = line.strip('\\r\\n')\n",
    "            idx = len(self._category_to_id)\n",
    "            self._category_to_id[category] = idx\n",
    "    def size(self):\n",
    "        print('category_to_id dict:{}'.format(self._category_to_id))\n",
    "        return len(self._category_to_id)\n",
    "    \n",
    "    def category_to_id(self, category):\n",
    "        if not category in self._category_to_id:\n",
    "            raise Exception(\"{} is not in our category\".format(category))\n",
    "        \n",
    "        return self._category_to_id[category]\n",
    "    \n",
    "          \n",
    "vocab = Vocab(vocab_file, hps.num_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "\n",
    "tf.logging.info('vocab_size: {}'.format(vocab_size))\n",
    "category_vocab = CategoryDict(category_file)\n",
    "num_classes = category_vocab.size()\n",
    "tf.logging.info('category_size: {}'.format(num_classes))\n",
    "test_str = '5'\n",
    "tf.logging.info('id:{}'.format(category_vocab.category_to_id(test_str)))\n",
    "# print(vocab.word_to_id())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Loading data from .\\deep_learn\\jd_deep_learn\\seg_train_data.txt\n",
      "INFO:tensorflow:Loading data from .\\deep_learn\\jd_deep_learn\\seg_test_data.txt\n",
      "(array([[   5,  754,   19,   23,    1,  249,  147, 1056,    1,  787,   30,\n         147,   96,  185,  393,  249,   10,  417,  110,    2,    1,   13,\n          18,   58,    1, 1279,    8,   14,    2,    6, 1004,   29,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n       [   0,  509,   89, 3391,    2,    0, 1079,   28,    2,    5, 1215,\n        1633,  235,    1, 1656,  285,   30,  902,  213,   34,    0,    1,\n           6,    8,   79,    2, 4490,    1,   96, 1029,  222,  461, 1289,\n        4326, 2365,    2,  227,    1, 3498,   22,   79, 1433,    5,    1,\n         187, 1516,    3,    4,   60,  100,  337, 1405,  335,    0,   16,\n         100,  331,  221,  106,  100, 1541,    1,  668,    3,  668, 2214,\n        2042,    1, 1822, 1716, 4048,  264, 1809,    4,   57, 2779,    1,\n         212, 2641,   60,  130, 1273,    1,  560, 2302,  209, 3584,   40,\n         196,    4,  312,  135,  638,  546,    1,  342,  123,  236,  158,\n        2229,  129,    1,   43,    3,  342,  123, 1767,  158,   10, 1493,\n           0,    1,   43,    3,  351,  123,  219,  556,   86, 1493],\n       [2619,  182,   10,   54, 1499,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0]]), array([3, 3, 2]))\n(array([[   5,   84, 1968,  198,  222,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n       [   5,   15,   31,    1,  155,  254, 1421,    2,  282,    6,  300,\n           2,    4,  365, 1937,  120,    1,   13,   85,  497,    1,  221,\n         161,  214,  121,  124,   23,    7,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n       [  25, 1298,   14,    2,  272,  108,   15, 3462,  300,    3, 1058,\n           3,   13,   41,  119, 2997,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0]]), array([3, 1, 0]))\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "class TextDataSet:\n",
    "    def __init__(self, filename, vocab, category_vocab, num_timesteps):\n",
    "        self._vocab = vocab\n",
    "        self._category_vocab = category_vocab\n",
    "        self._num_timesteps = num_timesteps\n",
    "        self._inputs = []\n",
    "        self._outputs = []\n",
    "        \n",
    "        self._indicator = 0\n",
    "        self._parse_file(filename)\n",
    "        \n",
    "    def _parse_file(self, filename):\n",
    "        tf.logging.info('Loading data from {}'.format(filename))\n",
    "        import csv\n",
    "        csv_reader = csv.reader(open(filename, encoding='utf-8'))\n",
    "        for row in csv_reader:\n",
    "            label, content = row[0].replace('\\n', '').split('\\t')\n",
    "            id_label = self._category_vocab.category_to_id(label) # 标签转number\n",
    "            # print(id_label)\n",
    "            id_words = self._vocab.sentence_to_id(content) # 文字转number\n",
    "            # print(id_words)\n",
    "            \n",
    "            id_words = id_words[0: self._num_timesteps] # 过长截断\n",
    "            padding_num = self._num_timesteps - len(id_words) # 过短padding\n",
    "            id_words = id_words + [self._vocab.unk for _ in range(padding_num)]\n",
    "            self._inputs.append(id_words)\n",
    "            self._outputs.append(id_label)\n",
    "            \n",
    "        self._inputs = np.asarray(self._inputs, dtype=np.int32)\n",
    "        self._outputs = np.asarray(self._outputs, dtype=np.int32)\n",
    "        self._random_shuffle()\n",
    "        \n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(len(self._inputs))\n",
    "        # tf.logging.info('_random_shuffle number: {}'.format(p))\n",
    "        self._inputs = self._inputs[p]\n",
    "        self._outputs = self._outputs[p]\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            raise Exception('batch size: {} is too large'.format(batch_size))\n",
    "        \n",
    "        batch_inputs = self._inputs[self._indicator: end_indicator]\n",
    "        batch_outputs = self._outputs[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_inputs, batch_outputs\n",
    "train_data = TextDataSet(seg_train_file, vocab, category_vocab, hps.num_timesteps)\n",
    "test_data = TextDataSet(seg_test_file, vocab, category_vocab, hps.num_timesteps)\n",
    "\n",
    "print(train_data.next_batch(3))\n",
    "print(test_data.next_batch(3))\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-0ef6fe122531>:36: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.conv1d instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-0ef6fe122531>:63: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From <ipython-input-5-0ef6fe122531>:69: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.dense instead.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-5-0ef6fe122531>:81: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.math.argmax` instead\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nDeprecated in favor of operator or tf.math.divide.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "y_pred = 0\n",
    "def create_model(hps, vocab_size, num_classes):\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "    \n",
    "    # inputs = tf.placeholder(tf.int32, (batch_size, num_timesteps))\n",
    "    inputs = tf.placeholder(tf.int32, (None, num_timesteps))\n",
    "    # outputs = tf.placeholder(tf.int32, (batch_size, ))\n",
    "    outputs = tf.placeholder(tf.int32, (None, ))\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    global_step = tf.Variable(\n",
    "        tf.zeros([], tf.int64), name='global_step', trainable=False\n",
    "    )\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    with tf.variable_scope(\n",
    "        'embedding', initializer=embedding_initializer):\n",
    "        embeddings = tf.get_variable(\n",
    "            'embedding',\n",
    "            [vocab_size, hps.num_embedding_size],\n",
    "            tf.float32\n",
    "        )\n",
    "        \"\"\"\n",
    "        简单的讲就是根据inputs中的id，寻找embedding中的对应元素。\n",
    "        比如，input_ids=[1,3,5]，则找出embedding中下标为1,3,5的向量组成一个矩阵返回。\n",
    "        \"\"\"\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "        \n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_size+hps.num_filters) / 3.0\n",
    "    cnn_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    with tf.variable_scope('cnn', initializer=cnn_init):\n",
    "        conv1 = tf.layers.conv1d(embed_inputs,\n",
    "                                 hps.num_filters,\n",
    "                                 hps.num_kernel_size,\n",
    "                                 activation=tf.nn.relu,\n",
    "                                 )\n",
    "        global_maxpooling = tf.reduce_max(conv1, axis=[1])\n",
    "        \n",
    "    \"\"\"with tf.variable_scope('lstm_nn', initializer=lstm_init):\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_layers):\n",
    "            # 循环初始化lstm\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "                hps.num_lstm_nodes[i],\n",
    "                state_is_tuple = True\n",
    "            )\n",
    "            # 使用dropout方法\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(\n",
    "                cell,\n",
    "                output_keep_prob = keep_prob\n",
    "            )\n",
    "            cells.append(cell)\n",
    "        # 合并两个cell\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        # 初始化cell内的值\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        run_outputs, _ = tf.nn.dynamic_rnn(\n",
    "            cell, embed_inputs, initial_state=initial_state\n",
    "        )\n",
    "        last = run_outputs[:, -1, :]\"\"\"\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    \n",
    "    with tf.variable_scope('fc', initializer=fc_init):\n",
    "        fc1 = tf.layers.dense(global_maxpooling,\n",
    "                              hps.num_fc_nodes,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name='fc1')\n",
    "        fc1_dropout = tf.contrib.layers.dropout(fc1, keep_prob)\n",
    "        logits = tf.layers.dense(fc1_dropout,\n",
    "                                 num_classes,\n",
    "                                 name='fc2')\n",
    "    with tf.name_scope('metrics'):\n",
    "        sofmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=outputs\n",
    "        )\n",
    "        loss = tf.reduce_mean(sofmax_loss)\n",
    "        y_pred = tf.arg_max(tf.nn.softmax(logits=logits),\n",
    "                            1,\n",
    "                            output_type=tf.int32)\n",
    "        correct_pred = tf.equal(outputs, y_pred)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "    with tf.name_scope('train_op'):\n",
    "        train_op = tf.train.AdamOptimizer(hps.learning_rate).minimize(loss, global_step=global_step)\n",
    "        \n",
    "    return ((inputs, outputs, keep_prob),\n",
    "            (loss, accuracy),\n",
    "            (train_op, global_step))\n",
    "# 调用函数\n",
    "placeholders, metrics, others = create_model(\n",
    "    hps, vocab_size, num_classes\n",
    ")\n",
    "inputs, outputs, keep_prod = placeholders\n",
    "loss, accuracy = metrics\n",
    "train_op, global_step = others\n",
    "\n",
    "\n",
    "\n",
    "model_dir = os.path.join(output_dir, 'model')\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "saver = tf.train.Saver()\n",
    "model_name = 'ckp-1000'\n",
    "model_path = os.path.join(model_dir, model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Train Step: 100, loss: 1.0961178541183472, accuracy: 0.47999998927116394\n",
      "INFO:tensorflow:Train Step: 200, loss: 1.1528830528259277, accuracy: 0.4099999964237213\n",
      "INFO:tensorflow:Train Step: 300, loss: 1.1192028522491455, accuracy: 0.38999998569488525\n",
      "INFO:tensorflow:Train Step: 400, loss: 0.8872318863868713, accuracy: 0.6000000238418579\n",
      "INFO:tensorflow:Train Step: 500, loss: 0.8053421974182129, accuracy: 0.6299999952316284\n",
      "INFO:tensorflow:Train Step: 600, loss: 0.9460598826408386, accuracy: 0.5799999833106995\n",
      "INFO:tensorflow:Train Step: 700, loss: 0.8905470371246338, accuracy: 0.6299999952316284\n",
      "INFO:tensorflow:Train Step: 800, loss: 0.9654304385185242, accuracy: 0.5699999928474426\n",
      "INFO:tensorflow:Train Step: 900, loss: 0.8554862141609192, accuracy: 0.5899999737739563\n",
      "INFO:tensorflow:Train Step: 1000, loss: 1.1096627712249756, accuracy: 0.49000000953674316\n",
      "INFO:tensorflow:Test Step: 1000, loss: 0.7573379278182983, accuracy: 0.6291000247001648\n",
      "INFO:tensorflow:Train Step: 1200, loss: 0.8411310315132141, accuracy: 0.6000000238418579\n",
      "INFO:tensorflow:Train Step: 1300, loss: 0.8136965036392212, accuracy: 0.6800000071525574\n",
      "INFO:tensorflow:Train Step: 1400, loss: 0.7597445845603943, accuracy: 0.6499999761581421\n",
      "INFO:tensorflow:Train Step: 1500, loss: 0.9251340627670288, accuracy: 0.6399999856948853\n",
      "INFO:tensorflow:Train Step: 1600, loss: 0.7098598480224609, accuracy: 0.699999988079071\n",
      "INFO:tensorflow:Train Step: 1700, loss: 0.7910519242286682, accuracy: 0.6100000143051147\n",
      "INFO:tensorflow:Train Step: 1800, loss: 0.7568551898002625, accuracy: 0.6499999761581421\n",
      "INFO:tensorflow:Train Step: 1900, loss: 0.8213995099067688, accuracy: 0.6399999856948853\n",
      "INFO:tensorflow:Train Step: 2000, loss: 0.6722873449325562, accuracy: 0.7799999713897705\n",
      "INFO:tensorflow:Train Step: 2100, loss: 0.6413004994392395, accuracy: 0.7300000190734863\n",
      "INFO:tensorflow:Test Step: 2100, loss: 0.664581835269928, accuracy: 0.6596999168395996\n",
      "INFO:tensorflow:Train Step: 2300, loss: 0.7821985483169556, accuracy: 0.7099999785423279\n",
      "INFO:tensorflow:Train Step: 2400, loss: 0.731980562210083, accuracy: 0.6899999976158142\n",
      "INFO:tensorflow:Train Step: 2500, loss: 0.5872756838798523, accuracy: 0.7599999904632568\n",
      "INFO:tensorflow:Train Step: 2600, loss: 0.5311170816421509, accuracy: 0.7900000214576721\n",
      "INFO:tensorflow:Train Step: 2700, loss: 0.704113781452179, accuracy: 0.6600000262260437\n",
      "INFO:tensorflow:Train Step: 2800, loss: 0.6130226254463196, accuracy: 0.7699999809265137\n",
      "INFO:tensorflow:Train Step: 2900, loss: 0.6226199865341187, accuracy: 0.7599999904632568\n",
      "INFO:tensorflow:Train Step: 3000, loss: 0.6209381818771362, accuracy: 0.7200000286102295\n",
      "INFO:tensorflow:Train Step: 3100, loss: 0.6958066821098328, accuracy: 0.7099999785423279\n",
      "INFO:tensorflow:Train Step: 3200, loss: 0.44917619228363037, accuracy: 0.8199999928474426\n",
      "INFO:tensorflow:Test Step: 3200, loss: 0.5434297323226929, accuracy: 0.6995000243186951\n",
      "INFO:tensorflow:Train Step: 3400, loss: 0.5794610381126404, accuracy: 0.7200000286102295\n",
      "INFO:tensorflow:Train Step: 3500, loss: 0.5495132207870483, accuracy: 0.7599999904632568\n",
      "INFO:tensorflow:Train Step: 3600, loss: 0.4824182391166687, accuracy: 0.800000011920929\n",
      "INFO:tensorflow:Train Step: 3700, loss: 0.47848057746887207, accuracy: 0.800000011920929\n",
      "INFO:tensorflow:Train Step: 3800, loss: 0.516404390335083, accuracy: 0.7699999809265137\n",
      "INFO:tensorflow:Train Step: 3900, loss: 0.49432384967803955, accuracy: 0.7699999809265137\n",
      "INFO:tensorflow:Train Step: 4000, loss: 0.4131808876991272, accuracy: 0.8600000143051147\n",
      "INFO:tensorflow:Train Step: 4100, loss: 0.30616599321365356, accuracy: 0.8999999761581421\n",
      "INFO:tensorflow:Train Step: 4200, loss: 0.5969943404197693, accuracy: 0.7200000286102295\n",
      "INFO:tensorflow:Train Step: 4300, loss: 0.3062168061733246, accuracy: 0.8700000047683716\n",
      "INFO:tensorflow:Test Step: 4300, loss: 0.7084347605705261, accuracy: 0.7387999892234802\n",
      "INFO:tensorflow:Train Step: 4500, loss: 0.48108336329460144, accuracy: 0.8100000023841858\n",
      "INFO:tensorflow:Train Step: 4600, loss: 0.5352790951728821, accuracy: 0.75\n",
      "INFO:tensorflow:Train Step: 4700, loss: 0.39847028255462646, accuracy: 0.8199999928474426\n",
      "INFO:tensorflow:Train Step: 4800, loss: 0.33709821105003357, accuracy: 0.8500000238418579\n",
      "INFO:tensorflow:Train Step: 4900, loss: 0.39965522289276123, accuracy: 0.8100000023841858\n",
      "INFO:tensorflow:Train Step: 5000, loss: 0.4937503933906555, accuracy: 0.75\n",
      "INFO:tensorflow:Train Step: 5100, loss: 0.467478483915329, accuracy: 0.8500000238418579\n",
      "INFO:tensorflow:Train Step: 5200, loss: 0.4247211217880249, accuracy: 0.8100000023841858\n",
      "INFO:tensorflow:Train Step: 5300, loss: 0.32845601439476013, accuracy: 0.8700000047683716\n",
      "INFO:tensorflow:Train Step: 5400, loss: 0.2928326725959778, accuracy: 0.8299999833106995\n",
      "INFO:tensorflow:Test Step: 5400, loss: 0.37899452447891235, accuracy: 0.7635000348091125\n",
      "INFO:tensorflow:Train Step: 5600, loss: 0.31874287128448486, accuracy: 0.8799999952316284\n",
      "INFO:tensorflow:Train Step: 5700, loss: 0.31296196579933167, accuracy: 0.8799999952316284\n",
      "INFO:tensorflow:Train Step: 5800, loss: 0.32212647795677185, accuracy: 0.8799999952316284\n",
      "INFO:tensorflow:Train Step: 5900, loss: 0.388191819190979, accuracy: 0.8299999833106995\n",
      "INFO:tensorflow:Train Step: 6000, loss: 0.21340876817703247, accuracy: 0.8999999761581421\n",
      "INFO:tensorflow:Train Step: 6100, loss: 0.24704168736934662, accuracy: 0.949999988079071\n",
      "INFO:tensorflow:Train Step: 6200, loss: 0.34093961119651794, accuracy: 0.8500000238418579\n",
      "INFO:tensorflow:Train Step: 6300, loss: 0.31076210737228394, accuracy: 0.8799999952316284\n",
      "INFO:tensorflow:Train Step: 6400, loss: 0.24647678434848785, accuracy: 0.9300000071525574\n",
      "INFO:tensorflow:Train Step: 6500, loss: 0.21263259649276733, accuracy: 0.8999999761581421\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:Test Step: 6500, loss: 0.20693008601665497, accuracy: 0.7940000891685486\n",
      "INFO:tensorflow:Train Step: 6700, loss: 0.28285661339759827, accuracy: 0.8899999856948853\n",
      "INFO:tensorflow:Train Step: 6800, loss: 0.17578841745853424, accuracy: 0.949999988079071\n",
      "INFO:tensorflow:Train Step: 6900, loss: 0.309698224067688, accuracy: 0.8899999856948853\n",
      "INFO:tensorflow:Train Step: 7000, loss: 0.24241992831230164, accuracy: 0.9100000262260437\n",
      "INFO:tensorflow:Train Step: 7100, loss: 0.33624032139778137, accuracy: 0.8500000238418579\n",
      "INFO:tensorflow:Train Step: 7200, loss: 0.2540130913257599, accuracy: 0.9100000262260437\n",
      "INFO:tensorflow:Train Step: 7300, loss: 0.17336755990982056, accuracy: 0.949999988079071\n",
      "INFO:tensorflow:Train Step: 7400, loss: 0.16924257576465607, accuracy: 0.9300000071525574\n",
      "INFO:tensorflow:Train Step: 7500, loss: 0.1712512969970703, accuracy: 0.9399999976158142\n",
      "INFO:tensorflow:Train Step: 7600, loss: 0.19214001297950745, accuracy: 0.9399999976158142\n",
      "INFO:tensorflow:Test Step: 7600, loss: 0.321730375289917, accuracy: 0.8188999891281128\n",
      "INFO:tensorflow:Train Step: 7800, loss: 0.20625270903110504, accuracy: 0.9300000071525574\n",
      "INFO:tensorflow:Train Step: 7900, loss: 0.24694524705410004, accuracy: 0.8799999952316284\n",
      "INFO:tensorflow:Train Step: 8000, loss: 0.16028793156147003, accuracy: 0.9399999976158142\n",
      "INFO:tensorflow:Train Step: 8100, loss: 0.12863099575042725, accuracy: 0.949999988079071\n",
      "INFO:tensorflow:Train Step: 8200, loss: 0.23719097673892975, accuracy: 0.9100000262260437\n",
      "INFO:tensorflow:Train Step: 8300, loss: 0.2406032681465149, accuracy: 0.8700000047683716\n",
      "INFO:tensorflow:Train Step: 8400, loss: 0.13391181826591492, accuracy: 0.949999988079071\n",
      "INFO:tensorflow:Train Step: 8500, loss: 0.09715202450752258, accuracy: 0.9700000286102295\n",
      "INFO:tensorflow:Train Step: 8600, loss: 0.14075447618961334, accuracy: 0.949999988079071\n",
      "INFO:tensorflow:Train Step: 8700, loss: 0.058114875108003616, accuracy: 1.0\n",
      "INFO:tensorflow:Test Step: 8700, loss: 0.3173435628414154, accuracy: 0.8236000537872314\n",
      "INFO:tensorflow:Train Step: 8900, loss: 0.18861675262451172, accuracy: 0.9200000166893005\n",
      "INFO:tensorflow:Train Step: 9000, loss: 0.2311251312494278, accuracy: 0.8999999761581421\n",
      "INFO:tensorflow:Train Step: 9100, loss: 0.11951835453510284, accuracy: 0.9599999785423279\n",
      "INFO:tensorflow:Train Step: 9200, loss: 0.1896134912967682, accuracy: 0.9100000262260437\n",
      "INFO:tensorflow:Train Step: 9300, loss: 0.07998992502689362, accuracy: 0.9800000190734863\n",
      "INFO:tensorflow:Train Step: 9400, loss: 0.23518791794776917, accuracy: 0.9100000262260437\n",
      "INFO:tensorflow:Train Step: 9500, loss: 0.1586361527442932, accuracy: 0.9399999976158142\n",
      "INFO:tensorflow:Train Step: 9600, loss: 0.1050848737359047, accuracy: 0.9700000286102295\n",
      "INFO:tensorflow:Train Step: 9700, loss: 0.12632504105567932, accuracy: 0.9599999785423279\n",
      "INFO:tensorflow:Train Step: 9800, loss: 0.17278990149497986, accuracy: 0.9100000262260437\n",
      "INFO:tensorflow:Test Step: 9800, loss: 0.15580257773399353, accuracy: 0.866599977016449\n",
      "INFO:tensorflow:Train Step: 10000, loss: 0.09226949512958527, accuracy: 0.9700000286102295\n",
      "INFO:tensorflow:Train Step: 10100, loss: 0.1752328872680664, accuracy: 0.9200000166893005\n",
      "INFO:tensorflow:Train Step: 10200, loss: 0.16909939050674438, accuracy: 0.9300000071525574\n",
      "INFO:tensorflow:Train Step: 10300, loss: 0.0976792722940445, accuracy: 0.9700000286102295\n",
      "INFO:tensorflow:Train Step: 10400, loss: 0.11198858916759491, accuracy: 0.949999988079071\n",
      "INFO:tensorflow:Train Step: 10500, loss: 0.08002195507287979, accuracy: 0.9700000286102295\n",
      "INFO:tensorflow:Train Step: 10600, loss: 0.05856551229953766, accuracy: 0.9800000190734863\n",
      "INFO:tensorflow:Train Step: 10700, loss: 0.040291450917720795, accuracy: 0.9900000095367432\n",
      "INFO:tensorflow:Train Step: 10800, loss: 0.09877143055200577, accuracy: 0.9599999785423279\n",
      "INFO:tensorflow:Train Step: 10900, loss: 0.07532148063182831, accuracy: 0.9700000286102295\n",
      "INFO:tensorflow:Test Step: 10900, loss: 0.18757539987564087, accuracy: 0.8803999423980713\n",
      "WARNING:tensorflow:From <ipython-input-6-042fed3bc879>:42: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: .\\deep_learn\\jd_deep_learn\\jd_cnn_runout\\jd_comment_cnn_serving3\\saved_model.pb\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "train_keep_prob_value = 0.8\n",
    "test_keep_prob_value = 1.0\n",
    "\n",
    "test_steps = 100\n",
    "num_train_steps = 10000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for i in range(num_train_steps):\n",
    "        batch_inputs, batch_labels = train_data.next_batch(hps.batch_size)\n",
    "        outputs_val = sess.run([loss, accuracy, train_op, global_step],\n",
    "                               feed_dict={\n",
    "                                   inputs: batch_inputs,\n",
    "                                   outputs: batch_labels,\n",
    "                                   keep_prod:train_keep_prob_value,\n",
    "                                   })\n",
    "        loss_val, accuracy_val, _, global_step_val = outputs_val\n",
    "        if (i+1) % 100 == 0:\n",
    "            tf.logging.info(\"Train Step: {}, loss: {}, accuracy: {}\".format(global_step_val, loss_val, accuracy_val))\n",
    "    \n",
    "        if (i+1) % 1000 == 0:\n",
    "            all_test_acc_cal = []\n",
    "            for j in range(test_steps):\n",
    "                test_inputs, test_labels = test_data.next_batch(hps.batch_size)\n",
    "                test_val = sess.run([loss, accuracy, train_op, global_step],\n",
    "                                    feed_dict= {\n",
    "                                        inputs: test_inputs,\n",
    "                                        outputs: test_labels,\n",
    "                                        keep_prod: test_keep_prob_value,\n",
    "                                    })\n",
    "                test_loss_val, test_accuarcy_val, _, test_step_val = test_val\n",
    "                all_test_acc_cal.append(test_accuarcy_val)\n",
    "            test_acc = np.mean(all_test_acc_cal)\n",
    "            saver.save(sess, os.path.join(model_dir, 'ckp-{}'.format(i+1)))\n",
    "            tf.logging.info(\"Test Step: {}, loss: {}, accuracy: {}\".format(global_step_val, test_loss_val, test_acc))\n",
    "    # save saved_model\n",
    "    # path = r'.\\deep_learn\\jd_deep_learn\\jd_lstm_runout\\model\\jd_comment_cnn_serving'\n",
    "    # builder =\n",
    "    builder = tf.saved_model.builder.SavedModelBuilder(r'.\\deep_learn\\jd_deep_learn\\jd_cnn_runout\\jd_comment_cnn_serving3')\n",
    "    input = {\n",
    "        'inputs': tf.saved_model.utils.build_tensor_info(inputs), \n",
    "            'keep_prob': tf.saved_model.utils.build_tensor_info(keep_prod)\n",
    "             }\n",
    "    sigs = {}\n",
    "    output = {'outputs': tf.saved_model.utils.build_tensor_info(sess.graph.get_tensor_by_name('metrics/ArgMax:0'))}\n",
    "    # sigs[tf.saved_model.tag_constants.]\n",
    "    signature = tf.saved_model.signature_def_utils.build_signature_def(input, output, \n",
    "                                                                       method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "    builder.add_meta_graph_and_variables(sess, tags=[tf.saved_model.tag_constants.SERVING],\n",
    "                                         signature_def_map={'jd_comment_cnn_predict': signature})\n",
    "    builder.save()\n",
    "#     SignatureDef = sm.signature_def_utils.build_signature_def(\n",
    "#                             inputs={'input_1': X_TensorInfo, 'input_2': scale_TensorInfo},\n",
    "#                             outputs={'output': y_TensorInfo},\n",
    "#                             method_name='what'\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
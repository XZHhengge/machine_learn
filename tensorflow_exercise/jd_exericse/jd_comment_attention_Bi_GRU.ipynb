{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
    "from tensorflow.contrib.rnn import GRUCell\n",
    "import math\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        num_embedding_size = 32,\n",
    "        num_timesteps = 120,\n",
    "        num_gru_size = 64,\n",
    "        batch_size = 100,\n",
    "        clip_lstm_grads = 1.0,\n",
    "        num_fc_nodes = 64,\n",
    "        learning_rate = 0.001,\n",
    "        num_word_threshold = 10,\n",
    "        num_attention_size = 50,\n",
    "\n",
    "    )\n",
    "\n",
    "hps = get_default_params()\n",
    "\n",
    "train_file = r'.\\deep_learn\\jd_deep_learn\\train_data.tsv'\n",
    "test_file = r'.\\deep_learn\\jd_deep_learn\\test_data.tsv'\n",
    "\n",
    "seg_train_file = r'.\\deep_learn\\jd_deep_learn\\seg_train_data.txt'\n",
    "seg_test_file = r'.\\deep_learn\\jd_deep_learn\\seg_test_data.txt'\n",
    "\n",
    "vocab_padding_file = r'.\\deep_learn\\jd_deep_learn\\jd_padding_vocab.txt'\n",
    "category_file = r'.\\deep_learn\\jd_deep_learn\\jd_category.txt'\n",
    "output_dir = r'.\\deep_learn\\jd_deep_learn\\jd_attention_gru_runout'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "INFO:tensorflow:vocab_size: 4762\n",
      "category_to_id dict:{'5': 0, '1': 1, '2': 2, '3': 3}\n",
      "INFO:tensorflow:category_size: 4\n",
      "INFO:tensorflow:id:0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def attention(inputs, attention_size, time_major=False, return_alphas=False):\n",
    "    \"\"\"\n",
    "    Attention mechanism layer which reduces RNN/Bi-RNN outputs with Attention vector.\n",
    "\n",
    "    The idea was proposed in the article by Z. Yang et al., \"Hierarchical Attention Networks\n",
    "     for Document Classification\", 2016: http://www.aclweb.org/anthology/N16-1174.\n",
    "    Variables notation is also inherited from the article\n",
    "    \n",
    "    Args:\n",
    "        inputs: The Attention inputs.\n",
    "            Matches outputs of RNN/Bi-RNN layer (not final state):\n",
    "                In case of RNN, this must be RNN outputs `Tensor`:\n",
    "                    If time_major == False (default), this must be a tensor of shape:\n",
    "                        `[batch_size, max_time, cell.output_size]`.\n",
    "                    If time_major == True, this must be a tensor of shape:\n",
    "                        `[max_time, batch_size, cell.output_size]`.\n",
    "                In case of Bidirectional RNN, this must be a tuple (outputs_fw, outputs_bw) containing the forward and\n",
    "                the backward RNN outputs `Tensor`.\n",
    "                    If time_major == False (default),\n",
    "                        outputs_fw is a `Tensor` shaped:\n",
    "                        `[batch_size, max_time, cell_fw.output_size]`\n",
    "                        and outputs_bw is a `Tensor` shaped:\n",
    "                        `[batch_size, max_time, cell_bw.output_size]`.\n",
    "                    If time_major == True,\n",
    "                        outputs_fw is a `Tensor` shaped:\n",
    "                        `[max_time, batch_size, cell_fw.output_size]`\n",
    "                        and outputs_bw is a `Tensor` shaped:\n",
    "                        `[max_time, batch_size, cell_bw.output_size]`.\n",
    "        attention_size: Linear size of the Attention weights.\n",
    "        time_major: The shape format of the `inputs` Tensors.\n",
    "            If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
    "            If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
    "            Using `time_major = True` is a bit more efficient because it avoids\n",
    "            transposes at the beginning and end of the RNN calculation.  However,\n",
    "            most TensorFlow data is batch-major, so by default this function\n",
    "            accepts input and emits output in batch-major form.\n",
    "        return_alphas: Whether to return attention coefficients variable along with layer's output.\n",
    "            Used for visualization purpose.\n",
    "    Returns:\n",
    "        The Attention output `Tensor`.\n",
    "        In case of RNN, this will be a `Tensor` shaped:\n",
    "            `[batch_size, cell.output_size]`.\n",
    "        In case of Bidirectional RNN, this will be a `Tensor` shaped:\n",
    "            `[batch_size, cell_fw.output_size + cell_bw.output_size]`.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        inputs = tf.concat(inputs, 2)\n",
    "\n",
    "    if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
    "\n",
    "    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n",
    "\n",
    "    # Trainable parameters\n",
    "    w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
    "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "\n",
    "    with tf.name_scope('v'):\n",
    "        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n",
    "\n",
    "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape\n",
    "    alphas = tf.nn.softmax(vu, name='alphas')         # (B,T) shape\n",
    "\n",
    "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
    "\n",
    "    if not return_alphas:\n",
    "        return output\n",
    "    else:\n",
    "        return output, alphas\n",
    "\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, filename, num_word_threshold):\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._padding = 1\n",
    "        self._num_word_threshold = num_word_threshold\n",
    "        self._read_dict(filename)\n",
    "        \n",
    "    def _read_dict(self, filename):\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, frequency = line.strip('\\r\\n').split('\\t')\n",
    "            frequency = int(frequency)\n",
    "            if frequency < self._num_word_threshold:\n",
    "                continue\n",
    "            idx = len(self._word_to_id)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            self._word_to_id[word] = idx\n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self._unk)\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    @property\n",
    "    def padding(self):\n",
    "        return self._padding\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "    \n",
    "    def sentence_to_id(self, sentence):\n",
    "        \n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence.split()]\n",
    "        return word_ids\n",
    "class CategoryDict:\n",
    "    def __init__(self, filename):\n",
    "        self._category_to_id = {}\n",
    "        with open(filename, 'r',encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            category = line.strip('\\r\\n')\n",
    "            idx = len(self._category_to_id)\n",
    "            self._category_to_id[category] = idx\n",
    "    def size(self):\n",
    "        print('category_to_id dict:{}'.format(self._category_to_id))\n",
    "        return len(self._category_to_id)\n",
    "    \n",
    "    def category_to_id(self, category):\n",
    "        if not category in self._category_to_id:\n",
    "            raise Exception(\"{} is not in our category\".format(category))\n",
    "        \n",
    "        return self._category_to_id[category]\n",
    "    \n",
    "          \n",
    "vocab = Vocab(vocab_padding_file, hps.num_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "\n",
    "tf.logging.info('vocab_size: {}'.format(vocab_size))\n",
    "category_vocab = CategoryDict(category_file)\n",
    "num_classes = category_vocab.size()\n",
    "tf.logging.info('category_size: {}'.format(num_classes))\n",
    "test_str = '5'\n",
    "tf.logging.info('id:{}'.format(category_vocab.category_to_id(test_str)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Loading data from .\\deep_learn\\jd_deep_learn\\seg_train_data.txt\n",
      "INFO:tensorflow:Loading data from .\\deep_learn\\jd_deep_learn\\seg_test_data.txt\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "class TextDataSet:\n",
    "    def __init__(self, filename, vocab, category_vocab, num_timesteps):\n",
    "        self._vocab = vocab\n",
    "        self._category_vocab = category_vocab\n",
    "        self._num_timesteps = num_timesteps\n",
    "        \n",
    "        self._inputs = []\n",
    "        self._outputs = []\n",
    "        \n",
    "        self._indicator = 0\n",
    "        self._parse_file(filename)\n",
    "        \n",
    "    def _parse_file(self, filename):\n",
    "        tf.logging.info('Loading data from {}'.format(filename))\n",
    "        import csv\n",
    "        csv_reader = csv.reader(open(filename, encoding='utf-8'))\n",
    "        for row in csv_reader:\n",
    "            label, content = row[0].replace('\\n', '').split('\\t')\n",
    "            id_label = self._category_vocab.category_to_id(label) # 标签转number\n",
    "            # print(id_label)\n",
    "            id_words = self._vocab.sentence_to_id(content) # 文字转number\n",
    "            # print(id_words)\n",
    "            \n",
    "            id_words = id_words[0: self._num_timesteps] # 过长截断\n",
    "            padding_num = self._num_timesteps - len(id_words) # 过短padding\n",
    "            id_words = id_words + [self._vocab.padding for _ in range(padding_num)]\n",
    "            self._inputs.append(id_words)\n",
    "            self._outputs.append(id_label)\n",
    "            \n",
    "        self._inputs = np.asarray(self._inputs, dtype=np.int32)\n",
    "        self._outputs = np.asarray(self._outputs, dtype=np.int32)\n",
    "        self._random_shuffle()\n",
    "        \n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(len(self._inputs))\n",
    "        self._inputs = self._inputs[p]\n",
    "        self._outputs = self._outputs[p]\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            raise Exception(\"batch size: {} is too large\".format(batch_size))\n",
    "        \n",
    "        batch_inputs = self._inputs[self._indicator: end_indicator]\n",
    "        batch_output = self._outputs[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_inputs, batch_output\n",
    "\n",
    "train_dataset = TextDataSet(seg_train_file, vocab, category_vocab, hps.num_timesteps)\n",
    "test_dataset = TextDataSet(seg_test_file, vocab, category_vocab, hps.num_timesteps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "35\n",
      "17\n",
      "33\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# print(train_dataset.next_batch(3)[0])\n",
    "for i in train_dataset.next_batch(3)[0].tolist():\n",
    "    \n",
    "    if 1 in i:\n",
    "        print(i.index(1))\n",
    "    else:\n",
    "        print(120)\n",
    "\n",
    "# print(np.array([list(x).index(1)+1 if 1 in x else hps.num_timesteps for x in train_dataset.next_batch(3)]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-6-7dac2a5902a6>:25: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-6-7dac2a5902a6>:29: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-7dac2a5902a6>:32: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From <ipython-input-6-7dac2a5902a6>:41: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-6-7dac2a5902a6>:47: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-7dac2a5902a6>:48: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-7dac2a5902a6>:60: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.math.argmax` instead\n",
      "INFO:tensorflow:variable name: embedding/embedding:0\n",
      "INFO:tensorflow:variable name: gru/bidirectional_rnn/fw/gru_cell/gates/kernel:0\n",
      "INFO:tensorflow:variable name: gru/bidirectional_rnn/fw/gru_cell/gates/bias:0\n",
      "INFO:tensorflow:variable name: gru/bidirectional_rnn/fw/gru_cell/candidate/kernel:0\n",
      "INFO:tensorflow:variable name: gru/bidirectional_rnn/fw/gru_cell/candidate/bias:0\n",
      "INFO:tensorflow:variable name: gru/bidirectional_rnn/bw/gru_cell/gates/kernel:0\n",
      "INFO:tensorflow:variable name: gru/bidirectional_rnn/bw/gru_cell/gates/bias:0\n",
      "INFO:tensorflow:variable name: gru/bidirectional_rnn/bw/gru_cell/candidate/kernel:0\n",
      "INFO:tensorflow:variable name: gru/bidirectional_rnn/bw/gru_cell/candidate/bias:0\n",
      "INFO:tensorflow:variable name: attention/Variable:0\n",
      "INFO:tensorflow:variable name: attention/Variable_1:0\n",
      "INFO:tensorflow:variable name: attention/Variable_2:0\n",
      "INFO:tensorflow:variable name: fc/fc1/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc1/bias:0\n",
      "INFO:tensorflow:variable name: fc/fc2/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc2/bias:0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def create_model(hps, vocab_size, num_classes):\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "    \n",
    "    inputs = tf.placeholder(tf.int32, (None, num_timesteps), name='inputs')\n",
    "    outputs = tf.placeholder(tf.int32, (None, ), name='outputs')\n",
    "    seq_len_ph = tf.placeholder(tf.int32, [None], name='seq_len_ph')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    global_step = tf.Variable(\n",
    "        tf.zeros([], tf.int64), name='global_step', trainable=False)\n",
    "    embedding_initalizer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    with tf.variable_scope(\n",
    "        'embedding', initializer=embedding_initalizer\n",
    "    ):\n",
    "        embeddings = tf.get_variable(\n",
    "            'embedding', [vocab_size+2, hps.num_embedding_size],\n",
    "            tf.float32\n",
    "        )\n",
    "        tf.summary.histogram('embeddings', embeddings)\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_size+hps.num_gru_size) / 3.0\n",
    "    gru_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    with tf.variable_scope('gru', initializer=gru_init):\n",
    "        rnn_outputs, _ = bi_rnn(GRUCell(hps.num_gru_size), \n",
    "                                GRUCell(hps.num_gru_size),\n",
    "                                inputs=embed_inputs, \n",
    "                                sequence_length=seq_len_ph, \n",
    "                                dtype=tf.float32)\n",
    "        \n",
    "        tf.summary.histogram('rnn_outputs', rnn_outputs)\n",
    "    attention_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    \n",
    "    with tf.variable_scope('attention', initializer=attention_init):\n",
    "        attention_output, alphas = \\\n",
    "            attention(rnn_outputs, \n",
    "                      hps.num_embedding_size, \n",
    "                      return_alphas=True)\n",
    "        tf.summary.histogram('alphas', alphas)\n",
    "        \n",
    "    drop = tf.nn.dropout(attention_output, keep_prob)\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('fc', initializer=fc_init):\n",
    "        fc1 = tf.layers.dense(drop,\n",
    "                              hps.num_fc_nodes,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name='fc1')\n",
    "        fc1_droput = tf.layers.dropout(fc1, keep_prob)\n",
    "        logits = tf.layers.dense(fc1_droput,\n",
    "                                 num_classes,\n",
    "                                 name='fc2')\n",
    "    with tf.name_scope('metrics'):\n",
    "        sofmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=outputs\n",
    "        )\n",
    "        loss = tf.reduce_mean(sofmax_loss)\n",
    "        \n",
    "        y_pre = tf.arg_max(tf.nn.softmax(logits=logits),\n",
    "                           1,\n",
    "                           output_type=tf.int32, name='y_pre')\n",
    "        correct_pred = tf.equal(outputs, y_pre)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    with tf.name_scope('train_op'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            tf.logging.info('variable name: {}'.format(var.name))\n",
    "            \n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(loss, tvars), hps.clip_lstm_grads\n",
    "        )\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=global_step)\n",
    "    \n",
    "    return ((inputs, outputs, keep_prob, seq_len_ph),\n",
    "            (loss, accuracy),\n",
    "            (train_op, global_step)\n",
    "            )\n",
    "\n",
    "placeholders, metrics, others = create_model(\n",
    "    hps, vocab_size, num_classes\n",
    ")\n",
    "inputs, outputs, keep_prod, seq_len_ph = placeholders\n",
    "loss, accuracy = metrics\n",
    "train_op, global_step = others"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\n",
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "\n",
    "megred_summary = tf.summary.merge_all()\n",
    "megred_summary_test = tf.summary.merge([loss_summary, accuracy_summary])\n",
    "\n",
    "LOG_DIR = r'.\\deep_learn\\jd_deep_learn\\jd_attention_gru_runout'\n",
    "run_label = 'run_jd_comment_attention_tensorboard'\n",
    "run_dir = os.path.join(LOG_DIR, run_label)\n",
    "if not os.path.exists(run_dir):\n",
    "    os.mkdir(run_dir)\n",
    "train_log_dir = os.path.join(run_dir, 'train')\n",
    "test_log_dir = os.path.join(run_dir, 'test')\n",
    "\n",
    "if not os.path.exists(train_log_dir):\n",
    "    os.mkdir(train_log_dir)\n",
    "\n",
    "if not os.path.exists(test_log_dir):\n",
    "    os.mkdir(test_log_dir)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Train Step: 99, loss: 1.116432785987854, accuracy: 0.5299999713897705\n",
      "INFO:tensorflow:Train Step: 199, loss: 1.0858169794082642, accuracy: 0.5699999928474426\n",
      "INFO:tensorflow:Train Step: 299, loss: 1.0871084928512573, accuracy: 0.4399999976158142\n",
      "INFO:tensorflow:Train Step: 399, loss: 0.9055508375167847, accuracy: 0.5699999928474426\n",
      "INFO:tensorflow:Train Step: 499, loss: 1.0280667543411255, accuracy: 0.6200000047683716\n",
      "INFO:tensorflow:Train Step: 599, loss: 0.8829067945480347, accuracy: 0.6200000047683716\n",
      "INFO:tensorflow:Train Step: 699, loss: 0.8730707764625549, accuracy: 0.6100000143051147\n",
      "INFO:tensorflow:Train Step: 799, loss: 0.8226014971733093, accuracy: 0.6000000238418579\n",
      "INFO:tensorflow:Train Step: 899, loss: 0.8157306909561157, accuracy: 0.6800000071525574\n",
      "INFO:tensorflow:Train Step: 999, loss: 0.807142972946167, accuracy: 0.6100000143051147\n",
      "INFO:tensorflow:Test Step: 999, loss: 0.8718658685684204, accuracy: 0.6102999448776245\n",
      "INFO:tensorflow:Train Step: 1099, loss: 0.8931964635848999, accuracy: 0.5699999928474426\n",
      "INFO:tensorflow:Train Step: 1199, loss: 0.9542104601860046, accuracy: 0.5099999904632568\n",
      "INFO:tensorflow:Train Step: 1299, loss: 1.049716591835022, accuracy: 0.49000000953674316\n",
      "INFO:tensorflow:Train Step: 1399, loss: 1.0539228916168213, accuracy: 0.6100000143051147\n",
      "INFO:tensorflow:Train Step: 1499, loss: 0.7550537586212158, accuracy: 0.6800000071525574\n",
      "INFO:tensorflow:Train Step: 1599, loss: 0.9052455425262451, accuracy: 0.5899999737739563\n",
      "INFO:tensorflow:Train Step: 1699, loss: 0.8800346255302429, accuracy: 0.5899999737739563\n",
      "INFO:tensorflow:Train Step: 1799, loss: 0.8264793157577515, accuracy: 0.6700000166893005\n",
      "INFO:tensorflow:Train Step: 1899, loss: 0.8345849514007568, accuracy: 0.5899999737739563\n",
      "INFO:tensorflow:Train Step: 1999, loss: 0.7553882002830505, accuracy: 0.6499999761581421\n",
      "INFO:tensorflow:Test Step: 1999, loss: 0.7953668832778931, accuracy: 0.618399977684021\n",
      "INFO:tensorflow:Train Step: 2099, loss: 0.9057052731513977, accuracy: 0.6299999952316284\n",
      "INFO:tensorflow:Train Step: 2199, loss: 0.8002768158912659, accuracy: 0.5699999928474426\n",
      "INFO:tensorflow:Train Step: 2299, loss: 0.6862151622772217, accuracy: 0.6800000071525574\n",
      "INFO:tensorflow:Train Step: 2399, loss: 0.7211925983428955, accuracy: 0.699999988079071\n",
      "INFO:tensorflow:Train Step: 2499, loss: 0.6426904797554016, accuracy: 0.7400000095367432\n",
      "INFO:tensorflow:Train Step: 2599, loss: 0.7792965769767761, accuracy: 0.6399999856948853\n",
      "INFO:tensorflow:Train Step: 2699, loss: 0.9092241525650024, accuracy: 0.5600000023841858\n",
      "INFO:tensorflow:Train Step: 2799, loss: 0.8772351741790771, accuracy: 0.6700000166893005\n",
      "INFO:tensorflow:Train Step: 2899, loss: 0.75225430727005, accuracy: 0.6200000047683716\n",
      "INFO:tensorflow:Train Step: 2999, loss: 0.8001318573951721, accuracy: 0.6800000071525574\n",
      "INFO:tensorflow:Test Step: 2999, loss: 0.6919316649436951, accuracy: 0.640999972820282\n",
      "INFO:tensorflow:Train Step: 3099, loss: 0.7996982336044312, accuracy: 0.6100000143051147\n",
      "INFO:tensorflow:Train Step: 3199, loss: 0.8117349147796631, accuracy: 0.6800000071525574\n",
      "INFO:tensorflow:Train Step: 3299, loss: 0.7695197463035583, accuracy: 0.6100000143051147\n",
      "INFO:tensorflow:Train Step: 3399, loss: 0.6924037337303162, accuracy: 0.7599999904632568\n",
      "INFO:tensorflow:Train Step: 3499, loss: 0.8279165029525757, accuracy: 0.6200000047683716\n",
      "INFO:tensorflow:Train Step: 3599, loss: 0.7061269283294678, accuracy: 0.7400000095367432\n",
      "INFO:tensorflow:Train Step: 3699, loss: 0.7131454348564148, accuracy: 0.6499999761581421\n",
      "INFO:tensorflow:Train Step: 3799, loss: 0.7138917446136475, accuracy: 0.6800000071525574\n",
      "INFO:tensorflow:Train Step: 3899, loss: 0.821562647819519, accuracy: 0.699999988079071\n",
      "INFO:tensorflow:Train Step: 3999, loss: 0.6699310541152954, accuracy: 0.7200000286102295\n",
      "INFO:tensorflow:Test Step: 3999, loss: 0.6778253316879272, accuracy: 0.650399923324585\n",
      "INFO:tensorflow:Train Step: 4099, loss: 0.7242854237556458, accuracy: 0.699999988079071\n",
      "INFO:tensorflow:Train Step: 4199, loss: 0.7551703453063965, accuracy: 0.6299999952316284\n",
      "INFO:tensorflow:Train Step: 4299, loss: 0.7570738196372986, accuracy: 0.6800000071525574\n",
      "INFO:tensorflow:Train Step: 4399, loss: 0.8165929317474365, accuracy: 0.6899999976158142\n",
      "INFO:tensorflow:Train Step: 4499, loss: 0.6918104290962219, accuracy: 0.6600000262260437\n",
      "INFO:tensorflow:Train Step: 4599, loss: 0.6638215780258179, accuracy: 0.75\n",
      "INFO:tensorflow:Train Step: 4699, loss: 0.84576815366745, accuracy: 0.6299999952316284\n",
      "INFO:tensorflow:Train Step: 4799, loss: 0.7027882933616638, accuracy: 0.6899999976158142\n",
      "INFO:tensorflow:Train Step: 4899, loss: 0.6934556365013123, accuracy: 0.6800000071525574\n",
      "INFO:tensorflow:Train Step: 4999, loss: 0.6100754737854004, accuracy: 0.7400000095367432\n",
      "INFO:tensorflow:Test Step: 4999, loss: 0.5173467993736267, accuracy: 0.6757000088691711\n",
      "INFO:tensorflow:Train Step: 5099, loss: 0.7477646470069885, accuracy: 0.6800000071525574\n",
      "INFO:tensorflow:Train Step: 5199, loss: 0.6940977573394775, accuracy: 0.699999988079071\n",
      "INFO:tensorflow:Train Step: 5299, loss: 0.8005044460296631, accuracy: 0.6600000262260437\n",
      "INFO:tensorflow:Train Step: 5399, loss: 0.8135650753974915, accuracy: 0.7099999785423279\n",
      "INFO:tensorflow:Train Step: 5499, loss: 0.746944785118103, accuracy: 0.6899999976158142\n",
      "INFO:tensorflow:Train Step: 5599, loss: 0.8463589549064636, accuracy: 0.6499999761581421\n",
      "INFO:tensorflow:Train Step: 5699, loss: 0.5613729357719421, accuracy: 0.7699999809265137\n",
      "INFO:tensorflow:Train Step: 5799, loss: 0.5883485078811646, accuracy: 0.7200000286102295\n",
      "INFO:tensorflow:Train Step: 5899, loss: 0.734792172908783, accuracy: 0.7300000190734863\n",
      "INFO:tensorflow:Train Step: 5999, loss: 0.49985697865486145, accuracy: 0.7900000214576721\n",
      "INFO:tensorflow:Test Step: 5999, loss: 0.6933602690696716, accuracy: 0.6743999719619751\n",
      "INFO:tensorflow:Train Step: 6099, loss: 0.7995091080665588, accuracy: 0.6399999856948853\n",
      "INFO:tensorflow:Train Step: 6199, loss: 0.4636910557746887, accuracy: 0.8199999928474426\n",
      "INFO:tensorflow:Train Step: 6299, loss: 0.6980401873588562, accuracy: 0.75\n",
      "INFO:tensorflow:Train Step: 6399, loss: 0.6796468496322632, accuracy: 0.7300000190734863\n",
      "INFO:tensorflow:Train Step: 6499, loss: 0.6338770389556885, accuracy: 0.7200000286102295\n",
      "INFO:tensorflow:Train Step: 6599, loss: 0.5771662592887878, accuracy: 0.75\n",
      "INFO:tensorflow:Train Step: 6699, loss: 0.619050920009613, accuracy: 0.7599999904632568\n",
      "INFO:tensorflow:Train Step: 6799, loss: 0.611091673374176, accuracy: 0.7200000286102295\n",
      "INFO:tensorflow:Train Step: 6899, loss: 0.6272640824317932, accuracy: 0.7400000095367432\n",
      "INFO:tensorflow:Train Step: 6999, loss: 0.5007942914962769, accuracy: 0.800000011920929\n",
      "INFO:tensorflow:Test Step: 6999, loss: 0.6318038105964661, accuracy: 0.7043001055717468\n",
      "INFO:tensorflow:Train Step: 7099, loss: 0.519112765789032, accuracy: 0.7799999713897705\n",
      "INFO:tensorflow:Train Step: 7199, loss: 0.7277597188949585, accuracy: 0.6499999761581421\n",
      "INFO:tensorflow:Train Step: 7299, loss: 0.48190170526504517, accuracy: 0.8299999833106995\n",
      "INFO:tensorflow:Train Step: 7399, loss: 0.6936956644058228, accuracy: 0.699999988079071\n",
      "INFO:tensorflow:Train Step: 7499, loss: 0.6066493988037109, accuracy: 0.7200000286102295\n",
      "INFO:tensorflow:Train Step: 7599, loss: 0.58307284116745, accuracy: 0.6800000071525574\n",
      "INFO:tensorflow:Train Step: 7699, loss: 0.6590173840522766, accuracy: 0.6899999976158142\n",
      "INFO:tensorflow:Train Step: 7799, loss: 0.5043099522590637, accuracy: 0.7900000214576721\n",
      "INFO:tensorflow:Train Step: 7899, loss: 0.6242625713348389, accuracy: 0.7599999904632568\n",
      "INFO:tensorflow:Train Step: 7999, loss: 0.4517950713634491, accuracy: 0.8500000238418579\n",
      "INFO:tensorflow:Test Step: 7999, loss: 0.549898624420166, accuracy: 0.714400053024292\n",
      "INFO:tensorflow:Train Step: 8099, loss: 0.4720527231693268, accuracy: 0.800000011920929\n",
      "INFO:tensorflow:Train Step: 8199, loss: 0.5549881458282471, accuracy: 0.75\n",
      "INFO:tensorflow:Train Step: 8299, loss: 0.5426268577575684, accuracy: 0.8199999928474426\n",
      "INFO:tensorflow:Train Step: 8399, loss: 0.5299609899520874, accuracy: 0.7799999713897705\n",
      "INFO:tensorflow:Train Step: 8499, loss: 0.5283132791519165, accuracy: 0.7900000214576721\n",
      "INFO:tensorflow:Train Step: 8599, loss: 0.41441524028778076, accuracy: 0.8299999833106995\n",
      "INFO:tensorflow:Train Step: 8699, loss: 0.5637874007225037, accuracy: 0.8199999928474426\n",
      "INFO:tensorflow:Train Step: 8799, loss: 0.49189502000808716, accuracy: 0.800000011920929\n",
      "INFO:tensorflow:Train Step: 8899, loss: 0.5431444048881531, accuracy: 0.7200000286102295\n",
      "INFO:tensorflow:Train Step: 8999, loss: 0.5486741065979004, accuracy: 0.75\n",
      "INFO:tensorflow:Test Step: 8999, loss: 0.4526493549346924, accuracy: 0.7367998361587524\n",
      "INFO:tensorflow:Train Step: 9099, loss: 0.3721284866333008, accuracy: 0.8899999856948853\n",
      "INFO:tensorflow:Train Step: 9199, loss: 0.5307245254516602, accuracy: 0.7900000214576721\n",
      "INFO:tensorflow:Train Step: 9299, loss: 0.3638274073600769, accuracy: 0.8500000238418579\n",
      "INFO:tensorflow:Train Step: 9399, loss: 0.38075101375579834, accuracy: 0.8399999737739563\n",
      "INFO:tensorflow:Train Step: 9499, loss: 0.29969164729118347, accuracy: 0.8899999856948853\n",
      "INFO:tensorflow:Train Step: 9599, loss: 0.41651996970176697, accuracy: 0.8100000023841858\n",
      "INFO:tensorflow:Train Step: 9699, loss: 0.4311237037181854, accuracy: 0.7900000214576721\n",
      "INFO:tensorflow:Train Step: 9799, loss: 0.3678377568721771, accuracy: 0.8399999737739563\n",
      "INFO:tensorflow:Train Step: 9899, loss: 0.3682524859905243, accuracy: 0.8299999833106995\n",
      "INFO:tensorflow:Train Step: 9999, loss: 0.4766046106815338, accuracy: 0.7900000214576721\n",
      "INFO:tensorflow:Test Step: 9999, loss: 0.5626447200775146, accuracy: 0.7417000532150269\n",
      "WARNING:tensorflow:From <ipython-input-8-43bad1abd48c>:71: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-43bad1abd48c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;34m'seq_len_ph'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_tensor_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq_len_ph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m              }\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'outputs'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_tensor_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'y_pre:0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m     \u001b[1;31m# signature = tf.saved_model.signature_def_utils.build_signature_def(input, output, 'jd_comment_lstm')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     signature_def_map = {'jd_comment_attention_gru_predict':tf.saved_model.signature_def_utils.build_signature_def(\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mget_tensor_by_name\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   3652\u001b[0m       raise TypeError(\"Tensor names are strings (or similar), not %s.\" %\n\u001b[0;32m   3653\u001b[0m                       type(name).__name__)\n\u001b[1;32m-> 3654\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_graph_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3655\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3656\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_get_tensor_by_tf_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3477\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3478\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3480\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3518\u001b[0m           raise KeyError(\"The name %s refers to a Tensor which does not \"\n\u001b[0;32m   3519\u001b[0m                          \u001b[1;34m\"exist. The operation, %s, does not exist in the \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3520\u001b[1;33m                          \"graph.\" % (repr(name), repr(op_name)))\n\u001b[0m\u001b[0;32m   3521\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3522\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mout_n\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"The name 'y_pre:0' refers to a Tensor which does not exist. The operation, 'y_pre', does not exist in the graph.\""
     ],
     "ename": "KeyError",
     "evalue": "\"The name 'y_pre:0' refers to a Tensor which does not exist. The operation, 'y_pre', does not exist in the graph.\"",
     "output_type": "error"
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "train_keep_prob_value = 0.8\n",
    "test_keep_prob_value = 1.0\n",
    "\n",
    "test_steps = 100\n",
    "num_train_steps = 10000\n",
    "output_summary_every_steps = 100\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    train_writer = tf.summary.FileWriter(train_log_dir, sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(test_log_dir)\n",
    "    \n",
    "    for i in range(num_train_steps):\n",
    "        batch_inputs, batch_labels = train_dataset.next_batch(\n",
    "            hps.batch_size\n",
    "        )\n",
    "        eval_ops = [loss, accuracy, train_op, global_step]\n",
    "        should_output_summary = ((i+1) % output_summary_every_steps == 0)\n",
    "        if should_output_summary:\n",
    "            eval_ops.append(megred_summary)\n",
    "        # print(batch_inputs)\n",
    "        # print(batch_inputs.tolist())\n",
    "        seq_len = np.array([x.index(1)+1 if 1 in x else hps.num_timesteps for x in batch_inputs.tolist()])\n",
    "        # print(seq_len.shape)\n",
    "        # [loss, accuracy, train_op, global_step, mergred_summary]\n",
    "        outputs_val = sess.run(eval_ops,\n",
    "                               feed_dict = {\n",
    "                                inputs: batch_inputs,\n",
    "                                outputs: batch_labels,\n",
    "                                   keep_prod: train_keep_prob_value,\n",
    "                                   seq_len_ph: seq_len\n",
    "                               })\n",
    "        loss_val, accuracy_val, = outputs_val[0:2]\n",
    "        if should_output_summary:\n",
    "            train_summary_str = outputs_val[-1]\n",
    "            train_writer.add_summary(train_summary_str, i+1)\n",
    "            test_summarys_str = sess.run([megred_summary_test],\n",
    "                                         feed_dict={\n",
    "                                             inputs: batch_inputs,\n",
    "                                            outputs: batch_labels,\n",
    "                                            keep_prod: train_keep_prob_value,\n",
    "                                                seq_len_ph: seq_len \n",
    "                                         })[0]\n",
    "            test_writer.add_summary(test_summarys_str, i+1)\n",
    "        if (i+1) % 100 == 0:\n",
    "            tf.logging.info(\"Train Step: {}, loss: {}, accuracy: {}\".format(i, loss_val, accuracy_val))\n",
    "\n",
    "        if (i+1) % 1000 == 0:\n",
    "            all_test_acc_cal = []\n",
    "            for j in range(test_steps):\n",
    "                test_inputs, test_labels = test_dataset.next_batch(hps.batch_size)\n",
    "                seq_len = np.array([list(x).index(1)+1 if 1 in x else hps.num_timesteps for x in test_inputs])\n",
    "                test_val = sess.run([loss, accuracy, train_op, global_step],\n",
    "                # test_val = sess.run([loss, accuracy, optimzer],\n",
    "                                    feed_dict= {\n",
    "                                        inputs: test_inputs,\n",
    "                                        outputs: test_labels,\n",
    "                                        seq_len_ph: seq_len,\n",
    "                                        keep_prod: test_keep_prob_value,\n",
    "                                    })\n",
    "                test_loss_val, test_accuarcy_val, _, test_step_val = test_val\n",
    "                # test_loss_val, test_accuarcy_val, optimzer = test_val\n",
    "                all_test_acc_cal.append(test_accuarcy_val)\n",
    "            test_acc = np.mean(all_test_acc_cal)\n",
    "            tf.logging.info(\"Test Step: {}, loss: {}, accuracy: {}\".format(i, test_loss_val, test_acc))\n",
    "    builder = tf.saved_model.builder.SavedModelBuilder(r'.\\deep_learn\\jd_deep_learn\\jd_attention_gru_runout\\1')\n",
    "    input = {\n",
    "        'inputs': tf.saved_model.utils.build_tensor_info(inputs), \n",
    "            'keep_prob': tf.saved_model.utils.build_tensor_info(keep_prod),\n",
    "        'seq_len_ph': tf.saved_model.utils.build_tensor_info(seq_len_ph)\n",
    "             }\n",
    "    for j in sess.graph.as_graph_def().node:\n",
    "        print(j.name)\n",
    "    output = {'outputs': tf.saved_model.utils.build_tensor_info(sess.graph.get_tensor_by_name('y_pre'))}\n",
    "    # signature = tf.saved_model.signature_def_utils.build_signature_def(input, output, 'jd_comment_lstm')\n",
    "    signature_def_map = {'jd_comment_attention_gru_predict':tf.saved_model.signature_def_utils.build_signature_def(\n",
    "        inputs=input, outputs=output,method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n",
    "    )}\n",
    "    builder.add_meta_graph_and_variables(sess, tags=[tf.saved_model.tag_constants.SERVING],\n",
    "                                         signature_def_map=signature_def_map)\n",
    "    builder.save()\n",
    "    print('builder done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
    "from tensorflow.contrib.rnn import GRUCell\n",
    "import math\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        num_embedding_size = 250,\n",
    "        num_timesteps = 150,\n",
    "        num_gru_size = 128,\n",
    "        batch_size = 100,\n",
    "        clip_lstm_grads = 1.0,\n",
    "        num_fc_nodes = 128,\n",
    "        learning_rate = 0.001,\n",
    "        num_word_threshold = 10,\n",
    "        num_attention_size = 150,\n",
    "\n",
    "    )\n",
    "\n",
    "hps = get_default_params()\n",
    "\n",
    "train_file = r'.\\deep_learn\\jd_deep_learn\\train_data.tsv'\n",
    "test_file = r'.\\deep_learn\\jd_deep_learn\\test_data.tsv'\n",
    "\n",
    "seg_train_file = r'.\\deep_learn\\jd_deep_learn\\seg_train_data.txt'\n",
    "seg_test_file = r'.\\deep_learn\\jd_deep_learn\\seg_test_data.txt'\n",
    "\n",
    "vocab_padding_file = r'.\\deep_learn\\jd_deep_learn\\jd_padding_vocab.txt'\n",
    "category_file = r'.\\deep_learn\\jd_deep_learn\\jd_category.txt'\n",
    "output_dir = r'.\\deep_learn\\jd_deep_learn\\jd_attention_gru_runout'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "INFO:tensorflow:vocab_size: 4762\n",
      "category_to_id dict:{'5': 0, '1': 1, '2': 2, '3': 3}\n",
      "INFO:tensorflow:category_size: 4\n",
      "INFO:tensorflow:id:0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def attention(inputs, attention_size, time_major=False, return_alphas=False):\n",
    "    \"\"\"\n",
    "    Attention mechanism layer which reduces RNN/Bi-RNN outputs with Attention vector.\n",
    "\n",
    "    The idea was proposed in the article by Z. Yang et al., \"Hierarchical Attention Networks\n",
    "     for Document Classification\", 2016: http://www.aclweb.org/anthology/N16-1174.\n",
    "    Variables notation is also inherited from the article\n",
    "    \n",
    "    Args:\n",
    "        inputs: The Attention inputs.\n",
    "            Matches outputs of RNN/Bi-RNN layer (not final state):\n",
    "                In case of RNN, this must be RNN outputs `Tensor`:\n",
    "                    If time_major == False (default), this must be a tensor of shape:\n",
    "                        `[batch_size, max_time, cell.output_size]`.\n",
    "                    If time_major == True, this must be a tensor of shape:\n",
    "                        `[max_time, batch_size, cell.output_size]`.\n",
    "                In case of Bidirectional RNN, this must be a tuple (outputs_fw, outputs_bw) containing the forward and\n",
    "                the backward RNN outputs `Tensor`.\n",
    "                    If time_major == False (default),\n",
    "                        outputs_fw is a `Tensor` shaped:\n",
    "                        `[batch_size, max_time, cell_fw.output_size]`\n",
    "                        and outputs_bw is a `Tensor` shaped:\n",
    "                        `[batch_size, max_time, cell_bw.output_size]`.\n",
    "                    If time_major == True,\n",
    "                        outputs_fw is a `Tensor` shaped:\n",
    "                        `[max_time, batch_size, cell_fw.output_size]`\n",
    "                        and outputs_bw is a `Tensor` shaped:\n",
    "                        `[max_time, batch_size, cell_bw.output_size]`.\n",
    "        attention_size: Linear size of the Attention weights.\n",
    "        time_major: The shape format of the `inputs` Tensors.\n",
    "            If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
    "            If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
    "            Using `time_major = True` is a bit more efficient because it avoids\n",
    "            transposes at the beginning and end of the RNN calculation.  However,\n",
    "            most TensorFlow data is batch-major, so by default this function\n",
    "            accepts input and emits output in batch-major form.\n",
    "        return_alphas: Whether to return attention coefficients variable along with layer's output.\n",
    "            Used for visualization purpose.\n",
    "    Returns:\n",
    "        The Attention output `Tensor`.\n",
    "        In case of RNN, this will be a `Tensor` shaped:\n",
    "            `[batch_size, cell.output_size]`.\n",
    "        In case of Bidirectional RNN, this will be a `Tensor` shaped:\n",
    "            `[batch_size, cell_fw.output_size + cell_bw.output_size]`.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        inputs = tf.concat(inputs, 2)\n",
    "\n",
    "    if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
    "\n",
    "    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n",
    "\n",
    "    # Trainable parameters\n",
    "    w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
    "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "\n",
    "    with tf.name_scope('v'):\n",
    "        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n",
    "\n",
    "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape\n",
    "    alphas = tf.nn.softmax(vu, name='alphas')         # (B,T) shape\n",
    "\n",
    "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
    "\n",
    "    if not return_alphas:\n",
    "        return output\n",
    "    else:\n",
    "        return output, alphas\n",
    "\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, filename, num_word_threshold):\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._padding = 1\n",
    "        self._num_word_threshold = num_word_threshold\n",
    "        self._read_dict(filename)\n",
    "        \n",
    "    def _read_dict(self, filename):\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, frequency = line.strip('\\r\\n').split('\\t')\n",
    "            frequency = int(frequency)\n",
    "            if frequency < self._num_word_threshold:\n",
    "                continue\n",
    "            idx = len(self._word_to_id)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            self._word_to_id[word] = idx\n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self._unk)\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    @property\n",
    "    def padding(self):\n",
    "        return self._padding\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "    \n",
    "    def sentence_to_id(self, sentence):\n",
    "        \n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence.split()]\n",
    "        return word_ids\n",
    "class CategoryDict:\n",
    "    def __init__(self, filename):\n",
    "        self._category_to_id = {}\n",
    "        with open(filename, 'r',encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            category = line.strip('\\r\\n')\n",
    "            idx = len(self._category_to_id)\n",
    "            self._category_to_id[category] = idx\n",
    "    def size(self):\n",
    "        print('category_to_id dict:{}'.format(self._category_to_id))\n",
    "        return len(self._category_to_id)\n",
    "    \n",
    "    def category_to_id(self, category):\n",
    "        if not category in self._category_to_id:\n",
    "            raise Exception(\"{} is not in our category\".format(category))\n",
    "        \n",
    "        return self._category_to_id[category]\n",
    "    \n",
    "          \n",
    "vocab = Vocab(vocab_padding_file, hps.num_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "\n",
    "tf.logging.info('vocab_size: {}'.format(vocab_size))\n",
    "category_vocab = CategoryDict(category_file)\n",
    "num_classes = category_vocab.size()\n",
    "tf.logging.info('category_size: {}'.format(num_classes))\n",
    "test_str = '5'\n",
    "tf.logging.info('id:{}'.format(category_vocab.category_to_id(test_str)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Loading data from .\\deep_learn\\jd_deep_learn\\seg_train_data.txt\n",
      "INFO:tensorflow:Loading data from .\\deep_learn\\jd_deep_learn\\seg_test_data.txt\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "class TextDataSet:\n",
    "    def __init__(self, filename, vocab, category_vocab, num_timesteps):\n",
    "        self._vocab = vocab\n",
    "        self._category_vocab = category_vocab\n",
    "        self._num_timesteps = num_timesteps\n",
    "        \n",
    "        self._inputs = []\n",
    "        self._outputs = []\n",
    "        \n",
    "        self._indicator = 0\n",
    "        self._parse_file(filename)\n",
    "        \n",
    "    def _parse_file(self, filename):\n",
    "        tf.logging.info('Loading data from {}'.format(filename))\n",
    "        import csv\n",
    "        csv_reader = csv.reader(open(filename, encoding='utf-8'))\n",
    "        for row in csv_reader:\n",
    "            label, content = row[0].replace('\\n', '').split('\\t')\n",
    "            id_label = self._category_vocab.category_to_id(label) # 标签转number\n",
    "            # print(id_label)\n",
    "            id_words = self._vocab.sentence_to_id(content) # 文字转number\n",
    "            # print(id_words)\n",
    "            \n",
    "            id_words = id_words[0: self._num_timesteps] # 过长截断\n",
    "            padding_num = self._num_timesteps - len(id_words) # 过短padding\n",
    "            id_words = id_words + [self._vocab.padding for _ in range(padding_num)]\n",
    "            self._inputs.append(id_words)\n",
    "            self._outputs.append(id_label)\n",
    "            \n",
    "        self._inputs = np.asarray(self._inputs, dtype=np.int32)\n",
    "        self._outputs = np.asarray(self._outputs, dtype=np.int32)\n",
    "        self._random_shuffle()\n",
    "        \n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(len(self._inputs))\n",
    "        self._inputs = self._inputs[p]\n",
    "        self._outputs = self._outputs[p]\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            raise Exception(\"batch size: {} is too large\".format(batch_size))\n",
    "        \n",
    "        batch_inputs = self._inputs[self._indicator: end_indicator]\n",
    "        batch_output = self._outputs[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_inputs, batch_output\n",
    "\n",
    "train_dataset = TextDataSet(seg_train_file, vocab, category_vocab, hps.num_timesteps)\n",
    "test_dataset = TextDataSet(seg_test_file, vocab, category_vocab, hps.num_timesteps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "19\n",
      "57\n",
      "7\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# print(train_dataset.next_batch(3)[0])\n",
    "for i in train_dataset.next_batch(3)[0].tolist():\n",
    "    \n",
    "    if 1 in i:\n",
    "        print(i.index(1))\n",
    "    else:\n",
    "        print(120)\n",
    "\n",
    "# print(np.array([list(x).index(1)+1 if 1 in x else hps.num_timesteps for x in train_dataset.next_batch(3)]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-6-7dac2a5902a6>:25: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-6-7dac2a5902a6>:29: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-7dac2a5902a6>:32: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From <ipython-input-6-7dac2a5902a6>:41: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-6-7dac2a5902a6>:47: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-7dac2a5902a6>:48: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-7dac2a5902a6>:60: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.math.argmax` instead\n",
      "INFO:tensorflow:variable name: embedding/embedding:0\n",
      "INFO:tensorflow:variable name: gru/bidirectional_rnn/fw/gru_cell/gates/kernel:0\n",
      "INFO:tensorflow:variable name: gru/bidirectional_rnn/fw/gru_cell/gates/bias:0\n",
      "INFO:tensorflow:variable name: gru/bidirectional_rnn/fw/gru_cell/candidate/kernel:0\n",
      "INFO:tensorflow:variable name: gru/bidirectional_rnn/fw/gru_cell/candidate/bias:0\n",
      "INFO:tensorflow:variable name: gru/bidirectional_rnn/bw/gru_cell/gates/kernel:0\n",
      "INFO:tensorflow:variable name: gru/bidirectional_rnn/bw/gru_cell/gates/bias:0\n",
      "INFO:tensorflow:variable name: gru/bidirectional_rnn/bw/gru_cell/candidate/kernel:0\n",
      "INFO:tensorflow:variable name: gru/bidirectional_rnn/bw/gru_cell/candidate/bias:0\n",
      "INFO:tensorflow:variable name: attention/Variable:0\n",
      "INFO:tensorflow:variable name: attention/Variable_1:0\n",
      "INFO:tensorflow:variable name: attention/Variable_2:0\n",
      "INFO:tensorflow:variable name: fc/fc1/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc1/bias:0\n",
      "INFO:tensorflow:variable name: fc/fc2/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc2/bias:0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def create_model(hps, vocab_size, num_classes):\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "    \n",
    "    inputs = tf.placeholder(tf.int32, (None, num_timesteps), name='inputs')\n",
    "    outputs = tf.placeholder(tf.int32, (None, ), name='outputs')\n",
    "    seq_len_ph = tf.placeholder(tf.int32, [None], name='seq_len_ph')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    global_step = tf.Variable(\n",
    "        tf.zeros([], tf.int64), name='global_step', trainable=False)\n",
    "    embedding_initalizer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    with tf.variable_scope(\n",
    "        'embedding', initializer=embedding_initalizer\n",
    "    ):\n",
    "        embeddings = tf.get_variable(\n",
    "            'embedding', [vocab_size+2, hps.num_embedding_size],\n",
    "            tf.float32\n",
    "        )\n",
    "        tf.summary.histogram('embeddings', embeddings)\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_size+hps.num_gru_size) / 3.0\n",
    "    gru_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    with tf.variable_scope('gru', initializer=gru_init):\n",
    "        rnn_outputs, _ = bi_rnn(GRUCell(hps.num_gru_size), \n",
    "                                GRUCell(hps.num_gru_size),\n",
    "                                inputs=embed_inputs, \n",
    "                                sequence_length=seq_len_ph, \n",
    "                                dtype=tf.float32)\n",
    "        \n",
    "        tf.summary.histogram('rnn_outputs', rnn_outputs)\n",
    "    attention_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    \n",
    "    with tf.variable_scope('attention', initializer=attention_init):\n",
    "        attention_output, alphas = \\\n",
    "            attention(rnn_outputs, \n",
    "                      hps.num_embedding_size, \n",
    "                      return_alphas=True)\n",
    "        tf.summary.histogram('alphas', alphas)\n",
    "        \n",
    "    drop = tf.nn.dropout(attention_output, keep_prob)\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('fc', initializer=fc_init):\n",
    "        fc1 = tf.layers.dense(drop,\n",
    "                              hps.num_fc_nodes,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name='fc1')\n",
    "        fc1_droput = tf.layers.dropout(fc1, keep_prob)\n",
    "        logits = tf.layers.dense(fc1_droput,\n",
    "                                 num_classes,\n",
    "                                 name='fc2')\n",
    "    with tf.name_scope('metrics'):\n",
    "        sofmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=outputs\n",
    "        )\n",
    "        loss = tf.reduce_mean(sofmax_loss)\n",
    "        \n",
    "        y_pre = tf.arg_max(tf.nn.softmax(logits=logits),\n",
    "                           1,\n",
    "                           output_type=tf.int32, name='y_pre')\n",
    "        correct_pred = tf.equal(outputs, y_pre)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    with tf.name_scope('train_op'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            tf.logging.info('variable name: {}'.format(var.name))\n",
    "            \n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(loss, tvars), hps.clip_lstm_grads\n",
    "        )\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=global_step)\n",
    "    \n",
    "    return ((inputs, outputs, keep_prob, seq_len_ph),\n",
    "            (loss, accuracy),\n",
    "            (train_op, global_step)\n",
    "            )\n",
    "\n",
    "placeholders, metrics, others = create_model(\n",
    "    hps, vocab_size, num_classes\n",
    ")\n",
    "inputs, outputs, keep_prod, seq_len_ph = placeholders\n",
    "loss, accuracy = metrics\n",
    "train_op, global_step = others"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\n",
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "\n",
    "megred_summary = tf.summary.merge_all()\n",
    "megred_summary_test = tf.summary.merge([loss_summary, accuracy_summary])\n",
    "\n",
    "LOG_DIR = r'.\\deep_learn\\jd_deep_learn\\jd_attention_gru_runout'\n",
    "run_label = 'run_jd_comment_attention_tensorboard'\n",
    "run_dir = os.path.join(LOG_DIR, run_label)\n",
    "if not os.path.exists(run_dir):\n",
    "    os.mkdir(run_dir)\n",
    "train_log_dir = os.path.join(run_dir, 'train')\n",
    "test_log_dir = os.path.join(run_dir, 'test')\n",
    "\n",
    "if not os.path.exists(train_log_dir):\n",
    "    os.mkdir(train_log_dir)\n",
    "\n",
    "if not os.path.exists(test_log_dir):\n",
    "    os.mkdir(test_log_dir)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Train Step: 99, accuracy: 0.5, loss: 1.0778677463531494\n",
      "INFO:tensorflow:Train Step: 199, accuracy: 0.47999998927116394, loss: 0.9918937683105469\n",
      "INFO:tensorflow:Train Step: 299, accuracy: 0.5799999833106995, loss: 0.8788570165634155\n",
      "INFO:tensorflow:Train Step: 399, accuracy: 0.6700000166893005, loss: 0.8147529363632202\n",
      "INFO:tensorflow:Train Step: 499, accuracy: 0.6399999856948853, loss: 0.8797489404678345\n",
      "INFO:tensorflow:Train Step: 599, accuracy: 0.6399999856948853, loss: 0.9102946519851685\n",
      "INFO:tensorflow:Train Step: 699, accuracy: 0.6200000047683716, loss: 0.8640000224113464\n",
      "INFO:tensorflow:Train Step: 799, accuracy: 0.550000011920929, loss: 0.7954970002174377\n",
      "INFO:tensorflow:Train Step: 899, accuracy: 0.5699999928474426, loss: 0.8540186882019043\n",
      "INFO:tensorflow:Train Step: 999, accuracy: 0.6299999952316284, loss: 0.7631736993789673\n",
      "INFO:tensorflow:Test Step: 999, accuracy: 0.6401000022888184,  loss: 0.6174926161766052\n",
      "INFO:tensorflow:Train Step: 1099, accuracy: 0.7599999904632568, loss: 0.6190456748008728\n",
      "INFO:tensorflow:Train Step: 1199, accuracy: 0.6700000166893005, loss: 0.8026280999183655\n",
      "INFO:tensorflow:Train Step: 1299, accuracy: 0.5600000023841858, loss: 0.9066229462623596\n",
      "INFO:tensorflow:Train Step: 1399, accuracy: 0.7300000190734863, loss: 0.5622537136077881\n",
      "INFO:tensorflow:Train Step: 1499, accuracy: 0.7900000214576721, loss: 0.6123208403587341\n",
      "INFO:tensorflow:Train Step: 1599, accuracy: 0.6399999856948853, loss: 0.7635080814361572\n",
      "INFO:tensorflow:Train Step: 1699, accuracy: 0.7200000286102295, loss: 0.6784964203834534\n",
      "INFO:tensorflow:Train Step: 1799, accuracy: 0.6600000262260437, loss: 0.7615168690681458\n",
      "INFO:tensorflow:Train Step: 1899, accuracy: 0.7099999785423279, loss: 0.6042300462722778\n",
      "INFO:tensorflow:Train Step: 1999, accuracy: 0.6800000071525574, loss: 0.6875830292701721\n",
      "INFO:tensorflow:Test Step: 1999, accuracy: 0.6888999342918396,  loss: 0.6905041337013245\n",
      "INFO:tensorflow:Train Step: 2099, accuracy: 0.7900000214576721, loss: 0.588303804397583\n",
      "INFO:tensorflow:Train Step: 2199, accuracy: 0.75, loss: 0.5824939608573914\n",
      "INFO:tensorflow:Train Step: 2299, accuracy: 0.7400000095367432, loss: 0.6173465847969055\n",
      "INFO:tensorflow:Train Step: 2399, accuracy: 0.699999988079071, loss: 0.5685898065567017\n",
      "INFO:tensorflow:Train Step: 2499, accuracy: 0.800000011920929, loss: 0.5465881824493408\n",
      "INFO:tensorflow:Train Step: 2599, accuracy: 0.8399999737739563, loss: 0.4506032168865204\n",
      "INFO:tensorflow:Train Step: 2699, accuracy: 0.8100000023841858, loss: 0.60655277967453\n",
      "INFO:tensorflow:Train Step: 2799, accuracy: 0.75, loss: 0.5774027109146118\n",
      "INFO:tensorflow:Train Step: 2899, accuracy: 0.8100000023841858, loss: 0.4910975396633148\n",
      "INFO:tensorflow:Train Step: 2999, accuracy: 0.8600000143051147, loss: 0.3519982099533081\n",
      "INFO:tensorflow:Test Step: 2999, accuracy: 0.7391998767852783,  loss: 0.489601731300354\n",
      "INFO:tensorflow:Train Step: 3099, accuracy: 0.8399999737739563, loss: 0.38285550475120544\n",
      "INFO:tensorflow:Train Step: 3199, accuracy: 0.800000011920929, loss: 0.4661127030849457\n",
      "INFO:tensorflow:Train Step: 3299, accuracy: 0.8399999737739563, loss: 0.3477441668510437\n",
      "INFO:tensorflow:Train Step: 3399, accuracy: 0.9200000166893005, loss: 0.30800461769104004\n",
      "INFO:tensorflow:Train Step: 3499, accuracy: 0.8799999952316284, loss: 0.24744939804077148\n",
      "INFO:tensorflow:Train Step: 3599, accuracy: 0.8199999928474426, loss: 0.33600345253944397\n",
      "INFO:tensorflow:Train Step: 3699, accuracy: 0.9200000166893005, loss: 0.23111256957054138\n",
      "INFO:tensorflow:Train Step: 3799, accuracy: 0.8199999928474426, loss: 0.3761202096939087\n",
      "INFO:tensorflow:Train Step: 3899, accuracy: 0.9200000166893005, loss: 0.21470099687576294\n",
      "INFO:tensorflow:Train Step: 3999, accuracy: 0.9399999976158142, loss: 0.19626036286354065\n",
      "INFO:tensorflow:Test Step: 3999, accuracy: 0.7938998341560364,  loss: 0.22184330224990845\n",
      "INFO:tensorflow:Train Step: 4099, accuracy: 0.9100000262260437, loss: 0.24753937125205994\n",
      "INFO:tensorflow:Train Step: 4199, accuracy: 0.9100000262260437, loss: 0.20828641951084137\n",
      "INFO:tensorflow:Train Step: 4299, accuracy: 0.8799999952316284, loss: 0.2803110182285309\n",
      "INFO:tensorflow:Train Step: 4399, accuracy: 0.9300000071525574, loss: 0.16557204723358154\n",
      "INFO:tensorflow:Train Step: 4499, accuracy: 0.8799999952316284, loss: 0.27545011043548584\n",
      "INFO:tensorflow:Train Step: 4599, accuracy: 0.8899999856948853, loss: 0.29920458793640137\n",
      "INFO:tensorflow:Train Step: 4699, accuracy: 0.9200000166893005, loss: 0.1807847023010254\n",
      "INFO:tensorflow:Train Step: 4799, accuracy: 0.9300000071525574, loss: 0.166494220495224\n",
      "INFO:tensorflow:Train Step: 4899, accuracy: 0.9800000190734863, loss: 0.07464269548654556\n",
      "INFO:tensorflow:Train Step: 4999, accuracy: 0.9599999785423279, loss: 0.1551210731267929\n",
      "INFO:tensorflow:Test Step: 4999, accuracy: 0.8446000814437866,  loss: 0.46154868602752686\n",
      "INFO:tensorflow:Train Step: 5099, accuracy: 0.9100000262260437, loss: 0.22824259102344513\n",
      "INFO:tensorflow:Train Step: 5199, accuracy: 0.9399999976158142, loss: 0.10927310585975647\n",
      "INFO:tensorflow:Train Step: 5299, accuracy: 0.9399999976158142, loss: 0.1398220658302307\n",
      "INFO:tensorflow:Train Step: 5399, accuracy: 0.9300000071525574, loss: 0.16600848734378815\n",
      "INFO:tensorflow:Train Step: 5499, accuracy: 0.9300000071525574, loss: 0.15498359501361847\n",
      "INFO:tensorflow:Train Step: 5599, accuracy: 0.9700000286102295, loss: 0.18432645499706268\n",
      "INFO:tensorflow:Train Step: 5699, accuracy: 0.9800000190734863, loss: 0.17197252810001373\n",
      "INFO:tensorflow:Train Step: 5799, accuracy: 0.9300000071525574, loss: 0.26011258363723755\n",
      "INFO:tensorflow:Train Step: 5899, accuracy: 0.9700000286102295, loss: 0.0726727843284607\n",
      "INFO:tensorflow:Train Step: 5999, accuracy: 0.949999988079071, loss: 0.10625364631414413\n",
      "INFO:tensorflow:Test Step: 5999, accuracy: 0.8916001319885254,  loss: 0.24833598732948303\n",
      "INFO:tensorflow:Train Step: 6099, accuracy: 0.9399999976158142, loss: 0.1620492786169052\n",
      "INFO:tensorflow:Train Step: 6199, accuracy: 0.9700000286102295, loss: 0.08488865196704865\n",
      "INFO:tensorflow:Train Step: 6299, accuracy: 0.9900000095367432, loss: 0.06603984534740448\n",
      "INFO:tensorflow:Train Step: 6399, accuracy: 0.9700000286102295, loss: 0.09261374175548553\n",
      "INFO:tensorflow:Train Step: 6499, accuracy: 0.9200000166893005, loss: 0.16072769463062286\n",
      "INFO:tensorflow:Train Step: 6599, accuracy: 0.9700000286102295, loss: 0.05576746165752411\n",
      "INFO:tensorflow:Train Step: 6699, accuracy: 0.9599999785423279, loss: 0.06865683943033218\n",
      "INFO:tensorflow:Train Step: 6799, accuracy: 0.9399999976158142, loss: 0.10975156724452972\n",
      "INFO:tensorflow:Train Step: 6899, accuracy: 0.9700000286102295, loss: 0.07294896245002747\n",
      "INFO:tensorflow:Train Step: 6999, accuracy: 0.9800000190734863, loss: 0.076883964240551\n",
      "INFO:tensorflow:Test Step: 6999, accuracy: 0.9198998808860779,  loss: 0.12448802590370178\n",
      "INFO:tensorflow:Train Step: 7099, accuracy: 0.9700000286102295, loss: 0.05461084842681885\n",
      "INFO:tensorflow:Train Step: 7199, accuracy: 0.9800000190734863, loss: 0.06240905448794365\n",
      "INFO:tensorflow:Train Step: 7299, accuracy: 0.9700000286102295, loss: 0.07029359042644501\n",
      "INFO:tensorflow:Train Step: 7399, accuracy: 0.9599999785423279, loss: 0.08953696489334106\n",
      "INFO:tensorflow:Train Step: 7499, accuracy: 0.9900000095367432, loss: 0.027042604982852936\n",
      "INFO:tensorflow:Train Step: 7599, accuracy: 0.9300000071525574, loss: 0.15543106198310852\n",
      "INFO:tensorflow:Train Step: 7699, accuracy: 0.9700000286102295, loss: 0.09146406501531601\n",
      "INFO:tensorflow:Train Step: 7799, accuracy: 0.9399999976158142, loss: 0.14840838313102722\n",
      "INFO:tensorflow:Train Step: 7899, accuracy: 0.9800000190734863, loss: 0.03305203840136528\n",
      "INFO:tensorflow:Train Step: 7999, accuracy: 0.9900000095367432, loss: 0.05289468169212341\n",
      "INFO:tensorflow:Test Step: 7999, accuracy: 0.9424999356269836,  loss: 0.09903708100318909\n",
      "INFO:tensorflow:Train Step: 8099, accuracy: 1.0, loss: 0.022288430482149124\n",
      "INFO:tensorflow:Train Step: 8199, accuracy: 0.9200000166893005, loss: 0.12265878915786743\n",
      "INFO:tensorflow:Train Step: 8299, accuracy: 0.9700000286102295, loss: 0.06847761571407318\n",
      "INFO:tensorflow:Train Step: 8399, accuracy: 0.9800000190734863, loss: 0.04152035713195801\n",
      "INFO:tensorflow:Train Step: 8499, accuracy: 0.9599999785423279, loss: 0.10012473165988922\n",
      "INFO:tensorflow:Train Step: 8599, accuracy: 0.9900000095367432, loss: 0.035128723829984665\n",
      "INFO:tensorflow:Train Step: 8699, accuracy: 1.0, loss: 0.0187444519251585\n",
      "INFO:tensorflow:Train Step: 8799, accuracy: 0.949999988079071, loss: 0.11161279678344727\n",
      "INFO:tensorflow:Train Step: 8899, accuracy: 0.9900000095367432, loss: 0.02341434173285961\n",
      "INFO:tensorflow:Train Step: 8999, accuracy: 1.0, loss: 0.02238200232386589\n",
      "INFO:tensorflow:Test Step: 8999, accuracy: 0.9546999335289001,  loss: 0.03125690296292305\n",
      "INFO:tensorflow:Train Step: 9099, accuracy: 0.9900000095367432, loss: 0.027552857995033264\n",
      "INFO:tensorflow:Train Step: 9199, accuracy: 1.0, loss: 0.019192153587937355\n",
      "INFO:tensorflow:Train Step: 9299, accuracy: 0.9700000286102295, loss: 0.055813346058130264\n",
      "INFO:tensorflow:Train Step: 9399, accuracy: 0.9700000286102295, loss: 0.06158884987235069\n",
      "INFO:tensorflow:Train Step: 9499, accuracy: 0.9900000095367432, loss: 0.07578186690807343\n",
      "INFO:tensorflow:Train Step: 9599, accuracy: 0.9800000190734863, loss: 0.03367634117603302\n",
      "INFO:tensorflow:Train Step: 9699, accuracy: 0.9900000095367432, loss: 0.02279968187212944\n",
      "INFO:tensorflow:Train Step: 9799, accuracy: 0.9700000286102295, loss: 0.07229648530483246\n",
      "INFO:tensorflow:Train Step: 9899, accuracy: 0.9900000095367432, loss: 0.022493433207273483\n",
      "INFO:tensorflow:Train Step: 9999, accuracy: 0.9900000095367432, loss: 0.02770151197910309\n",
      "INFO:tensorflow:Test Step: 9999, accuracy: 0.9620999693870544,  loss: 0.08788622915744781\n",
      "WARNING:tensorflow:From <ipython-input-8-1116e991cf46>:71: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "Tensor(\"metrics/y_pre:0\", shape=(?,), dtype=int32)\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: .\\deep_learn\\jd_deep_learn\\jd_attention_gru_runout\\1\\saved_model.pb\n",
      "builder done\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "train_keep_prob_value = 0.8\n",
    "test_keep_prob_value = 1.0\n",
    "\n",
    "test_steps = 100\n",
    "num_train_steps = 10000\n",
    "output_summary_every_steps = 100\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    train_writer = tf.summary.FileWriter(train_log_dir, sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(test_log_dir)\n",
    "    \n",
    "    for i in range(num_train_steps):\n",
    "        batch_inputs, batch_labels = train_dataset.next_batch(\n",
    "            hps.batch_size\n",
    "        )\n",
    "        eval_ops = [loss, accuracy, train_op, global_step]\n",
    "        should_output_summary = ((i+1) % output_summary_every_steps == 0)\n",
    "        if should_output_summary:\n",
    "            eval_ops.append(megred_summary)\n",
    "        # print(batch_inputs)\n",
    "        # print(batch_inputs.tolist())\n",
    "        seq_len = np.array([x.index(1)+1 if 1 in x else hps.num_timesteps for x in batch_inputs.tolist()])\n",
    "        # print(seq_len.shape)\n",
    "        # [loss, accuracy, train_op, global_step, mergred_summary]\n",
    "        outputs_val = sess.run(eval_ops,\n",
    "                               feed_dict = {\n",
    "                                inputs: batch_inputs,\n",
    "                                outputs: batch_labels,\n",
    "                                   keep_prod: train_keep_prob_value,\n",
    "                                   seq_len_ph: seq_len\n",
    "                               })\n",
    "        loss_val, accuracy_val, = outputs_val[0:2]\n",
    "        if should_output_summary:\n",
    "            train_summary_str = outputs_val[-1]\n",
    "            train_writer.add_summary(train_summary_str, i+1)\n",
    "            test_summarys_str = sess.run([megred_summary_test],\n",
    "                                         feed_dict={\n",
    "                                             inputs: batch_inputs,\n",
    "                                            outputs: batch_labels,\n",
    "                                            keep_prod: train_keep_prob_value,\n",
    "                                                seq_len_ph: seq_len \n",
    "                                         })[0]\n",
    "            test_writer.add_summary(test_summarys_str, i+1)\n",
    "        if (i+1) % 100 == 0:\n",
    "            tf.logging.info(\"Train Step: {}, accuracy: {}, loss: {}\".format(i, accuracy_val, loss_val))\n",
    "\n",
    "        if (i+1) % 1000 == 0:\n",
    "            all_test_acc_cal = []\n",
    "            for j in range(test_steps):\n",
    "                test_inputs, test_labels = test_dataset.next_batch(hps.batch_size)\n",
    "                seq_len = np.array([list(x).index(1)+1 if 1 in x else hps.num_timesteps for x in test_inputs])\n",
    "                test_val = sess.run([loss, accuracy, train_op, global_step],\n",
    "                # test_val = sess.run([loss, accuracy, optimzer],\n",
    "                                    feed_dict= {\n",
    "                                        inputs: test_inputs,\n",
    "                                        outputs: test_labels,\n",
    "                                        seq_len_ph: seq_len,\n",
    "                                        keep_prod: test_keep_prob_value,\n",
    "                                    })\n",
    "                test_loss_val, test_accuarcy_val, _, test_step_val = test_val\n",
    "                # test_loss_val, test_accuarcy_val, optimzer = test_val\n",
    "                all_test_acc_cal.append(test_accuarcy_val)\n",
    "            test_acc = np.mean(all_test_acc_cal)\n",
    "            tf.logging.info(\"Test Step: {}, accuracy: {},  loss: {}\".format(i, test_acc,  test_loss_val))\n",
    "    builder = tf.saved_model.builder.SavedModelBuilder(r'.\\deep_learn\\jd_deep_learn\\jd_attention_gru_runout\\1')\n",
    "    input = {\n",
    "        'inputs': tf.saved_model.utils.build_tensor_info(inputs), \n",
    "            'keep_prob': tf.saved_model.utils.build_tensor_info(keep_prod),\n",
    "        'seq_len_ph': tf.saved_model.utils.build_tensor_info(seq_len_ph)\n",
    "             }\n",
    "    print(sess.graph.get_tensor_by_name('metrics/y_pre:0'))\n",
    "    output = {'outputs': tf.saved_model.utils.build_tensor_info(sess.graph.get_tensor_by_name('metrics/y_pre:0'))}\n",
    "    # signature = tf.saved_model.signature_def_utils.build_signature_def(input, output, 'jd_comment_lstm')\n",
    "    signature_def_map = {'jd_comment_attention_gru_predict':tf.saved_model.signature_def_utils.build_signature_def(\n",
    "        inputs=input, outputs=output,method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n",
    "    )}\n",
    "    builder.add_meta_graph_and_variables(sess, tags=[tf.saved_model.tag_constants.SERVING],\n",
    "                                         signature_def_map=signature_def_map)\n",
    "    builder.save()\n",
    "    print('builder done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "## 下面高参数 10K 次 train: 99.7%, Valid: 92.7%, Test: 93.2%\n",
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        num_embedding_size = 32,\n",
    "        # 一个句子取前50个分词\n",
    "        # num_timesteps = 50,\n",
    "        num_timesteps = 200,\n",
    "        # num_lstm_nodes = [32, 32],\n",
    "        num_lstm_nodes = [64, 64],\n",
    "        num_lstm_layers = 2,\n",
    "        # num_fc_nodes = 32,\n",
    "        num_fc_nodes = 64,\n",
    "        batch_size = 100,\n",
    "        clip_lstm_grads = 1.0,\n",
    "        learning_rate = 0.001,\n",
    "        num_word_threshold = 10,\n",
    "    )\n",
    "hps = get_default_params()\n",
    "\n",
    "seg_train_file = '.\\deep_learn\\sohu_seg_train_file.txt'\n",
    "seg_test_file = '.\\deep_learn\\sohu_seg_test_file.txt'\n",
    "\n",
    "vocab_file =  '.\\deep_learn\\sohu_vocab.txt'\n",
    "category_file = '.\\deep_learn\\sohu_category.txt'\n",
    "output_file = '.\\deep_learn\\sohu_run_text_run'\n",
    "\n",
    "if not os.path.exists(output_file):\n",
    "    os.mkdir(output_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "INFO:tensorflow:vocab_size: 85428\n",
      "INFO:tensorflow:category_size: 12\n",
      "INFO:tensorflow:id:11\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# api sentence_to_id的实现\n",
    "class Vocab:\n",
    "    def __init__(self, filename, num_word_threshold):\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._num_word_threshold = num_word_threshold\n",
    "        self._read_dict(filename)\n",
    "    \n",
    "    def _read_dict(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, frequency = line.strip('\\r\\n').split('\\t')\n",
    "            frequency = int(frequency)\n",
    "            if frequency < self._num_word_threshold:\n",
    "                continue\n",
    "            idx = len(self._word_to_id)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            self._word_to_id[word] = idx\n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self._unk)\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "    \n",
    "    def sentence_to_id(self, sentence):\n",
    "        # 分词后的句子用空格来隔开每一个词语,这里每一句话里的分词用id来表示\n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence.split()]\n",
    "        return word_ids\n",
    "\n",
    "class CategoryDict:\n",
    "    def __init__(self, filename):\n",
    "        self._category_to_id = {}\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            category = line.strip('\\r\\n')\n",
    "            idx = len(self._category_to_id)\n",
    "            self._category_to_id[category] = idx\n",
    "            \n",
    "    def size(self):\n",
    "        return len(self._category_to_id)\n",
    "    \n",
    "    def category_to_id(self, category):\n",
    "        if not category in self._category_to_id:\n",
    "            # print(self._category_to_id)\n",
    "            raise Exception(\"{} is not in our category\".format(category))\n",
    "            \n",
    "        return self._category_to_id[category]\n",
    "          \n",
    "vocab = Vocab(vocab_file, hps.num_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "\n",
    "tf.logging.info('vocab_size: {}'.format(vocab_size))\n",
    "category_vocab = CategoryDict(category_file)\n",
    "num_classes = category_vocab.size()\n",
    "tf.logging.info('category_size: {}'.format(num_classes))\n",
    "test_str = '女人'\n",
    "tf.logging.info('id:{}'.format(category_vocab.category_to_id(test_str)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Loading data from .\\deep_learn\\sohu_seg_train_file.txt\n",
      "INFO:tensorflow:Loading data from .\\deep_learn\\sohu_seg_test_file.txt\n",
      "(array([[  809,    39, 11753,   258,     0,  1437,  3073,  3067,     9,\n",
      "           39,     4,  3090,   922,    13,   272,  1347,  1797,    13,\n",
      "          209,    53,    78,   414,    64,    34,   105,   180,   272,\n",
      "         3541,   111,   146,    17,    46,     6,     6,    76,  2225,\n",
      "            9,   344,    20,    15,   202,    26, 31842,   146,     9,\n",
      "            4,   344,    20,    15,   202,    25,     1,     9,     4,\n",
      "            4,    18,   809,  2092,   141,  1800,  4546, 10592,  3090,\n",
      "         7723,    11, 12717, 28594,    12,  3067,     3,   519,    23,\n",
      "            4,   430,     2, 10439,  7806,     9,    39,     4, 13272,\n",
      "        10592,  3090,     3, 11753,     7,   241,    35,   171,  6886,\n",
      "          258,  4093,   679,     8,    72,  1860,    16,     9,   113,\n",
      "           46,     2,  1762, 20723,     1,  7806,     2,  1946,    21,\n",
      "         6441,     8,  9784,   809,  8967,   241,    20,     9,    15,\n",
      "          430,     2,    36, 13891,  1549,     3,  7806,     2,   298,\n",
      "            9, 10225,  1762,   476,    35,  2218, 33426,     3,   237,\n",
      "          237,   237,   467,   129,  3067,   110,     7,    80,  1585,\n",
      "           35,  2631,   305,    41,  5406,     9,   430,     1, 35406,\n",
      "         2718, 22299,  2229, 39361,  1844,    75,  5130,  6374, 12347,\n",
      "            3,   298,    17,   430,     1, 35406,     0, 73487,    14,\n",
      "         3623,  1438,   225,   725,  3890,    16,    47,  1437,  1339,\n",
      "          239,  9873,     1,  3347,    75, 50463,  7893, 32141,   353,\n",
      "            3,   298,     6,     6,   430,     1,  7806,  3860, 22443,\n",
      "         6881,  3347],\n",
      "       [  272,  1267,  4621,  2974, 28367,    11,    85,   153,   141,\n",
      "         1755,    12,   459,   168,  1412, 10234,     0,    85,   153,\n",
      "          141,    10, 20521,  1039,    26,    85,    34,    68,    59,\n",
      "           78,    53,   105,   153,   167,    71,    64,    68,    34,\n",
      "           71,    71,   141,    64,    71,    59,    78,    64,   154,\n",
      "           59,    25,     2, 20080,     1, 17314,     9,     4,   947,\n",
      "            9,     4,   952,     2,   179,     3,   134,  2681,    75,\n",
      "         4185,    40,    11,   814,  8231,   319,  1137,    12,     1,\n",
      "          148,   316,   454,    28,   776,     8,   584,     2,   956,\n",
      "            5,  8451,     5,   355,     5,   229,   195,   584,     2,\n",
      "         1824,  3276,    14,   624,     5,  1670,    54,  1345,     1,\n",
      "           10,   927,   219,    35,     2,   218,   439,     1,  1048,\n",
      "           47,  1451,    38,  5672,    11,  6772,    12,     3,   115,\n",
      "            1,   258,    38,   142,   172,    28, 18447,    85,   153,\n",
      "          141,     1,  8810,     2,    85,   153,   141,   173,    22,\n",
      "         2293,     5,  3244,     5,  3172,    14,  4001,     3,   307,\n",
      "          111,     5,   217,  3269, 35485,    85,   153,   141,     3,\n",
      "           11,   822,   115,   268,  1155,     0,    85,   153,   141,\n",
      "            2,  3293,     1,  4621,   168,  2333,   652,     5,   586,\n",
      "         2370,   219,  7985,   609,     1,     7,   268,   722,   239,\n",
      "           30,    85,   153,   141,   254,   152,   108,  2974, 28367,\n",
      "          459,     3,    12,    16,    46,  6679,     1,   633,   596,\n",
      "          459,     2],\n",
      "       [  652, 30542,   840,  2209,   564,   702,  6429,    89,  4826,\n",
      "        23985,  2841,     0,  5491,  2348,     5,  3062,     2,  6267,\n",
      "           73,   358, 15336,     1,    94,  1312,   277,     1,     7,\n",
      "          652,     2,   124, 60868,   174,     1,    11,  2209,   564,\n",
      "           12,    43,    11,  6972,   432,    12,   118,   507,     8,\n",
      "         1325,     2,  2216,     3,   124,   494,  8112,  1267,     2,\n",
      "          821,  6230,     1,   199,     7,   375,  2980,    91,  2757,\n",
      "            1,   124,  1267,   112,     2, 10331,    73,  7863,   480,\n",
      "            2,  3430,     1,   281,  4196,  1157,   836,  3882,     1,\n",
      "         6972,   673,   458,   432,     3, 46626,   225,  2209,   564,\n",
      "        35844,   148, 23679,  1646,  1721,     2,    11, 58536,    12,\n",
      "          161,  5824,     1,   839,   375,   652,  1091,   121,  2785,\n",
      "            2,  3720,    47,     1,  5171,  1822,    22,  8112,  1267,\n",
      "            1,   309,    22,     2,   749, 67096,     1,   199,  3101,\n",
      "           36,   888,    48,    75, 48654,     1,   115,  2247,  2897,\n",
      "          101, 30138,   309, 44570,     2,    46,  2599,     3,  1091,\n",
      "          421,  1829,   369,     1,    11,  2209,   564,    12,   130,\n",
      "         4471,   164,     3,   602,   375,     6,     4,   504,   652,\n",
      "         6036,   150, 82282,     6,    24,    16,    15,     4,   156,\n",
      "           65,   769,     2, 39297,   225,     1,    33,   130,     8,\n",
      "        11837,   780,     1,   472, 11607,  3360,     2,   399,  1008,\n",
      "            1,   671, 51456,  1581,    16,     4,    61,     1,   650,\n",
      "            6,     9]]), array([5, 2, 2]))\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "'(array([[ 3879,  3048,     0,  1302,  6449,  9225, 32020, 75465,    26,\\n          938,    25, 42449,  1743,  5589,  1071, 28916,   875,     7,\\n         1071,  9306, 13826, 20919,   190,     1,  5614,  7024,   359,\\n        68957,   559,  9255, 11563,   172,  2431,   627,  7521,    51,\\n         5280, 15051,    50,    94,     0,     0,    65,   938, 42449,\\n           90,  2603,  2234,     8,  3420],\\n       [  192,   116,   310, 21315,  4187,  7290,  2223,  5918,    11,\\n        33789,    12,   922,    13,  6187,  1061,     7, 37987,  4187,\\n            2,   192,   116,   310,   880, 25294,     1,   104,   675,\\n         9024,     2,  1643,    10,    11,  8970,    12,     3,   968,\\n            1,     7,    11,  6890,   639,    13,   310,    42,     9,\\n            4,     4,    18,  3163,  3163]]), array([ 9, 10]))\\n'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 4
    }
   ],
   "source": [
    "# \n",
    "class TextDataSet:\n",
    "    def __init__(self, filename, vocab, category_vocab, num_timesteps):\n",
    "        self._vocab = vocab\n",
    "        self._category_vocab = category_vocab\n",
    "        self._num_timesteps = num_timesteps\n",
    "        \n",
    "        self._inputs = []\n",
    "        self._outputs = []\n",
    "        \n",
    "        self._indicator = 0\n",
    "        self._parse_file(filename)\n",
    "        \n",
    "    def _parse_file(self, filename):\n",
    "        tf.logging.info('Loading data from {}'.format(filename))\n",
    "        lines = 0\n",
    "        import re\n",
    "        if re.findall('train', filename):\n",
    "            with open(filename, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "        elif re.findall('test', filename):\n",
    "            with open(filename, 'r', encoding='utf-8-sig') as f:\n",
    "                lines = f.readlines()  \n",
    "        for line in lines:\n",
    "            label, content = line.strip('\\r\\n').split('\\t')\n",
    "            id_label = self._category_vocab.category_to_id(label)\n",
    "            id_words = self._vocab.sentence_to_id(content)\n",
    "            \n",
    "            id_words = id_words[0: self._num_timesteps]\n",
    "            padding_num = self._num_timesteps - len(id_words)\n",
    "            id_words = id_words + [\n",
    "                self._vocab.unk for i in range(padding_num)]\n",
    "            self._inputs.append(id_words)\n",
    "            self._outputs.append(id_label)\n",
    "\n",
    "        \n",
    "        self._inputs = np.asarray(self._inputs, dtype=np.int32)\n",
    "        self._outputs = np.asarray(self._outputs, dtype=np.int32)\n",
    "        self._random_shuffle()\n",
    "        \n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(len(self._inputs))\n",
    "        self._inputs = self._inputs[p]\n",
    "        self._outputs = self._outputs[p]\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            raise Exception(\"batch size: {} is too large\".format(batch_size))\n",
    "        \n",
    "        batch_inputs = self._inputs[self._indicator: end_indicator]\n",
    "        batch_output = self._outputs[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_inputs, batch_output\n",
    "\n",
    "train_dataset = TextDataSet(seg_train_file, vocab, category_vocab, hps.num_timesteps)\n",
    "test_dataset = TextDataSet(seg_test_file, vocab, category_vocab, hps.num_timesteps)\n",
    "\n",
    "print(train_dataset.next_batch(3))\n",
    "# print(test_dataset.next_batch(2))\n",
    "# 因为next_batch == 2 ,输出为\n",
    "'''(array([[ 3879,  3048,     0,  1302,  6449,  9225, 32020, 75465,    26,\n",
    "          938,    25, 42449,  1743,  5589,  1071, 28916,   875,     7,\n",
    "         1071,  9306, 13826, 20919,   190,     1,  5614,  7024,   359,\n",
    "        68957,   559,  9255, 11563,   172,  2431,   627,  7521,    51,\n",
    "         5280, 15051,    50,    94,     0,     0,    65,   938, 42449,\n",
    "           90,  2603,  2234,     8,  3420],\n",
    "       [  192,   116,   310, 21315,  4187,  7290,  2223,  5918,    11,\n",
    "        33789,    12,   922,    13,  6187,  1061,     7, 37987,  4187,\n",
    "            2,   192,   116,   310,   880, 25294,     1,   104,   675,\n",
    "         9024,     2,  1643,    10,    11,  8970,    12,     3,   968,\n",
    "            1,     7,    11,  6890,   639,    13,   310,    42,     9,\n",
    "            4,     4,    18,  3163,  3163]]), array([ 9, 10]))\n",
    "'''\n",
    "# 只选择句子前50个分词, 输出为2*50的矩阵,第一个为句子分词的索引id, 第二个为分类的索引id\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "embeddings <tf.Variable 'embedding/embedding:0' shape=(85428, 32) dtype=float32_ref>\n",
      "embed_inputs Tensor(\"embedding/embedding_lookup/Identity:0\", shape=(100, 200, 32), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-5-2a0a042cf192>:39: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-2a0a042cf192>:48: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-2a0a042cf192>:53: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "run_outputs------ Tensor(\"lstm_nn/rnn/transpose_1:0\", shape=(100, 200, 64), dtype=float32)\n",
      "last------------ Tensor(\"lstm_nn/strided_slice:0\", shape=(100, 64), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-5-2a0a042cf192>:58: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From <ipython-input-5-2a0a042cf192>:64: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "fc1_dropout Tensor(\"fc/Dropout/dropout_1/mul:0\", shape=(100, 64), dtype=float32)\n",
      "logits Tensor(\"fc/fc2/BiasAdd:0\", shape=(100, 12), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-5-2a0a042cf192>:80: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.math.argmax` instead\n",
      "INFO:tensorflow:variable name: embedding/embedding:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: fc/fc1/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc1/bias:0\n",
      "INFO:tensorflow:variable name: fc/fc2/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc2/bias:0\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def create_model(hps, vocab_size, num_classes):\n",
    "    # 取一个句子的前50个分词, num_classes为固定的50个分词\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    # 训练批次大小\n",
    "    batch_size = hps.batch_size\n",
    "    # 输入为[批次的大小,50]\n",
    "    inputs = tf.placeholder(tf.int32, (batch_size, num_timesteps))\n",
    "    # 输出为[批次的大小,]\n",
    "    outputs = tf.placeholder(tf.int32, (batch_size, ))\n",
    "    # dropout的使用\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    # 保存训练到哪一步\n",
    "    global_step = tf.Variable(\n",
    "        tf.zeros([], tf.int64), name='global_step', trainable=False)\n",
    "    # 随机化embedding 编码\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    with tf.variable_scope(\n",
    "        'embedding', initializer=embedding_initializer):\n",
    "        embeddings = tf.get_variable(\n",
    "            'embedding',\n",
    "            [vocab_size, hps.num_embedding_size],\n",
    "            tf.float32)\n",
    "        print('embeddings', embeddings) # (85428, 32)\n",
    "        # 把输入的分词中的id -> embedding编码形式\n",
    "        # ex [1, 10, 7] -> [embeddings[1], embeddings[10], embeddings[7]]\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "        print('embed_inputs', embed_inputs) # (100, 600, 32)\n",
    "    # 网络initializer的一种方法\n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_size + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    lstm_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    # 构建lstm\n",
    "    with tf.variable_scope('lstm_nn', initializer=lstm_init):\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_layers):\n",
    "            # 循环初始化lstm\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "                hps.num_lstm_nodes[i],\n",
    "                state_is_tuple = True\n",
    "            )\n",
    "            # 使用dropout方法\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(\n",
    "                cell,\n",
    "                output_keep_prob = keep_prob\n",
    "            )\n",
    "            cells.append(cell)\n",
    "        # 合并两个cell\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        # 初始化cell内的值\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        # run_outputs: [batch_size, num_timesteps, lstm_outpus[-1]]\n",
    "        run_outputs, _ = tf.nn.dynamic_rnn(\n",
    "            cell, embed_inputs, initial_state=initial_state\n",
    "        )\n",
    "        print('run_outputs------', run_outputs) # (100, 600, 64)\n",
    "        last = run_outputs[:, -1, :] # (100, 64)\n",
    "        print('last------------', last)\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    # lstm连接到全连接层\n",
    "    with tf.variable_scope('fc', initializer=fc_init):\n",
    "        fc1 = tf.layers.dense(last, # (100, 64)\n",
    "                              hps.num_fc_nodes, # 64\n",
    "                              activation=tf.nn.relu,\n",
    "                              name='fc1')\n",
    "        # 使用dropout方法\n",
    "        fc1_dropout = tf.contrib.layers.dropout(fc1, keep_prob)\n",
    "        print('fc1_dropout', fc1_dropout)\n",
    "        logits = tf.layers.dense(fc1_dropout,\n",
    "                                 num_classes,\n",
    "                                 name='fc2')\n",
    "        print('logits', logits)\n",
    "    # 计算损失函数\n",
    "    with tf.name_scope('metrics'):\n",
    "        sofmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels= outputs\n",
    "        )\n",
    "        loss = tf.reduce_mean(sofmax_loss)\n",
    "        y_pred = tf.arg_max(tf.nn.softmax(logits=logits),\n",
    "                            1,\n",
    "                            output_type= tf.int32)\n",
    "        correct_pred = tf.equal(outputs, y_pred)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    # 构建train_op\n",
    "    with tf.name_scope('train_op'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            tf.logging.info('variable name: {}'.format(var.name))\n",
    "        # 限制训练时的梯度大小,使得不会出现梯度爆炸\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(loss, tvars), hps.clip_lstm_grads\n",
    "        )\n",
    "        # 梯度应用到变量中去\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars), global_step= global_step\n",
    "        )\n",
    "    \n",
    "    return ((inputs, outputs, keep_prob),\n",
    "            (loss, accuracy),\n",
    "            (train_op, global_step))\n",
    "\n",
    "placeholders, metrics, others = create_model(\n",
    "    hps, vocab_size, num_classes\n",
    ")\n",
    "inputs, outputs, keep_prod = placeholders\n",
    "loss, accuracy = metrics\n",
    "train_op, global_step = others"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Train Step: 20, loss: 2.436840057373047, accuracy: 0.11999999731779099\n",
      "INFO:tensorflow:Train Step: 40, loss: 2.459986686706543, accuracy: 0.09000000357627869\n",
      "INFO:tensorflow:Train Step: 60, loss: 2.456941604614258, accuracy: 0.07999999821186066\n",
      "INFO:tensorflow:Train Step: 80, loss: 2.477553367614746, accuracy: 0.07999999821186066\n",
      "INFO:tensorflow:Train Step: 100, loss: 2.4217422008514404, accuracy: 0.1599999964237213\n",
      "INFO:tensorflow:Test Step: 100, loss: 2.1643269062042236, accuracy: 0.14980000257492065\n",
      "INFO:tensorflow:Train Step: 220, loss: 2.393247127532959, accuracy: 0.15000000596046448\n",
      "INFO:tensorflow:Train Step: 240, loss: 2.236553907394409, accuracy: 0.10999999940395355\n",
      "INFO:tensorflow:Train Step: 260, loss: 2.2571945190429688, accuracy: 0.17000000178813934\n",
      "INFO:tensorflow:Train Step: 280, loss: 2.291293144226074, accuracy: 0.14000000059604645\n",
      "INFO:tensorflow:Train Step: 300, loss: 2.164581537246704, accuracy: 0.20000000298023224\n",
      "INFO:tensorflow:Test Step: 300, loss: 2.1775240898132324, accuracy: 0.20839999616146088\n",
      "INFO:tensorflow:Train Step: 420, loss: 2.1433122158050537, accuracy: 0.23999999463558197\n",
      "INFO:tensorflow:Train Step: 440, loss: 1.8755638599395752, accuracy: 0.2199999988079071\n",
      "INFO:tensorflow:Train Step: 460, loss: 1.9595838785171509, accuracy: 0.25\n",
      "INFO:tensorflow:Train Step: 480, loss: 2.0929667949676514, accuracy: 0.25\n",
      "INFO:tensorflow:Train Step: 500, loss: 1.9717612266540527, accuracy: 0.27000001072883606\n",
      "INFO:tensorflow:Test Step: 500, loss: 1.8820816278457642, accuracy: 0.27799999713897705\n",
      "INFO:tensorflow:Train Step: 620, loss: 1.9985570907592773, accuracy: 0.2800000011920929\n",
      "INFO:tensorflow:Train Step: 640, loss: 1.829493761062622, accuracy: 0.33000001311302185\n",
      "INFO:tensorflow:Train Step: 660, loss: 1.7848472595214844, accuracy: 0.3199999928474426\n",
      "INFO:tensorflow:Train Step: 680, loss: 1.8535583019256592, accuracy: 0.28999999165534973\n",
      "INFO:tensorflow:Train Step: 700, loss: 1.7686684131622314, accuracy: 0.3499999940395355\n",
      "INFO:tensorflow:Test Step: 700, loss: 1.7966783046722412, accuracy: 0.31269997358322144\n",
      "INFO:tensorflow:Train Step: 820, loss: 1.8303565979003906, accuracy: 0.30000001192092896\n",
      "INFO:tensorflow:Train Step: 840, loss: 1.9595931768417358, accuracy: 0.25999999046325684\n",
      "INFO:tensorflow:Train Step: 860, loss: 1.7189688682556152, accuracy: 0.3400000035762787\n",
      "INFO:tensorflow:Train Step: 880, loss: 1.7250784635543823, accuracy: 0.3700000047683716\n",
      "INFO:tensorflow:Train Step: 900, loss: 1.6999452114105225, accuracy: 0.3400000035762787\n",
      "INFO:tensorflow:Test Step: 900, loss: 1.6424570083618164, accuracy: 0.37129998207092285\n",
      "INFO:tensorflow:Train Step: 1020, loss: 1.7860440015792847, accuracy: 0.3700000047683716\n",
      "INFO:tensorflow:Train Step: 1040, loss: 1.655471682548523, accuracy: 0.4000000059604645\n",
      "INFO:tensorflow:Train Step: 1060, loss: 1.8392709493637085, accuracy: 0.36000001430511475\n",
      "INFO:tensorflow:Train Step: 1080, loss: 1.766209363937378, accuracy: 0.36000001430511475\n",
      "INFO:tensorflow:Train Step: 1100, loss: 1.7343045473098755, accuracy: 0.3799999952316284\n",
      "INFO:tensorflow:Test Step: 1100, loss: 1.3341177701950073, accuracy: 0.4287000000476837\n",
      "INFO:tensorflow:Train Step: 1220, loss: 1.7064114809036255, accuracy: 0.4000000059604645\n",
      "INFO:tensorflow:Train Step: 1240, loss: 1.7511868476867676, accuracy: 0.4000000059604645\n",
      "INFO:tensorflow:Train Step: 1260, loss: 1.3018317222595215, accuracy: 0.5400000214576721\n",
      "INFO:tensorflow:Train Step: 1280, loss: 1.526301383972168, accuracy: 0.46000000834465027\n",
      "INFO:tensorflow:Train Step: 1300, loss: 1.6451115608215332, accuracy: 0.49000000953674316\n",
      "INFO:tensorflow:Test Step: 1300, loss: 1.3385026454925537, accuracy: 0.48469996452331543\n",
      "INFO:tensorflow:Train Step: 1420, loss: 1.394330620765686, accuracy: 0.5199999809265137\n",
      "INFO:tensorflow:Train Step: 1440, loss: 1.3962388038635254, accuracy: 0.550000011920929\n",
      "INFO:tensorflow:Train Step: 1460, loss: 1.8304585218429565, accuracy: 0.4699999988079071\n",
      "INFO:tensorflow:Train Step: 1480, loss: 1.4097501039505005, accuracy: 0.47999998927116394\n",
      "INFO:tensorflow:Train Step: 1500, loss: 1.5312649011611938, accuracy: 0.44999998807907104\n",
      "INFO:tensorflow:Test Step: 1500, loss: 1.1852660179138184, accuracy: 0.5426000356674194\n",
      "INFO:tensorflow:Train Step: 1620, loss: 1.237115502357483, accuracy: 0.5400000214576721\n",
      "INFO:tensorflow:Train Step: 1640, loss: 1.352535367012024, accuracy: 0.5299999713897705\n",
      "INFO:tensorflow:Train Step: 1660, loss: 1.2310938835144043, accuracy: 0.5199999809265137\n",
      "INFO:tensorflow:Train Step: 1680, loss: 1.5220338106155396, accuracy: 0.5\n",
      "INFO:tensorflow:Train Step: 1700, loss: 1.2806308269500732, accuracy: 0.6100000143051147\n",
      "INFO:tensorflow:Test Step: 1700, loss: 0.9843441843986511, accuracy: 0.5952000021934509\n",
      "INFO:tensorflow:Train Step: 1820, loss: 1.401811122894287, accuracy: 0.4300000071525574\n",
      "INFO:tensorflow:Train Step: 1840, loss: 1.3986427783966064, accuracy: 0.49000000953674316\n",
      "INFO:tensorflow:Train Step: 1860, loss: 1.0898572206497192, accuracy: 0.6200000047683716\n",
      "INFO:tensorflow:Train Step: 1880, loss: 1.2620371580123901, accuracy: 0.550000011920929\n",
      "INFO:tensorflow:Train Step: 1900, loss: 1.1567302942276, accuracy: 0.5199999809265137\n",
      "INFO:tensorflow:Test Step: 1900, loss: 1.0035924911499023, accuracy: 0.6428999900817871\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# train:\n",
    "init_op = tf.global_variables_initializer()\n",
    "train_keep_prob_value = 0.8\n",
    "test_keep_prob_value = 1.0\n",
    "\n",
    "test_steps = 100\n",
    "num_train_steps = 1000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for i in range(num_train_steps):\n",
    "        batch_inputs, batch_labels = train_dataset.next_batch(\n",
    "            hps.batch_size\n",
    "        )\n",
    "        outputs_val = sess.run([loss, accuracy, train_op, global_step],\n",
    "                               feed_dict = {\n",
    "                                inputs: batch_inputs,\n",
    "                                outputs: batch_labels,\n",
    "                                   keep_prod: train_keep_prob_value,\n",
    "                               })\n",
    "        loss_val, accuracy_val, _, global_step_val = outputs_val\n",
    "        if (i+1) % 20 == 0:\n",
    "            tf.logging.info(\"Train Step: {}, loss: {}, accuracy: {}\".format(global_step_val, loss_val, accuracy_val))\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            all_test_acc_cal = []\n",
    "            for j in range(test_steps):\n",
    "                test_inputs, test_labels = test_dataset.next_batch(hps.batch_size)\n",
    "                test_val = sess.run([loss, accuracy, train_op, global_step],\n",
    "                                    feed_dict= {\n",
    "                                        inputs: test_inputs,\n",
    "                                        outputs: test_labels,\n",
    "                                        keep_prod: test_keep_prob_value,\n",
    "                                    })\n",
    "                test_loss_val, test_accuarcy_val, _, test_step_val = test_val\n",
    "                all_test_acc_cal.append(test_accuarcy_val)\n",
    "            test_acc = np.mean(all_test_acc_cal)\n",
    "            tf.logging.info(\"Test Step: {}, loss: {}, accuracy: {}\".format(global_step_val, test_loss_val, test_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
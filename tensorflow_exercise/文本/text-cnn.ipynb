{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\nIf you depend on functionality not listed there, please file an issue.\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "## 下面高参数 10K 次 train: 99.7%, Valid: 92.7%, Test: 93.2%\n",
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        # num_embedding_size = 16,\n",
    "        num_embedding_size = 32,\n",
    "        # 一个句子取前50个分词\n",
    "        # num_timesteps = 50,\n",
    "        num_timesteps = 600,\n",
    "        # num_filters = 128,\n",
    "        num_filters = 256,\n",
    "        num_kernel_size = 3,\n",
    "        num_fc_nodes = 32,\n",
    "        # num_fc_nodes = 64,\n",
    "        batch_size = 100,\n",
    "        learning_rate = 0.001,\n",
    "        num_word_threshold = 10,\n",
    "    )\n",
    "hps = get_default_params()\n",
    "\n",
    "train_file = '.\\deep_learn\\sohu_seg_train_file.txt'\n",
    "test_file = '.\\deep_learn\\sohu_seg_test_file.txt'\n",
    "\n",
    "vocab_file =  '.\\deep_learn\\sohu_vocab.txt'\n",
    "category_file = '.\\deep_learn\\sohu_category.txt'\n",
    "output_file = '.\\deep_learn\\sohu_run_text_run'\n",
    "\n",
    "if not os.path.exists(output_file):\n",
    "    os.mkdir(output_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "INFO:tensorflow:vocab_size: 85428\n",
      "INFO:tensorflow:category_size: 12\n",
      "INFO:tensorflow:id:11\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# api sentence_to_id的实现\n",
    "class Vocab:\n",
    "    def __init__(self, filename, num_word_threshold):\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._num_word_threshold = num_word_threshold\n",
    "        self._read_dict(filename)\n",
    "    \n",
    "    def _read_dict(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, frequency = line.strip('\\r\\n').split('\\t')\n",
    "            frequency = int(frequency)\n",
    "            if frequency < self._num_word_threshold:\n",
    "                continue\n",
    "            idx = len(self._word_to_id)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            self._word_to_id[word] = idx\n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self._unk)\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "    \n",
    "    def sentence_to_id(self, sentence):\n",
    "        # 分词后的句子用空格来隔开每一个词语,这里每一句话里的分词用id来表示\n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence.split()]\n",
    "        return word_ids\n",
    "\n",
    "class CategoryDict:\n",
    "    def __init__(self, filename):\n",
    "        self._category_to_id = {}\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            category = line.strip('\\r\\n')\n",
    "            idx = len(self._category_to_id)\n",
    "            self._category_to_id[category] = idx\n",
    "            \n",
    "    def size(self):\n",
    "        return len(self._category_to_id)\n",
    "    \n",
    "    def category_to_id(self, category):\n",
    "        if not category in self._category_to_id:\n",
    "            print(self._category_to_id)\n",
    "            raise Exception(\"{} is not in our category\".format(category))\n",
    "            \n",
    "        return self._category_to_id[category]\n",
    "          \n",
    "vocab = Vocab(vocab_file, hps.num_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "\n",
    "tf.logging.info('vocab_size: {}'.format(vocab_size))\n",
    "category_vocab = CategoryDict(category_file)\n",
    "num_classes = category_vocab.size()\n",
    "tf.logging.info('category_size: {}'.format(num_classes))\n",
    "test_str = '女人'\n",
    "tf.logging.info('id:{}'.format(category_vocab.category_to_id(test_str)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Loading data from .\\deep_learn\\sohu_seg_train_file.txt\n",
      "INFO:tensorflow:Loading data from .\\deep_learn\\sohu_seg_test_file.txt\n",
      "(array([[    7,    27,    14, ...,     2,   165,     1],\n       [21154,   373,    13, ...,   363,    73,   572]]), array([11,  4]))\n(array([[21562,  9878,    77, ...,  9728,     2,    31],\n       [  725,  7560,   146, ...,  1967, 29053,     0]]), array([10,  6]))\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "'(array([[ 3879,  3048,     0,  1302,  6449,  9225, 32020, 75465,    26,\\n          938,    25, 42449,  1743,  5589,  1071, 28916,   875,     7,\\n         1071,  9306, 13826, 20919,   190,     1,  5614,  7024,   359,\\n        68957,   559,  9255, 11563,   172,  2431,   627,  7521,    51,\\n         5280, 15051,    50,    94,     0,     0,    65,   938, 42449,\\n           90,  2603,  2234,     8,  3420],\\n       [  192,   116,   310, 21315,  4187,  7290,  2223,  5918,    11,\\n        33789,    12,   922,    13,  6187,  1061,     7, 37987,  4187,\\n            2,   192,   116,   310,   880, 25294,     1,   104,   675,\\n         9024,     2,  1643,    10,    11,  8970,    12,     3,   968,\\n            1,     7,    11,  6890,   639,    13,   310,    42,     9,\\n            4,     4,    18,  3163,  3163]]), array([ 9, 10]))\\n'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 4
    }
   ],
   "source": [
    "# \n",
    "class TextDataSet:\n",
    "    def __init__(self, filename, vocab, category_vocab, num_timesteps):\n",
    "        self._vocab = vocab\n",
    "        self._category_vocab = category_vocab\n",
    "        self._num_timesteps = num_timesteps\n",
    "        \n",
    "        self._inputs = []\n",
    "        self._outputs = []\n",
    "        \n",
    "        self._indicator = 0\n",
    "        self._parse_file(filename)\n",
    "        \n",
    "    def _parse_file(self, filename):\n",
    "        tf.logging.info('Loading data from {}'.format(filename))\n",
    "        lines = 0\n",
    "        import re\n",
    "        if re.findall('train', filename):\n",
    "            with open(filename, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "        elif re.findall('test', filename):\n",
    "            with open(filename, 'r', encoding='utf-8-sig') as f:\n",
    "                lines = f.readlines()  \n",
    "        for line in lines:\n",
    "            label, content = line.strip('\\r\\n').split('\\t')\n",
    "            id_label = self._category_vocab.category_to_id(label)\n",
    "            id_words = self._vocab.sentence_to_id(content)\n",
    "            \n",
    "            id_words = id_words[0: self._num_timesteps]\n",
    "            padding_num = self._num_timesteps - len(id_words)\n",
    "            id_words = id_words + [\n",
    "                self._vocab.unk for i in range(padding_num)]\n",
    "            self._inputs.append(id_words)\n",
    "            self._outputs.append(id_label)\n",
    "\n",
    "        \n",
    "        self._inputs = np.asarray(self._inputs, dtype=np.int32)\n",
    "        self._outputs = np.asarray(self._outputs, dtype=np.int32)\n",
    "        self._random_shuffle()\n",
    "        \n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(len(self._inputs))\n",
    "        self._inputs = self._inputs[p]\n",
    "        self._outputs = self._outputs[p]\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            raise Exception(\"batch size: {} is too large\".format(batch_size))\n",
    "        \n",
    "        batch_inputs = self._inputs[self._indicator: end_indicator]\n",
    "        batch_output = self._outputs[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_inputs, batch_output\n",
    "\n",
    "train_dataset = TextDataSet(train_file, vocab, category_vocab, hps.num_timesteps)\n",
    "test_dataset = TextDataSet(test_file, vocab, category_vocab, hps.num_timesteps)\n",
    "\n",
    "print(train_dataset.next_batch(2))\n",
    "print(test_dataset.next_batch(2))\n",
    "# 因为next_batch == 2 ,输出为\n",
    "'''(array([[ 3879,  3048,     0,  1302,  6449,  9225, 32020, 75465,    26,\n",
    "          938,    25, 42449,  1743,  5589,  1071, 28916,   875,     7,\n",
    "         1071,  9306, 13826, 20919,   190,     1,  5614,  7024,   359,\n",
    "        68957,   559,  9255, 11563,   172,  2431,   627,  7521,    51,\n",
    "         5280, 15051,    50,    94,     0,     0,    65,   938, 42449,\n",
    "           90,  2603,  2234,     8,  3420],\n",
    "       [  192,   116,   310, 21315,  4187,  7290,  2223,  5918,    11,\n",
    "        33789,    12,   922,    13,  6187,  1061,     7, 37987,  4187,\n",
    "            2,   192,   116,   310,   880, 25294,     1,   104,   675,\n",
    "         9024,     2,  1643,    10,    11,  8970,    12,     3,   968,\n",
    "            1,     7,    11,  6890,   639,    13,   310,    42,     9,\n",
    "            4,     4,    18,  3163,  3163]]), array([ 9, 10]))\n",
    "'''\n",
    "# 只选择句子前50个分词, 输出为2*50的矩阵,第一个为句子分词的索引id, 第二个为分类的索引id\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-bbcaa0ffe53b>:36: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.conv1d instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-bbcaa0ffe53b>:71: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From <ipython-input-5-bbcaa0ffe53b>:77: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.dense instead.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-5-bbcaa0ffe53b>:91: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.math.argmax` instead\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nDeprecated in favor of operator or tf.math.divide.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def create_model(hps, vocab_size, num_classes):\n",
    "    # 取一个句子的前50个分词, num_classes为固定的50个分词\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    # 训练批次大小\n",
    "    batch_size = hps.batch_size\n",
    "    # 输入为[批次的大小,50]\n",
    "    inputs = tf.placeholder(tf.int32, (batch_size, num_timesteps))\n",
    "    # 输出为[批次的大小,]\n",
    "    outputs = tf.placeholder(tf.int32, (batch_size, ))\n",
    "    # dropout的使用\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    # 保存训练到哪一步\n",
    "    global_step = tf.Variable(\n",
    "        tf.zeros([], tf.int64), name='global_step', trainable=False)\n",
    "    # 随机化embedding 编码\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    with tf.variable_scope(\n",
    "        'embedding', initializer=embedding_initializer):\n",
    "        embeddings = tf.get_variable(\n",
    "            'embedding',\n",
    "            [vocab_size, hps.num_embedding_size],\n",
    "            tf.float32)\n",
    "        # 把输入的分词中的id -> embedding编码形式\n",
    "        # ex [1, 10, 7] -> [embeddings[1], embeddings[10], embeddings[7]]\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "    # 卷积实现\n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_size + hps.num_filters) / 3.0\n",
    "    cnn_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    with tf.variable_scope('cnn', initializer= cnn_init):\n",
    "        # embed_inputs : [batch_size, timesteps, embed_size]\n",
    "        # conv1d : [batch_size, timesteps, num_filters]\n",
    "        conv1d = tf.layers.conv1d(embed_inputs,\n",
    "                                  hps.num_filters,\n",
    "                                  hps.num_kernel_size,\n",
    "                                  activation=tf.nn.relu,\n",
    "                                  )\n",
    "        global_maxpooling = tf.reduce_max(conv1d, axis=[1])\n",
    "    \n",
    "     \n",
    "    \"\"\"\n",
    "    # 网络initializer的一种方法\n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_size + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    lstm_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    # 构建lstm\n",
    "    with tf.variable_scope('lstm_nn', initializer=lstm_init):\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_laysers):\n",
    "            # 循环初始化lstm\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "                hps.num_lstm_nodes[i],\n",
    "                state_is_tuple = True\n",
    "            )\n",
    "            # 使用dropout方法\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(\n",
    "                cell,\n",
    "                output_keep_prob = keep_prob\n",
    "            )\n",
    "            cells.append(cell)\n",
    "        # 合并两个cell\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        # 初始化cell内的值\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        # run_outputs: [batch_size, num_timesteps, lstm_outpus[-1]]\n",
    "        run_outpus, _ = tf.nn.dynamic_rnn(\n",
    "            cell, embed_inputs, initial_state=initial_state\n",
    "        )\n",
    "        print(run_outpus)\n",
    "        last = run_outpus[:, -1, :]\n",
    "    \"\"\"\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    # cnn连接到全连接层\n",
    "    with tf.variable_scope('fc', initializer=fc_init):\n",
    "        fc1 = tf.layers.dense(global_maxpooling,\n",
    "                              hps.num_fc_nodes,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name='fc1')\n",
    "        # 使用dropout方法\n",
    "        fc1_dropout = tf.contrib.layers.dropout(fc1, keep_prob)\n",
    "        logits = tf.layers.dense(fc1_dropout,\n",
    "                                 num_classes,\n",
    "                                 name='fc2')\n",
    "    # 计算损失函数\n",
    "    with tf.name_scope('metrics'):\n",
    "        sofmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels= outputs\n",
    "        )\n",
    "        loss = tf.reduce_mean(sofmax_loss)\n",
    "        y_pred = tf.arg_max(tf.nn.softmax(logits=logits),\n",
    "                            1,\n",
    "                            output_type= tf.int32)\n",
    "        correct_pred = tf.equal(outputs, y_pred)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    # 构建train_op\n",
    "    with tf.name_scope('train_op'):\n",
    "        train_op = tf.train.AdamOptimizer(hps.learning_rate).minimize(loss,\n",
    "                                                                      global_step=global_step)\n",
    "        \"\"\"\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            tf.logging.info('variable name: {}'.format(var.name))\n",
    "        # 限制训练时的梯度大小,使得不会出现梯度爆炸\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(loss, tvars), hps.clip_lstm_grads\n",
    "        )\n",
    "        # 梯度应用到变量中去\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars), global_step= global_step\n",
    "        )\n",
    "        \"\"\"\n",
    "    return ((inputs, outputs, keep_prob),\n",
    "            (loss, accuracy),\n",
    "            (train_op, global_step))\n",
    "\n",
    "placeholders, metrics, others = create_model(\n",
    "    hps, vocab_size, num_classes\n",
    ")\n",
    "inputs, outputs, keep_prod = placeholders\n",
    "loss, accuracy = metrics\n",
    "train_op, global_step = others"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Train Step: 100, loss: 2.1734323501586914, accuracy: 0.1899999976158142\n",
      "INFO:tensorflow:Train Step: 200, loss: 1.6766830682754517, accuracy: 0.46000000834465027\n",
      "INFO:tensorflow:Train Step: 300, loss: 1.199046015739441, accuracy: 0.6499999761581421\n",
      "INFO:tensorflow:Train Step: 400, loss: 0.9451839923858643, accuracy: 0.7099999785423279\n",
      "INFO:tensorflow:Train Step: 500, loss: 0.9733993411064148, accuracy: 0.6899999976158142\n",
      "INFO:tensorflow:Train Step: 600, loss: 1.0734984874725342, accuracy: 0.6700000166893005\n",
      "INFO:tensorflow:Train Step: 700, loss: 0.8732958436012268, accuracy: 0.75\n",
      "INFO:tensorflow:Train Step: 800, loss: 0.5862252116203308, accuracy: 0.8199999928474426\n",
      "INFO:tensorflow:Train Step: 900, loss: 0.6773641109466553, accuracy: 0.7900000214576721\n",
      "INFO:tensorflow:Train Step: 1000, loss: 0.6384667754173279, accuracy: 0.800000011920929\n",
      "INFO:tensorflow:------Test Step: 1000, loss: 0.4643302261829376, accuracy: 0.8248999714851379\n",
      "INFO:tensorflow:Train Step: 1200, loss: 0.40767645835876465, accuracy: 0.8999999761581421\n",
      "INFO:tensorflow:Train Step: 1300, loss: 0.5792720913887024, accuracy: 0.8399999737739563\n",
      "INFO:tensorflow:Train Step: 1400, loss: 0.5205734372138977, accuracy: 0.8399999737739563\n",
      "INFO:tensorflow:Train Step: 1500, loss: 0.528281033039093, accuracy: 0.8799999952316284\n",
      "INFO:tensorflow:Train Step: 1600, loss: 0.29979923367500305, accuracy: 0.8999999761581421\n",
      "INFO:tensorflow:Train Step: 1700, loss: 0.39159882068634033, accuracy: 0.8799999952316284\n",
      "INFO:tensorflow:Train Step: 1800, loss: 0.2770010530948639, accuracy: 0.9100000262260437\n",
      "INFO:tensorflow:Train Step: 1900, loss: 0.29813769459724426, accuracy: 0.9200000166893005\n",
      "INFO:tensorflow:Train Step: 2000, loss: 0.18718379735946655, accuracy: 0.949999988079071\n",
      "INFO:tensorflow:------Test Step: 2000, loss: 0.3825661540031433, accuracy: 0.8906999826431274\n",
      "INFO:tensorflow:Train Step: 2200, loss: 0.23322167992591858, accuracy: 0.9399999976158142\n",
      "INFO:tensorflow:Train Step: 2300, loss: 0.28693005442619324, accuracy: 0.8999999761581421\n",
      "INFO:tensorflow:Train Step: 2400, loss: 0.24037818610668182, accuracy: 0.9100000262260437\n",
      "INFO:tensorflow:Train Step: 2500, loss: 0.2422339767217636, accuracy: 0.9399999976158142\n",
      "INFO:tensorflow:Train Step: 2600, loss: 0.1464793086051941, accuracy: 0.949999988079071\n",
      "INFO:tensorflow:Train Step: 2700, loss: 0.1524079591035843, accuracy: 0.9599999785423279\n",
      "INFO:tensorflow:Train Step: 2800, loss: 0.1501169502735138, accuracy: 0.9399999976158142\n",
      "INFO:tensorflow:Train Step: 2900, loss: 0.17127057909965515, accuracy: 0.9300000071525574\n",
      "INFO:tensorflow:Train Step: 3000, loss: 0.1965836137533188, accuracy: 0.9399999976158142\n",
      "INFO:tensorflow:------Test Step: 3000, loss: 0.2602224349975586, accuracy: 0.9147000312805176\n",
      "INFO:tensorflow:Train Step: 3200, loss: 0.1369345784187317, accuracy: 0.9599999785423279\n",
      "INFO:tensorflow:Train Step: 3300, loss: 0.16897006332874298, accuracy: 0.9599999785423279\n",
      "INFO:tensorflow:Train Step: 3400, loss: 0.07694555073976517, accuracy: 0.9900000095367432\n",
      "INFO:tensorflow:Train Step: 3500, loss: 0.13626891374588013, accuracy: 0.949999988079071\n",
      "INFO:tensorflow:Train Step: 3600, loss: 0.1065327525138855, accuracy: 0.9700000286102295\n",
      "INFO:tensorflow:Train Step: 3700, loss: 0.19511449337005615, accuracy: 0.9399999976158142\n",
      "INFO:tensorflow:Train Step: 3800, loss: 0.23602204024791718, accuracy: 0.9100000262260437\n",
      "INFO:tensorflow:Train Step: 3900, loss: 0.098324716091156, accuracy: 0.9599999785423279\n",
      "INFO:tensorflow:Train Step: 4000, loss: 0.09215443581342697, accuracy: 0.9700000286102295\n",
      "INFO:tensorflow:------Test Step: 4000, loss: 0.13635754585266113, accuracy: 0.9343000054359436\n",
      "INFO:tensorflow:Train Step: 4200, loss: 0.08731052279472351, accuracy: 0.9800000190734863\n",
      "INFO:tensorflow:Train Step: 4300, loss: 0.04639478027820587, accuracy: 0.9900000095367432\n",
      "INFO:tensorflow:Train Step: 4400, loss: 0.112421415746212, accuracy: 0.9700000286102295\n",
      "INFO:tensorflow:Train Step: 4500, loss: 0.09822449833154678, accuracy: 0.9599999785423279\n",
      "INFO:tensorflow:Train Step: 4600, loss: 0.08253344893455505, accuracy: 0.9800000190734863\n",
      "INFO:tensorflow:Train Step: 4700, loss: 0.09773676097393036, accuracy: 0.9700000286102295\n",
      "INFO:tensorflow:Train Step: 4800, loss: 0.0667368695139885, accuracy: 0.9700000286102295\n",
      "INFO:tensorflow:Train Step: 4900, loss: 0.07429177314043045, accuracy: 0.9700000286102295\n",
      "INFO:tensorflow:Train Step: 5000, loss: 0.09702543169260025, accuracy: 0.9599999785423279\n",
      "INFO:tensorflow:------Test Step: 5000, loss: 0.11968334019184113, accuracy: 0.9492000341415405\n",
      "INFO:tensorflow:Train Step: 5200, loss: 0.14667198061943054, accuracy: 0.9399999976158142\n",
      "INFO:tensorflow:Train Step: 5300, loss: 0.09086254239082336, accuracy: 0.9599999785423279\n",
      "INFO:tensorflow:Train Step: 5400, loss: 0.1292676031589508, accuracy: 0.9700000286102295\n",
      "INFO:tensorflow:Train Step: 5500, loss: 0.11684270948171616, accuracy: 0.9700000286102295\n",
      "INFO:tensorflow:Train Step: 5600, loss: 0.049312081187963486, accuracy: 0.9900000095367432\n",
      "INFO:tensorflow:Train Step: 5700, loss: 0.07312804460525513, accuracy: 0.9700000286102295\n",
      "INFO:tensorflow:Train Step: 5800, loss: 0.16260631382465363, accuracy: 0.949999988079071\n",
      "INFO:tensorflow:Train Step: 5900, loss: 0.058107681572437286, accuracy: 0.9900000095367432\n",
      "INFO:tensorflow:Train Step: 6000, loss: 0.13278184831142426, accuracy: 0.949999988079071\n",
      "INFO:tensorflow:------Test Step: 6000, loss: 0.0638197585940361, accuracy: 0.9566999673843384\n",
      "INFO:tensorflow:Train Step: 6200, loss: 0.09513310343027115, accuracy: 0.949999988079071\n",
      "INFO:tensorflow:Train Step: 6300, loss: 0.27388548851013184, accuracy: 0.9200000166893005\n",
      "INFO:tensorflow:Train Step: 6400, loss: 0.1110631451010704, accuracy: 0.9599999785423279\n",
      "INFO:tensorflow:Train Step: 6500, loss: 0.05613388121128082, accuracy: 0.9800000190734863\n",
      "INFO:tensorflow:Train Step: 6600, loss: 0.08585406094789505, accuracy: 0.9599999785423279\n",
      "INFO:tensorflow:Train Step: 6700, loss: 0.06864289939403534, accuracy: 0.9700000286102295\n",
      "INFO:tensorflow:Train Step: 6800, loss: 0.04373442009091377, accuracy: 0.9800000190734863\n",
      "INFO:tensorflow:Train Step: 6900, loss: 0.10639237612485886, accuracy: 0.9599999785423279\n",
      "INFO:tensorflow:Train Step: 7000, loss: 0.098793163895607, accuracy: 0.9599999785423279\n",
      "INFO:tensorflow:------Test Step: 7000, loss: 0.05554491654038429, accuracy: 0.9668001532554626\n",
      "INFO:tensorflow:Train Step: 7200, loss: 0.09161131829023361, accuracy: 0.9599999785423279\n",
      "INFO:tensorflow:Train Step: 7300, loss: 0.09280245751142502, accuracy: 0.9599999785423279\n",
      "INFO:tensorflow:Train Step: 7400, loss: 0.0697471871972084, accuracy: 0.9800000190734863\n",
      "INFO:tensorflow:Train Step: 7500, loss: 0.12275227904319763, accuracy: 0.9599999785423279\n",
      "INFO:tensorflow:Train Step: 7600, loss: 0.05587182939052582, accuracy: 0.9800000190734863\n",
      "INFO:tensorflow:Train Step: 7700, loss: 0.06054152548313141, accuracy: 0.9900000095367432\n",
      "INFO:tensorflow:Train Step: 7800, loss: 0.0654718354344368, accuracy: 0.9800000190734863\n",
      "INFO:tensorflow:Train Step: 7900, loss: 0.04038846492767334, accuracy: 0.9900000095367432\n",
      "INFO:tensorflow:Train Step: 8000, loss: 0.08966193348169327, accuracy: 0.9599999785423279\n",
      "INFO:tensorflow:------Test Step: 8000, loss: 0.14194540679454803, accuracy: 0.971799910068512\n",
      "INFO:tensorflow:Train Step: 8200, loss: 0.10313142091035843, accuracy: 0.9800000190734863\n",
      "INFO:tensorflow:Train Step: 8300, loss: 0.04989884793758392, accuracy: 0.9700000286102295\n",
      "INFO:tensorflow:Train Step: 8400, loss: 0.07208988070487976, accuracy: 0.9700000286102295\n",
      "INFO:tensorflow:Train Step: 8500, loss: 0.026102792471647263, accuracy: 0.9900000095367432\n",
      "INFO:tensorflow:Train Step: 8600, loss: 0.0658157542347908, accuracy: 0.9800000190734863\n",
      "INFO:tensorflow:Train Step: 8700, loss: 0.042220138013362885, accuracy: 0.9900000095367432\n",
      "INFO:tensorflow:Train Step: 8800, loss: 0.10006696730852127, accuracy: 0.9800000190734863\n",
      "INFO:tensorflow:Train Step: 8900, loss: 0.020552797242999077, accuracy: 0.9900000095367432\n",
      "INFO:tensorflow:Train Step: 9000, loss: 0.04462449252605438, accuracy: 0.9900000095367432\n",
      "INFO:tensorflow:------Test Step: 9000, loss: 0.10953279584646225, accuracy: 0.9718000292778015\n",
      "INFO:tensorflow:Train Step: 9200, loss: 0.09412766247987747, accuracy: 0.9700000286102295\n",
      "INFO:tensorflow:Train Step: 9300, loss: 0.03680218756198883, accuracy: 0.9900000095367432\n",
      "INFO:tensorflow:Train Step: 9400, loss: 0.06637883931398392, accuracy: 0.9800000190734863\n",
      "INFO:tensorflow:Train Step: 9500, loss: 0.03181936591863632, accuracy: 1.0\n",
      "INFO:tensorflow:Train Step: 9600, loss: 0.058227065950632095, accuracy: 0.9800000190734863\n",
      "INFO:tensorflow:Train Step: 9700, loss: 0.057847581803798676, accuracy: 0.9900000095367432\n",
      "INFO:tensorflow:Train Step: 9800, loss: 0.0806412324309349, accuracy: 0.9800000190734863\n",
      "INFO:tensorflow:Train Step: 9900, loss: 0.09813631325960159, accuracy: 0.9700000286102295\n",
      "INFO:tensorflow:Train Step: 10000, loss: 0.05735326185822487, accuracy: 0.9900000095367432\n",
      "INFO:tensorflow:------Test Step: 10000, loss: 0.0303265992552042, accuracy: 0.9754000306129456\n",
      "INFO:tensorflow:Train Step: 10200, loss: 0.036877721548080444, accuracy: 0.9900000095367432\n",
      "INFO:tensorflow:Train Step: 10300, loss: 0.030267054215073586, accuracy: 0.9800000190734863\n",
      "INFO:tensorflow:Train Step: 10400, loss: 0.02517365850508213, accuracy: 0.9800000190734863\n",
      "INFO:tensorflow:Train Step: 10500, loss: 0.04924513399600983, accuracy: 0.9900000095367432\n",
      "INFO:tensorflow:Train Step: 10600, loss: 0.03928139805793762, accuracy: 0.9800000190734863\n",
      "INFO:tensorflow:Train Step: 10700, loss: 0.0393814779818058, accuracy: 0.9800000190734863\n",
      "INFO:tensorflow:Train Step: 10800, loss: 0.05325887352228165, accuracy: 0.9900000095367432\n",
      "INFO:tensorflow:Train Step: 10900, loss: 0.040825843811035156, accuracy: 0.9900000095367432\n",
      "INFO:tensorflow:Train Step: 11000, loss: 0.09227301925420761, accuracy: 0.9700000286102295\n",
      "INFO:tensorflow:------Test Step: 11000, loss: 0.10362249612808228, accuracy: 0.9792000651359558\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# train:\n",
    "init_op = tf.global_variables_initializer()\n",
    "train_keep_prob_value = 0.8\n",
    "test_keep_prob_value = 1.0\n",
    "\n",
    "test_steps = 100\n",
    "num_train_steps = 10000\n",
    "\n",
    "# Train : 100%\n",
    "# Test : 95.3%\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for i in range(num_train_steps):\n",
    "        batch_inputs, batch_labels = train_dataset.next_batch(\n",
    "            hps.batch_size\n",
    "        )\n",
    "        outputs_val = sess.run([loss, accuracy, train_op, global_step],\n",
    "                               feed_dict = {\n",
    "                                inputs: batch_inputs,\n",
    "                                outputs: batch_labels,\n",
    "                                   keep_prod: train_keep_prob_value,\n",
    "                               })\n",
    "        loss_val, accuracy_val, _, global_step_val = outputs_val\n",
    "        if global_step_val % 100 == 0:\n",
    "            tf.logging.info(\"Train Step: {}, loss: {}, accuracy: {}\".format(global_step_val, loss_val, accuracy_val))\n",
    "\n",
    "        if global_step_val % 1000 == 0:\n",
    "            all_test_acc_cal = []\n",
    "            for j in range(test_steps):\n",
    "                test_inputs, test_labels = test_dataset.next_batch(hps.batch_size)\n",
    "                test_val = sess.run([loss, accuracy, train_op, global_step],\n",
    "                                    feed_dict= {\n",
    "                                        inputs: test_inputs,\n",
    "                                        outputs: test_labels,\n",
    "                                        keep_prod: test_keep_prob_value,\n",
    "                                    })\n",
    "                test_loss_val, test_accuarcy_val, _, test_step_val = test_val\n",
    "                all_test_acc_cal.append(test_accuarcy_val)\n",
    "            test_acc = np.mean(all_test_acc_cal)\n",
    "            tf.logging.info(\"------Test Step: {}, loss: {}, accuracy: {}\".format(global_step_val, test_loss_val, test_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
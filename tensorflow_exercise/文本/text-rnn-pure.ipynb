{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\nIf you depend on functionality not listed there, please file an issue.\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "## 下面高参数 10K 次 train: 99.7%, Valid: 92.7%, Test: 93.2%\n",
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        # num_embedding_size = 16,\n",
    "        num_embedding_size = 32,\n",
    "        # 一个句子取前50个分词\n",
    "        # num_timesteps = 50,\n",
    "        num_timesteps = 600,\n",
    "        # num_lstm_nodes = [32, 32],\n",
    "        num_lstm_nodes = [64, 64],\n",
    "        num_lstm_laysers = 2,\n",
    "        # num_fc_nodes = 32,\n",
    "        num_fc_nodes = 64,\n",
    "        batch_size = 100,\n",
    "        clip_lstm_grads = 1.0,\n",
    "        learning_rate = 0.001,\n",
    "        num_word_threshold = 10,\n",
    "    )\n",
    "hps = get_default_params()\n",
    "\n",
    "train_file = '.\\deep_learn\\sohu_seg_train_file.txt'\n",
    "test_file = '.\\deep_learn\\sohu_seg_test_file.txt'\n",
    "\n",
    "vocab_file =  '.\\deep_learn\\sohu_vocab.txt'\n",
    "category_file = '.\\deep_learn\\sohu_category.txt'\n",
    "output_file = '.\\deep_learn\\sohu_run_text_run'\n",
    "\n",
    "if not os.path.exists(output_file):\n",
    "    os.mkdir(output_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "INFO:tensorflow:vocab_size: 85428\n",
      "INFO:tensorflow:category_size: 12\n",
      "INFO:tensorflow:id:11\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# api sentence_to_id的实现\n",
    "class Vocab:\n",
    "    def __init__(self, filename, num_word_threshold):\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._num_word_threshold = num_word_threshold\n",
    "        self._read_dict(filename)\n",
    "    \n",
    "    def _read_dict(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, frequency = line.strip('\\r\\n').split('\\t')\n",
    "            frequency = int(frequency)\n",
    "            if frequency < self._num_word_threshold:\n",
    "                continue\n",
    "            idx = len(self._word_to_id)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            self._word_to_id[word] = idx\n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self._unk)\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "    \n",
    "    def sentence_to_id(self, sentence):\n",
    "        # 分词后的句子用空格来隔开每一个词语,这里每一句话里的分词用id来表示\n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence.split()]\n",
    "        return word_ids\n",
    "\n",
    "class CategoryDict:\n",
    "    def __init__(self, filename):\n",
    "        self._category_to_id = {}\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            category = line.strip('\\r\\n')\n",
    "            idx = len(self._category_to_id)\n",
    "            self._category_to_id[category] = idx\n",
    "            \n",
    "    def size(self):\n",
    "        return len(self._category_to_id)\n",
    "    \n",
    "    def category_to_id(self, category):\n",
    "        if not category in self._category_to_id:\n",
    "            print(self._category_to_id)\n",
    "            raise Exception(\"{} is not in our category\".format(category))\n",
    "            \n",
    "        return self._category_to_id[category]\n",
    "          \n",
    "vocab = Vocab(vocab_file, hps.num_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "\n",
    "tf.logging.info('vocab_size: {}'.format(vocab_size))\n",
    "category_vocab = CategoryDict(category_file)\n",
    "num_classes = category_vocab.size()\n",
    "tf.logging.info('category_size: {}'.format(num_classes))\n",
    "test_str = '女人'\n",
    "tf.logging.info('id:{}'.format(category_vocab.category_to_id(test_str)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Loading data from .\\deep_learn\\sohu_seg_train_file.txt\n",
      "INFO:tensorflow:Loading data from .\\deep_learn\\sohu_seg_test_file.txt\n",
      "(array([[    0,    13,    63, ...,     1,    45,   175],\n       [31051, 27947,    93, ..., 27947,     5, 59416]]), array([ 2, 11]))\n(array([[ 5616,   371,  2335, ..., 10866,    95,    14],\n       [ 1150,    18,   344, ...,     3,   190,  1652]]), array([7, 1]))\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "'(array([[ 3879,  3048,     0,  1302,  6449,  9225, 32020, 75465,    26,\\n          938,    25, 42449,  1743,  5589,  1071, 28916,   875,     7,\\n         1071,  9306, 13826, 20919,   190,     1,  5614,  7024,   359,\\n        68957,   559,  9255, 11563,   172,  2431,   627,  7521,    51,\\n         5280, 15051,    50,    94,     0,     0,    65,   938, 42449,\\n           90,  2603,  2234,     8,  3420],\\n       [  192,   116,   310, 21315,  4187,  7290,  2223,  5918,    11,\\n        33789,    12,   922,    13,  6187,  1061,     7, 37987,  4187,\\n            2,   192,   116,   310,   880, 25294,     1,   104,   675,\\n         9024,     2,  1643,    10,    11,  8970,    12,     3,   968,\\n            1,     7,    11,  6890,   639,    13,   310,    42,     9,\\n            4,     4,    18,  3163,  3163]]), array([ 9, 10]))\\n'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 4
    }
   ],
   "source": [
    "# \n",
    "class TextDataSet:\n",
    "    def __init__(self, filename, vocab, category_vocab, num_timesteps):\n",
    "        self._vocab = vocab\n",
    "        self._category_vocab = category_vocab\n",
    "        self._num_timesteps = num_timesteps\n",
    "        \n",
    "        self._inputs = []\n",
    "        self._outputs = []\n",
    "        \n",
    "        self._indicator = 0\n",
    "        self._parse_file(filename)\n",
    "        \n",
    "    def _parse_file(self, filename):\n",
    "        tf.logging.info('Loading data from {}'.format(filename))\n",
    "        lines = 0\n",
    "        import re\n",
    "        if re.findall('train', filename):\n",
    "            with open(filename, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "        elif re.findall('test', filename):\n",
    "            with open(filename, 'r', encoding='utf-8-sig') as f:\n",
    "                lines = f.readlines()  \n",
    "        for line in lines:\n",
    "            label, content = line.strip('\\r\\n').split('\\t')\n",
    "            id_label = self._category_vocab.category_to_id(label)\n",
    "            id_words = self._vocab.sentence_to_id(content)\n",
    "            \n",
    "            id_words = id_words[0: self._num_timesteps]\n",
    "            padding_num = self._num_timesteps - len(id_words)\n",
    "            id_words = id_words + [\n",
    "                self._vocab.unk for i in range(padding_num)]\n",
    "            self._inputs.append(id_words)\n",
    "            self._outputs.append(id_label)\n",
    "\n",
    "        \n",
    "        self._inputs = np.asarray(self._inputs, dtype=np.int32)\n",
    "        self._outputs = np.asarray(self._outputs, dtype=np.int32)\n",
    "        self._random_shuffle()\n",
    "        \n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(len(self._inputs))\n",
    "        self._inputs = self._inputs[p]\n",
    "        self._outputs = self._outputs[p]\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            raise Exception(\"batch size: {} is too large\".format(batch_size))\n",
    "        \n",
    "        batch_inputs = self._inputs[self._indicator: end_indicator]\n",
    "        batch_output = self._outputs[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_inputs, batch_output\n",
    "\n",
    "train_dataset = TextDataSet(train_file, vocab, category_vocab, hps.num_timesteps)\n",
    "test_dataset = TextDataSet(test_file, vocab, category_vocab, hps.num_timesteps)\n",
    "\n",
    "print(train_dataset.next_batch(2))\n",
    "print(test_dataset.next_batch(2))\n",
    "# 因为next_batch == 2 ,输出为\n",
    "'''(array([[ 3879,  3048,     0,  1302,  6449,  9225, 32020, 75465,    26,\n",
    "          938,    25, 42449,  1743,  5589,  1071, 28916,   875,     7,\n",
    "         1071,  9306, 13826, 20919,   190,     1,  5614,  7024,   359,\n",
    "        68957,   559,  9255, 11563,   172,  2431,   627,  7521,    51,\n",
    "         5280, 15051,    50,    94,     0,     0,    65,   938, 42449,\n",
    "           90,  2603,  2234,     8,  3420],\n",
    "       [  192,   116,   310, 21315,  4187,  7290,  2223,  5918,    11,\n",
    "        33789,    12,   922,    13,  6187,  1061,     7, 37987,  4187,\n",
    "            2,   192,   116,   310,   880, 25294,     1,   104,   675,\n",
    "         9024,     2,  1643,    10,    11,  8970,    12,     3,   968,\n",
    "            1,     7,    11,  6890,   639,    13,   310,    42,     9,\n",
    "            4,     4,    18,  3163,  3163]]), array([ 9, 10]))\n",
    "'''\n",
    "# 只选择句子前50个分词, 输出为2*50的矩阵,第一个为句子分词的索引id, 第二个为分类的索引id\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-8f7bfbd65579>:130: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From <ipython-input-5-8f7bfbd65579>:136: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.dense instead.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-5-8f7bfbd65579>:150: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.math.argmax` instead\n",
      "INFO:tensorflow:variable name: embedding/embedding:0\n",
      "INFO:tensorflow:variable name: lstm_nn/inputs/x_weights:0\n",
      "INFO:tensorflow:variable name: lstm_nn/inputs/h_weights:0\n",
      "INFO:tensorflow:variable name: lstm_nn/inputs/biases:0\n",
      "INFO:tensorflow:variable name: lstm_nn/outputs/x_weights:0\n",
      "INFO:tensorflow:variable name: lstm_nn/outputs/h_weights:0\n",
      "INFO:tensorflow:variable name: lstm_nn/outputs/biases:0\n",
      "INFO:tensorflow:variable name: lstm_nn/forget/x_weights:0\n",
      "INFO:tensorflow:variable name: lstm_nn/forget/h_weights:0\n",
      "INFO:tensorflow:variable name: lstm_nn/forget/biases:0\n",
      "INFO:tensorflow:variable name: lstm_nn/memory/x_weights:0\n",
      "INFO:tensorflow:variable name: lstm_nn/memory/h_weights:0\n",
      "INFO:tensorflow:variable name: lstm_nn/memory/biases:0\n",
      "INFO:tensorflow:variable name: fc/fc1/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc1/bias:0\n",
      "INFO:tensorflow:variable name: fc/fc2/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc2/bias:0\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def create_model(hps, vocab_size, num_classes):\n",
    "    # 取一个句子的前50个分词, num_classes为固定的50个分词\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    # 训练批次大小\n",
    "    batch_size = hps.batch_size\n",
    "    # 输入为[批次的大小,50]\n",
    "    inputs = tf.placeholder(tf.int32, (batch_size, num_timesteps))\n",
    "    # 输出为[批次的大小,]\n",
    "    outputs = tf.placeholder(tf.int32, (batch_size, ))\n",
    "    # dropout的使用\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    # 保存训练到哪一步\n",
    "    global_step = tf.Variable(\n",
    "        tf.zeros([], tf.int64), name='global_step', trainable=False)\n",
    "    # 随机化embedding 编码\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0) \n",
    "    \n",
    "    with tf.variable_scope(\n",
    "        'embedding', initializer=embedding_initializer):\n",
    "        embeddings = tf.get_variable(\n",
    "            'embedding',\n",
    "            [vocab_size, hps.num_embedding_size],\n",
    "            tf.float32)\n",
    "        # 把输入的分词中的id -> embedding编码形式\n",
    "        # ex [1, 10, 7] -> [embeddings[1], embeddings[10], embeddings[7]]\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "    # 网络initializer的一种方法\n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_size + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    lstm_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    \n",
    "    def _generate_params_for_lstm_cell(x_size, h_size, bias_size):\n",
    "        \"\"\"generates parameters for pure lstm implementation\"\"\"\n",
    "        x_w = tf.get_variable('x_weights', x_size)\n",
    "        h_w = tf.get_variable('h_weights', h_size)\n",
    "        b = tf.get_variable('biases', bias_size,\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "        return x_w, h_w, b\n",
    "    # 构建lstm\n",
    "    with tf.variable_scope('lstm_nn', initializer=lstm_init):\n",
    "        '''\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_laysers):\n",
    "            # 循环初始化lstm\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "                hps.num_lstm_nodes[i],\n",
    "                state_is_tuple = True\n",
    "            )\n",
    "            # 使用dropout方法\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(\n",
    "                cell,\n",
    "                output_keep_prob = keep_prob\n",
    "            )\n",
    "            cells.append(cell)\n",
    "        # 合并两个cell\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        # 初始化cell内的值\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        # run_outputs: [batch_size, num_timesteps, lstm_outpus[-1]]\n",
    "        run_outpus, _ = tf.nn.dynamic_rnn(\n",
    "            cell, embed_inputs, initial_state=initial_state\n",
    "        )\n",
    "        print(run_outpus)\n",
    "        last = run_outpus[:, -1, :]\n",
    "        '''\n",
    "        # lstm的输入门初始变量\n",
    "        with tf.variable_scope('inputs'):\n",
    "            ix, ih, ib = _generate_params_for_lstm_cell(\n",
    "                x_size=[hps.num_embedding_size, hps.num_lstm_nodes[0]],\n",
    "                h_size=[hps.num_lstm_nodes[0], hps.num_lstm_nodes[0]],\n",
    "                bias_size=[1, hps.num_lstm_nodes[0]]\n",
    "            )\n",
    "        # lstm的输出门初始变量\n",
    "        with tf.variable_scope('outputs'):\n",
    "            ox, oh, ob = _generate_params_for_lstm_cell(\n",
    "                x_size=[hps.num_embedding_size, hps.num_lstm_nodes[0]],\n",
    "                h_size=[hps.num_lstm_nodes[0], hps.num_lstm_nodes[0]],\n",
    "                bias_size=[1, hps.num_lstm_nodes[0]]\n",
    "            )       \n",
    "        # lstm的遗忘门初始变量\n",
    "        with tf.variable_scope('forget'):\n",
    "            fx, fh, fb = _generate_params_for_lstm_cell(\n",
    "                x_size=[hps.num_embedding_size, hps.num_lstm_nodes[0]],\n",
    "                h_size=[hps.num_lstm_nodes[0], hps.num_lstm_nodes[0]],\n",
    "                bias_size=[1, hps.num_lstm_nodes[0]]\n",
    "            )  \n",
    "        # lstm的tanh的变换初始变量\n",
    "        with tf.variable_scope('memory'):\n",
    "            cx, ch, cb = _generate_params_for_lstm_cell(\n",
    "                x_size=[hps.num_embedding_size, hps.num_lstm_nodes[0]],\n",
    "                h_size=[hps.num_lstm_nodes[0], hps.num_lstm_nodes[0]],\n",
    "                bias_size=[1, hps.num_lstm_nodes[0]]\n",
    "            )              \n",
    "        # 横向隐含状态初始变量\n",
    "        state = tf.Variable(\n",
    "            tf.zeros([batch_size, hps.num_lstm_nodes[0]]),\n",
    "            trainable=False\n",
    "        )\n",
    "        # 初始化的h初始变量（候选记忆细胞）\n",
    "        h = tf.Variable(\n",
    "            tf.zeros([batch_size, hps.num_lstm_nodes[0]]),\n",
    "            trainable=False\n",
    "        )\n",
    "        # https://www.cnblogs.com/mantch/p/11369812.html LSTM结构详解\n",
    "        for i in range(num_timesteps):\n",
    "            # [batch_size, 1, embed_size]\n",
    "            embed_input = embed_inputs[:, i, :]\n",
    "            embed_input = tf.reshape(embed_input,\n",
    "                                     [batch_size, hps.num_embedding_size])\n",
    "            # 遗忘门\n",
    "            forget_gate = tf.sigmoid(\n",
    "                tf.matmul(embed_input, fx) + tf.matmul(h, fh) + fb\n",
    "            )\n",
    "            # 输入门\n",
    "            input_gate = tf.sigmoid(\n",
    "                tf.matmul(embed_input, ix) + tf.matmul(h, ih) + ib\n",
    "            )\n",
    "            # 输出门\n",
    "            output_gate = tf.sigmoid(\n",
    "                tf.matmul(embed_input, ox) + tf.matmul(h, oh) + ob\n",
    "            )\n",
    "            min_state = tf.tanh(\n",
    "                tf.matmul(embed_input, cx) + tf.matmul(h, ch) + cb\n",
    "            )\n",
    "            state = min_state + input_gate + state + forget_gate\n",
    "            h = output_gate * tf.tanh(state)\n",
    "        last = h\n",
    "        \n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    # lstm连接到全连接层\n",
    "    with tf.variable_scope('fc', initializer=fc_init):\n",
    "        fc1 = tf.layers.dense(last,\n",
    "                              hps.num_fc_nodes,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name='fc1')\n",
    "        # 使用dropout方法\n",
    "        fc1_dropout = tf.contrib.layers.dropout(fc1, keep_prob)\n",
    "        logits = tf.layers.dense(fc1_dropout,\n",
    "                                 num_classes,\n",
    "                                 name='fc2')\n",
    "    # 计算损失函数\n",
    "    with tf.name_scope('metrics'):\n",
    "        sofmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels= outputs\n",
    "        )\n",
    "        loss = tf.reduce_mean(sofmax_loss)\n",
    "        y_pred = tf.arg_max(tf.nn.softmax(logits=logits),\n",
    "                            1,\n",
    "                            output_type= tf.int32)\n",
    "        correct_pred = tf.equal(outputs, y_pred)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    # 构建train_op\n",
    "    with tf.name_scope('train_op'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            tf.logging.info('variable name: {}'.format(var.name))\n",
    "        # 限制训练时的梯度大小,使得不会出现梯度爆炸\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(loss, tvars), hps.clip_lstm_grads\n",
    "        )\n",
    "        # 梯度应用到变量中去\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars), global_step= global_step\n",
    "        )\n",
    "    \n",
    "    return ((inputs, outputs, keep_prob),\n",
    "            (loss, accuracy),\n",
    "            (train_op, global_step))\n",
    "\n",
    "placeholders, metrics, others = create_model(\n",
    "    hps, vocab_size, num_classes\n",
    ")\n",
    "inputs, outputs, keep_prod = placeholders\n",
    "loss, accuracy = metrics\n",
    "train_op, global_step = others"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Train Step: 100, loss: 2.388051748275757, accuracy: 0.1899999976158142\n",
      "INFO:tensorflow:Train Step: 200, loss: 2.319303274154663, accuracy: 0.18000000715255737\n",
      "INFO:tensorflow:Train Step: 300, loss: 2.2724437713623047, accuracy: 0.20999999344348907\n",
      "INFO:tensorflow:Train Step: 400, loss: 2.348346471786499, accuracy: 0.11999999731779099\n",
      "INFO:tensorflow:Train Step: 500, loss: 2.2883803844451904, accuracy: 0.10999999940395355\n",
      "INFO:tensorflow:Train Step: 600, loss: 2.2187585830688477, accuracy: 0.15000000596046448\n",
      "INFO:tensorflow:Train Step: 700, loss: 2.273815870285034, accuracy: 0.1599999964237213\n",
      "INFO:tensorflow:Train Step: 800, loss: 2.140721559524536, accuracy: 0.25\n",
      "INFO:tensorflow:Train Step: 900, loss: 2.2136003971099854, accuracy: 0.1899999976158142\n",
      "INFO:tensorflow:Train Step: 1000, loss: 2.1126694679260254, accuracy: 0.23000000417232513\n",
      "INFO:tensorflow:------Test Step: 1000, loss: 1.9916354417800903, accuracy: 0.22930000722408295\n",
      "INFO:tensorflow:Train Step: 1200, loss: 1.807185173034668, accuracy: 0.3400000035762787\n",
      "INFO:tensorflow:Train Step: 1300, loss: 1.8892314434051514, accuracy: 0.27000001072883606\n",
      "INFO:tensorflow:Train Step: 1400, loss: 1.9385796785354614, accuracy: 0.2199999988079071\n",
      "INFO:tensorflow:Train Step: 1500, loss: 1.8082202672958374, accuracy: 0.33000001311302185\n",
      "INFO:tensorflow:Train Step: 1600, loss: 1.8529444932937622, accuracy: 0.30000001192092896\n",
      "INFO:tensorflow:Train Step: 1700, loss: 1.857688307762146, accuracy: 0.23999999463558197\n",
      "INFO:tensorflow:Train Step: 1800, loss: 1.848873496055603, accuracy: 0.33000001311302185\n",
      "INFO:tensorflow:Train Step: 1900, loss: 1.866414189338684, accuracy: 0.2800000011920929\n",
      "INFO:tensorflow:Train Step: 2000, loss: 1.5967669486999512, accuracy: 0.4000000059604645\n",
      "INFO:tensorflow:------Test Step: 2000, loss: 1.8723549842834473, accuracy: 0.2952999770641327\n",
      "INFO:tensorflow:Train Step: 2200, loss: 1.5814396142959595, accuracy: 0.3799999952316284\n",
      "INFO:tensorflow:Train Step: 2300, loss: 1.585400104522705, accuracy: 0.4399999976158142\n",
      "INFO:tensorflow:Train Step: 2400, loss: 1.590401291847229, accuracy: 0.3499999940395355\n",
      "INFO:tensorflow:Train Step: 2500, loss: 1.6206402778625488, accuracy: 0.41999998688697815\n",
      "INFO:tensorflow:Train Step: 2600, loss: 1.6015801429748535, accuracy: 0.3799999952316284\n",
      "INFO:tensorflow:Train Step: 2700, loss: 1.5912599563598633, accuracy: 0.36000001430511475\n",
      "INFO:tensorflow:Train Step: 2800, loss: 1.5069785118103027, accuracy: 0.4399999976158142\n",
      "INFO:tensorflow:Train Step: 2900, loss: 1.5640980005264282, accuracy: 0.4300000071525574\n",
      "INFO:tensorflow:Train Step: 3000, loss: 1.5028789043426514, accuracy: 0.38999998569488525\n",
      "INFO:tensorflow:------Test Step: 3000, loss: 1.8038498163223267, accuracy: 0.34860000014305115\n",
      "INFO:tensorflow:Train Step: 3200, loss: 1.5413039922714233, accuracy: 0.41999998688697815\n",
      "INFO:tensorflow:Train Step: 3300, loss: 1.507624864578247, accuracy: 0.41999998688697815\n",
      "INFO:tensorflow:Train Step: 3400, loss: 1.4520875215530396, accuracy: 0.4699999988079071\n",
      "INFO:tensorflow:Train Step: 3500, loss: 1.3162750005722046, accuracy: 0.44999998807907104\n",
      "INFO:tensorflow:Train Step: 3600, loss: 1.3912780284881592, accuracy: 0.4000000059604645\n",
      "INFO:tensorflow:Train Step: 3700, loss: 1.5487879514694214, accuracy: 0.38999998569488525\n",
      "INFO:tensorflow:Train Step: 3800, loss: 1.4412832260131836, accuracy: 0.4399999976158142\n",
      "INFO:tensorflow:Train Step: 3900, loss: 1.3985052108764648, accuracy: 0.4699999988079071\n",
      "INFO:tensorflow:Train Step: 4000, loss: 1.3319134712219238, accuracy: 0.4300000071525574\n",
      "INFO:tensorflow:------Test Step: 4000, loss: 1.6872886419296265, accuracy: 0.38679996132850647\n",
      "INFO:tensorflow:Train Step: 4200, loss: 1.2735010385513306, accuracy: 0.46000000834465027\n",
      "INFO:tensorflow:Train Step: 4300, loss: 1.2039778232574463, accuracy: 0.6000000238418579\n",
      "INFO:tensorflow:Train Step: 4400, loss: 1.217349886894226, accuracy: 0.5299999713897705\n",
      "INFO:tensorflow:Train Step: 4500, loss: 1.3047839403152466, accuracy: 0.46000000834465027\n",
      "INFO:tensorflow:Train Step: 4600, loss: 1.3137054443359375, accuracy: 0.5299999713897705\n",
      "INFO:tensorflow:Train Step: 4700, loss: 1.3227823972702026, accuracy: 0.5199999809265137\n",
      "INFO:tensorflow:Train Step: 4800, loss: 1.0753480195999146, accuracy: 0.6000000238418579\n",
      "INFO:tensorflow:Train Step: 4900, loss: 1.3319685459136963, accuracy: 0.550000011920929\n",
      "INFO:tensorflow:Train Step: 5000, loss: 1.2390522956848145, accuracy: 0.550000011920929\n",
      "INFO:tensorflow:------Test Step: 5000, loss: 1.4699522256851196, accuracy: 0.4285000264644623\n",
      "INFO:tensorflow:Train Step: 5200, loss: 1.2186694145202637, accuracy: 0.5400000214576721\n",
      "INFO:tensorflow:Train Step: 5300, loss: 1.1443580389022827, accuracy: 0.550000011920929\n",
      "INFO:tensorflow:Train Step: 5400, loss: 1.2075477838516235, accuracy: 0.4699999988079071\n",
      "INFO:tensorflow:Train Step: 5500, loss: 1.1981319189071655, accuracy: 0.6000000238418579\n",
      "INFO:tensorflow:Train Step: 5600, loss: 1.2828882932662964, accuracy: 0.4699999988079071\n",
      "INFO:tensorflow:Train Step: 5700, loss: 1.0124642848968506, accuracy: 0.6399999856948853\n",
      "INFO:tensorflow:Train Step: 5800, loss: 1.0106284618377686, accuracy: 0.5899999737739563\n",
      "INFO:tensorflow:Train Step: 5900, loss: 1.0878018140792847, accuracy: 0.5699999928474426\n",
      "INFO:tensorflow:Train Step: 6000, loss: 1.0348734855651855, accuracy: 0.5299999713897705\n",
      "INFO:tensorflow:------Test Step: 6000, loss: 1.543936014175415, accuracy: 0.47510001063346863\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c862fec3741e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m                                 \u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                                 \u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                                    \u001b[0mkeep_prod\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_keep_prob_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m                                })\n\u001b[0;32m     22\u001b[0m         \u001b[0mloss_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ],
   "source": [
    "# train:\n",
    "init_op = tf.global_variables_initializer()\n",
    "train_keep_prob_value = 0.8\n",
    "test_keep_prob_value = 1.0\n",
    "\n",
    "test_steps = 100\n",
    "num_train_steps = 10000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for i in range(num_train_steps):\n",
    "        batch_inputs, batch_labels = train_dataset.next_batch(\n",
    "            hps.batch_size\n",
    "        )\n",
    "        outputs_val = sess.run([loss, accuracy, train_op, global_step],\n",
    "                               feed_dict = {\n",
    "                                inputs: batch_inputs,\n",
    "                                outputs: batch_labels,\n",
    "                                   keep_prod: train_keep_prob_value,\n",
    "                               })\n",
    "        loss_val, accuracy_val, _, global_step_val = outputs_val\n",
    "        if global_step_val % 100 == 0:\n",
    "            tf.logging.info(\"Train Step: {}, loss: {}, accuracy: {}\".format(global_step_val, loss_val, accuracy_val))\n",
    "\n",
    "        if global_step_val % 1000 == 0:\n",
    "            all_test_acc_cal = []\n",
    "            for j in range(test_steps):\n",
    "                test_inputs, test_labels = test_dataset.next_batch(hps.batch_size)\n",
    "                test_val = sess.run([loss, accuracy, train_op, global_step],\n",
    "                                    feed_dict= {\n",
    "                                        inputs: test_inputs,\n",
    "                                        outputs: test_labels,\n",
    "                                        keep_prod: test_keep_prob_value,\n",
    "                                    })\n",
    "                test_loss_val, test_accuarcy_val, _, test_step_val = test_val\n",
    "                all_test_acc_cal.append(test_accuarcy_val)\n",
    "            test_acc = np.mean(all_test_acc_cal)\n",
    "            tf.logging.info(\"------Test Step: {}, loss: {}, accuracy: {}\".format(global_step_val, test_loss_val, test_acc))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
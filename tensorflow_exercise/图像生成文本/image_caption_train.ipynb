{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\nIf you depend on functionality not listed there, please file an issue.\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Data generator\n",
    "    a. Loads vocab\n",
    "    b. Loads image features\n",
    "    c. provide data for training.\n",
    "2. Builds image caption model\n",
    "3. Trains the model\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import gfile\n",
    "from tensorflow import logging\n",
    "import pprint\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "input_description_file = r\".\\deep_learn\\image2text\\results_20130124.token\"\n",
    "input_img_feature_dir = r\".\\deep_learn\\image2text\\download_inception_v3_features\"\n",
    "input_vocab_file = r\".\\deep_learn\\image2text\\vocab.txt\"\n",
    "output_dir = r\".\\deep_learn\\image2text\\local_run\"\n",
    "\n",
    "if not  gfile.Exists(output_dir):\n",
    "    gfile.MakeDirs(output_dir)\n",
    "\n",
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        # 过滤低频率词汇\n",
    "        num_vocab_word_threshold = 3,\n",
    "        num_embedding_nodes = 32,\n",
    "        num_timesteps = 10,\n",
    "        num_lstm_nodes = [64, 64],\n",
    "        num_lstm_layers = 2,\n",
    "        num_fc_nodes = 32,\n",
    "        batch_size = 80,\n",
    "        cell_type = \"lstm\",\n",
    "        clip_lstm_grads = 1.0,\n",
    "        learning_rate = 0.001,\n",
    "        keep_prob = 0.8,\n",
    "        log_frequent = 100,\n",
    "        save_frequent = 1000,\n",
    "    )\n",
    "\n",
    "hps = get_default_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "INFO:tensorflow:vocab_size : 10875\n",
      "[1494, 389, 1, 0]\n'the of man shirt'\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, filename, word_num_threshold):\n",
    "        self._id_to_word = {}\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._eos = -1\n",
    "        self._word_num_threshold = word_num_threshold\n",
    "        self._read_dict(filename)\n",
    "        \n",
    "    def _read_dict(self, filename):\n",
    "        with gfile.GFile(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            \n",
    "            word, occurrence = line.strip('\\r\\n').split('\\t')\n",
    "            occurrence = int(occurrence)\n",
    "            if occurrence < self._word_num_threshold:\n",
    "                continue\n",
    "            idx = len(self._id_to_word)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            elif word == '.':\n",
    "                self._eos = idx\n",
    "            if word in self._word_to_id or idx in self._id_to_word:\n",
    "                raise Exception(\"duplicate words in vocab\")\n",
    "            self._word_to_id[word] = idx\n",
    "            self._id_to_word[idx] = word\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    @property\n",
    "    def eos(self):\n",
    "        return self._eos\n",
    "    \n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self._unk)\n",
    "    \n",
    "    def id_to_word(self, word_id):\n",
    "        return self._id_to_word.get(word_id, '<UNK>')\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._id_to_word)\n",
    "    \n",
    "    def encode(self, sentence):\n",
    "        return [self.word_to_id(word) for word in sentence.split(' ')]\n",
    "    \n",
    "    def decode(self, sentence_id):\n",
    "        words =  [self.id_to_word(word_id) for word_id in sentence_id]\n",
    "        return ' '.join(words)\n",
    "\n",
    "vocab = Vocab(input_vocab_file, hps.num_vocab_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "logging.info('vocab_size : {}'.format(vocab_size))\n",
    "\n",
    "pprint.pprint(vocab.encode(\"I have a dream.\"))\n",
    "pprint.pprint(vocab.decode([5, 10, 9, 21]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "INFO:tensorflow:num of all imgs: 31783\n",
      "['A man in jeans is reclining on a green metal bench along a busy sidewalk and '\n 'crowded street .',\n 'A white male with a blue sweater and gray pants laying on a sidewalk bench .',\n 'A man in a blue shirt and gray pants is sleeping on a sidewalk bench .',\n 'A person is sleeping on a bench , next to cars .',\n 'A man sleeping on a bench in a city area .']\nINFO:tensorflow:num of all imgs: 31783\n",
      "[[3, 9, 4, 132, 8, 3532, 6, 1, 48, 337, 146, 139, 1, 244, 93, 7, 380, 36, 2],\n [3, 20, 179, 11, 1, 26, 284, 7, 120, 128, 297, 6, 1, 93, 146, 2],\n [3, 9, 4, 1, 26, 21, 7, 120, 128, 8, 340, 6, 1, 93, 146, 2],\n [3, 63, 8, 340, 6, 1, 146, 12, 70, 15, 518, 2],\n [3, 9, 340, 6, 1, 146, 4, 1, 112, 171, 2]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def parse_token_file(token_file):\n",
    "    \"\"\"Parses image description file.\"\"\"\n",
    "    img_name_to_tokens = {}\n",
    "    with gfile.GFile(token_file, 'r', ) as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        img_id, description = line.strip('\\r\\n').split('\\t')\n",
    "        img_name, _ = img_id.split('#')\n",
    "        img_name_to_tokens.setdefault(img_name, [])\n",
    "        img_name_to_tokens[img_name].append(description)\n",
    "    return img_name_to_tokens\n",
    "\n",
    "def convert_token_to_id(img_name_to_tokens, vocab):\n",
    "    \"\"\"Converts tokens of each description of imgs to id.\"\"\"\n",
    "    img_name_to_tokens_id = {}\n",
    "    for img_name in img_name_to_tokens:\n",
    "        img_name_to_tokens_id.setdefault(img_name, [])\n",
    "        for description in img_name_to_tokens[img_name]:\n",
    "            token_ids = vocab.encode(description)\n",
    "            img_name_to_tokens_id[img_name].append(token_ids)\n",
    "    return img_name_to_tokens_id\n",
    "\n",
    "img_name_to_tokens = parse_token_file(input_description_file)\n",
    "img_name_to_tokens_id = convert_token_to_id(img_name_to_tokens, vocab)\n",
    "\n",
    "logging.info(\"num of all imgs: {}\".format(len(img_name_to_tokens)))\n",
    "pprint.pprint(img_name_to_tokens['2778832101.jpg'])\n",
    "logging.info(\"num of all imgs: {}\".format(len(img_name_to_tokens_id)))\n",
    "pprint.pprint(img_name_to_tokens_id['2778832101.jpg'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-0.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-1.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-10.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-11.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-12.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-13.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-14.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-15.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-16.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-17.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-18.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-19.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-2.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-20.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-21.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-22.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-23.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-24.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-25.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-26.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-27.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-28.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-29.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-3.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-30.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-31.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-4.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-5.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-6.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-7.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-8.pickle',\n '.\\\\deep_learn\\\\image2text\\\\download_inception_v3_features\\\\image_features-9.pickle']\nINFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-0.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-1.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-10.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-11.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-12.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-13.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-14.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-15.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-16.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-17.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-18.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-19.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-2.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-20.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-21.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-22.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-23.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-24.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-25.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-26.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-27.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-28.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-29.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-3.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-30.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-31.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-4.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-5.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-6.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-7.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-8.pickle\n",
      "INFO:tensorflow:loading .\\deep_learn\\image2text\\download_inception_v3_features\\image_features-9.pickle\n",
      "(31783, 2048)\n(31783,)\n",
      "INFO:tensorflow:img_feature_dim: 2048\n",
      "INFO:tensorflow:caption_data_size: 31783\n",
      "array([[0.25741392, 1.233734  , 0.34878004, ..., 0.06556617, 0.00271783,\n        0.05481452],\n       [0.08573674, 0.3077234 , 0.05328725, ..., 0.19580363, 0.28018603,\n        0.3523698 ],\n       [1.9507629 , 0.15958266, 0.29698613, ..., 0.4804738 , 0.06262321,\n        0.04273409],\n       [0.02441712, 0.19991331, 0.2539213 , ..., 0.08636304, 0.21047041,\n        0.15270972],\n       [0.31966874, 0.07895095, 0.2011755 , ..., 0.4296467 , 0.12850341,\n        0.03609855]], dtype=float32)\narray([[ 153,   69,    4,  564,  541,  138,    6,    1,   89,  656],\n       [ 248,   41,  710, 1314,    2,    2,    2,    2,    2,    2],\n       [   3,   13,   32,   92,    1, 1176,    8,  605,   15,   19],\n       [   3,   13,    8,  860,  307,    4,    5,   43,    2,    2],\n       [  47,  723,   98,   35,  415,    6,    5,   89,    2,    2]])\narray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])\narray(['5621180809.jpg', '4804878120.jpg', '2653034718.jpg',\n       '2915400809.jpg', '7720725874.jpg'], dtype='<U14')\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "class ImageCaptionData:\n",
    "    \"\"\"Provides data for image caption model.\"\"\"\n",
    "    def __init__(self,\n",
    "                 img_name_to_tokens_id,\n",
    "                 img_feature_dir,\n",
    "                 num_timesteps,\n",
    "                 vocab,\n",
    "                 deterministic = False):\n",
    "        self._vocab = vocab\n",
    "        self._img_name_to_tokens_id = img_name_to_tokens_id\n",
    "        # 截取句子的长度\n",
    "        self._num_timesteps = num_timesteps\n",
    "        # 是否要进行shuffle()随机打乱\n",
    "        self._deterministic = deterministic\n",
    "        self._indicator = 0\n",
    "        # 图片名\n",
    "        self._img_feature_filenames = []\n",
    "        # 图片特征\n",
    "        self._img_feature_data = []\n",
    "        \n",
    "        self._all_img_feature_filepaths = []\n",
    "        for filename in gfile.ListDirectory(img_feature_dir):\n",
    "            if not filename[0] == '.':\n",
    "                self._all_img_feature_filepaths.append(\n",
    "                    os.path.join(img_feature_dir, filename)\n",
    "                )\n",
    "        pprint.pprint(self._all_img_feature_filepaths)\n",
    "        self._load_img_feature_pickle()\n",
    "        \n",
    "        if not self._deterministic:\n",
    "            self._random_shuffle()\n",
    "            \n",
    "    def _load_img_feature_pickle(self):\n",
    "        \"\"\"Loads img feature data from pickle\"\"\"\n",
    "        for filepath in self._all_img_feature_filepaths:\n",
    "            logging.info(\"loading {}\".format(filepath))\n",
    "            with gfile.GFile(filepath, 'rb') as f:\n",
    "                filenames, features = pickle.load(f)\n",
    "                # 图片名字列表的合并\n",
    "                self._img_feature_filenames += filenames\n",
    "                self._img_feature_data.append(features)\n",
    "        #vstack [# (1000, 1, 1, 2048, #(1000, 1, 1, 2048] -> #(2000, 1, 1, 2048)\n",
    "        self._img_feature_data = np.vstack(self._img_feature_data)\n",
    "        origin_shape = self._img_feature_data.shape\n",
    "        self._img_feature_data = np.reshape(self._img_feature_data,\n",
    "                                            (origin_shape[0], origin_shape[3]))\n",
    "        self._img_feature_filenames = np.asarray(self._img_feature_filenames)\n",
    "        print(self._img_feature_data.shape)\n",
    "        print(self._img_feature_filenames.shape)\n",
    "        \n",
    "    def size(self):\n",
    "        return len(self._img_feature_filenames)\n",
    "    \n",
    "    def _random_shuffle(self):\n",
    "        \"\"\"Shuffle data randomly.\"\"\"\n",
    "        p = np.random.permutation(self.size())\n",
    "        self._img_feature_filenames = self._img_feature_filenames[p]\n",
    "        self._img_feature_data = self._img_feature_data[p]\n",
    "    # 图片特征的维度\n",
    "    def img_feature_size(self):\n",
    "        return self._img_feature_data.shape[1]\n",
    "    # 选择文件描述\n",
    "    def _img_desc(self, batch_filenames):\n",
    "        \"\"\"Gets description for filenames in batch.\"\"\"\n",
    "        batch_sentence_ids = []\n",
    "        batch_weights = []\n",
    "        for filename in batch_filenames:\n",
    "            token_ids_set = self._img_name_to_tokens_id[filename]\n",
    "            chosen_token_ids = random.choice(token_ids_set)\n",
    "            chosen_token_ids_length = len(chosen_token_ids)\n",
    "            # 填充为1\n",
    "            weight = [1 for i in range(chosen_token_ids_length)]\n",
    "            if chosen_token_ids_length >= self._num_timesteps:\n",
    "                chosen_token_ids = chosen_token_ids[0: self._num_timesteps]\n",
    "                weight = weight[0: self._num_timesteps]\n",
    "            else:\n",
    "                remaing_length = self._num_timesteps - chosen_token_ids_length\n",
    "                # 用eos进行填充\n",
    "                chosen_token_ids += [self._vocab.eos for i in range(remaing_length)]\n",
    "                weight += [0 for i in range(remaing_length)]\n",
    "            batch_sentence_ids.append(chosen_token_ids)\n",
    "            batch_weights.append(weight)\n",
    "        batch_sentence_ids = np.asarray(batch_sentence_ids)\n",
    "        batch_weights = np.asarray(batch_weights)\n",
    "        return batch_sentence_ids, batch_weights\n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"Returns next batch size.\"\"\"\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self.size():\n",
    "            if not self._deterministic:\n",
    "                self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = self._indicator + batch_size\n",
    "        assert end_indicator < self.size()\n",
    "        \n",
    "        batch_filenames = self._img_feature_filenames[self._indicator: end_indicator]\n",
    "        batch_img_features = self._img_feature_data[self._indicator: end_indicator]\n",
    "        # batch_weights的作用为在 (sentence_ids: [100,101,123,4,5,0,0,0] -> [1,1,1,1,1,0,0,0] 0为 <UNK>未出现过的词) 把没有意义的词在训练时，梯度下降和loss计算中去掉\n",
    "        batch_sentence_ids, batch_weights = self._img_desc(batch_filenames)\n",
    "        self._indicator = end_indicator\n",
    "        return batch_img_features, batch_sentence_ids, batch_weights, batch_filenames\n",
    "    \n",
    "caption_data = ImageCaptionData(img_name_to_tokens_id,\n",
    "                                input_img_feature_dir,\n",
    "                                hps.num_timesteps,\n",
    "                                vocab)\n",
    "img_feature_dim = caption_data.img_feature_size()\n",
    "caption_data_size = caption_data.size()\n",
    "logging.info(\"img_feature_dim: {}\".format(img_feature_dim))\n",
    "logging.info(\"caption_data_size: {}\".format(caption_data_size))\n",
    "\n",
    "batch_img_features, batch_sentence_ids, batch_weights, batch_img_names = caption_data.next_batch(5) \n",
    "pprint.pprint(batch_img_features)\n",
    "pprint.pprint(batch_sentence_ids)\n",
    "pprint.pprint(batch_weights)\n",
    "pprint.pprint(batch_img_names)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-04be1d712af0>:56: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From <ipython-input-5-04be1d712af0>:63: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-04be1d712af0>:6: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-04be1d712af0>:79: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-04be1d712af0>:85: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-5-04be1d712af0>:94: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.dropout instead.\n",
      "INFO:tensorflow:variable name: embedding/embedding:0\n",
      "INFO:tensorflow:variable name: img_feature_embed/dense/kernel:0\n",
      "INFO:tensorflow:variable name: img_feature_embed/dense/bias:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: fc/fc1/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc1/bias:0\n",
      "INFO:tensorflow:variable name: fc/logits/kernel:0\n",
      "INFO:tensorflow:variable name: fc/logits/bias:0\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def create_rnn_cell(hidden_dim, cell_type):\n",
    "    \"\"\"Return specific cell according to cell_type.\"\"\"\n",
    "    if cell_type == 'lstm':\n",
    "        return tf.contrib.rnn.BasicLSTMCell(hidden_dim,\n",
    "                                            state_is_tuple=True)\n",
    "    elif cell_type == 'gru':\n",
    "        return tf.contrib.rnn.GRUCell(hidden_dim)\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"{} type has not been supported.\".format(cell_type))\n",
    "    \n",
    "def dropout(cell, keep_prob):\n",
    "    \"\"\"Wrap cell with dropout.\"\"\"\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob = keep_prob)\n",
    "\n",
    "def get_train_model(hps, vocab_size, img_feature_dim):\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "    img_feature = tf.placeholder(tf.float32,\n",
    "                                 (batch_size, img_feature_dim))\n",
    "    sentence = tf.placeholder(tf.int32, \n",
    "                              (batch_size, num_timesteps))\n",
    "    # 填充\n",
    "    mask = tf.placeholder(tf.int32, (batch_size, num_timesteps))\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    global_step = tf.Variable(tf.zeros([], tf.int32),\n",
    "                              name='global_step',\n",
    "                              trainable=False)\n",
    "    \n",
    "    # prediction process:\n",
    "    # sentence: [a, b, c, d, e]\n",
    "    # input: [img, a, b, c, d]\n",
    "    # img_feature: [0.4, 0.3, 10, 2]\n",
    "    # predict #1 : img_feature - >embedding_img - >lstm -> (a)\n",
    "    # predict #2 : a -> embedding_word -> lstm -> (b)\n",
    "    # predict #3 : b -> embedding_word -> lstm -> (c)\n",
    "    # .... 原始流程如上，但是我们这里把embedding_img设置成和embedding_word大小一样\n",
    "    \n",
    "    # Sets up embedding layer.\n",
    "    embedding_initializer = tf.random_uniform_initializer(-0.1, 1.0)\n",
    "    with tf.variable_scope('embedding',\n",
    "                           initializer=embedding_initializer):\n",
    "        # 句子转化成embedding \n",
    "        embeddings = tf.get_variable('embedding',\n",
    "                                     [vocab_size, hps.num_embedding_nodes],\n",
    "                                     tf.float32)\n",
    "        # embed_token_ids: [batch_size, num_timesteps-1, num_embedding_nodes]\n",
    "        embed_token_ids = tf.nn.embedding_lookup(\n",
    "            embeddings,\n",
    "            # \n",
    "            sentence[:, 0: num_timesteps -1 ]\n",
    "        )\n",
    "    img_feature_embed_init = tf.uniform_unit_scaling_initializer(\n",
    "        factor=1.0\n",
    "    )\n",
    "    with tf.variable_scope('img_feature_embed',\n",
    "                           initializer=img_feature_embed_init):\n",
    "        # img_features: [batch_size, img_feature_dim]\n",
    "        # embed_img : [batch_size, num_embedding_nodes]\n",
    "        embed_img = tf.layers.dense(img_feature,\n",
    "                                    hps.num_embedding_nodes)\n",
    "        # embed_img : [batch_size, 1, num_embedding_nodes]\n",
    "        embed_img = tf.expand_dims(embed_img, 1)\n",
    "        # embed_input : [batch_size, num_timestpes, num_embedding_nodes]\n",
    "        # 图片和句子embedding合并\n",
    "        embed_inputs = tf.concat([embed_img, embed_token_ids], axis=1)\n",
    "        \n",
    "    # Sets up rnn netword\n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_nodes + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    rnn_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    with tf.variable_scope('lstm_nn', initializer= rnn_init):\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_layers):\n",
    "            cell = create_rnn_cell(hps.num_lstm_nodes[i], hps.cell_type)\n",
    "            cell = dropout(cell, keep_prob)\n",
    "            cells.append(cell)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        \n",
    "        init_state = cell.zero_state(hps.batch_size, tf.float32)\n",
    "        # rnn_outputs: [batch_size, num_timesteps, hps.num_lstm_node[-1]\n",
    "        rnn_outputs, _ = tf.nn.dynamic_rnn(cell,\n",
    "                                           embed_inputs,\n",
    "                                           initial_state = init_state)\n",
    "    # Sets up fully-connected layer\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor= 1.0)\n",
    "    with tf.variable_scope('fc', initializer=fc_init):\n",
    "        rnn_outputs_2d = tf.reshape(rnn_outputs,\n",
    "                                    [-1, hps.num_lstm_nodes[-1]])\n",
    "        fc1 = tf.layers.dense(rnn_outputs_2d,\n",
    "                              hps.num_fc_nodes,\n",
    "                              name='fc1')\n",
    "        fc1_dropout = tf.layers.dropout(fc1, keep_prob)\n",
    "        fc1_relu = tf.nn.relu(fc1_dropout)\n",
    "        logits = tf.layers.dense(fc1_relu,\n",
    "                                 vocab_size,\n",
    "                                 name='logits')\n",
    "        \n",
    "    # Calculates loss\n",
    "    with tf.variable_scope('loss'):\n",
    "        sentence_flatten = tf.reshape(sentence, [-1])\n",
    "        mask_flatten = tf.reshape(mask, [-1])\n",
    "        mask_sum = tf.reduce_sum(mask_flatten)\n",
    "        \n",
    "        sofxmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits,\n",
    "            labels=sentence_flatten\n",
    "        )\n",
    "        # 去除填充的eso\n",
    "        weighted_softmax_loss = tf.multiply(\n",
    "            sofxmax_loss, tf.cast(mask_flatten, tf.float32)\n",
    "        )\n",
    "        loss = tf.reduce_sum(weighted_softmax_loss) / tf.cast(mask_sum, tf.float32)\n",
    "        \n",
    "        prediction = tf.argmax(logits, 1, output_type=tf.int32)\n",
    "        correct_prediction = tf.equal(prediction,\n",
    "                                      sentence_flatten)\n",
    "        weighted_correct_prediction = tf.multiply(\n",
    "            tf.cast(correct_prediction, tf.float32),\n",
    "            tf.cast(mask_flatten, tf.float32)\n",
    "        )\n",
    "        # accuracy = tf.reduce_sum(weighted_correct_prediction) / tf.cast(mask_sum, tf.float32)\n",
    "        accuracy = tf.reduce_sum(weighted_correct_prediction) / tf.cast(mask_sum, tf.float32)\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        \n",
    "    # Defines train op.\n",
    "    with tf.variable_scope('train_op'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            logging.info('variable name: {}'.format(var.name))\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(loss, tvars), hps.clip_lstm_grads\n",
    "        )\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars), global_step= global_step\n",
    "        )\n",
    "        \n",
    "    return ((img_feature, sentence, mask, keep_prob),\n",
    "            (loss, accuracy, train_op),\n",
    "            global_step)\n",
    "\n",
    "placehoders, metrics, global_step = get_train_model(\n",
    "    hps, vocab_size, img_feature_dim\n",
    ")\n",
    "img_feature, sentence, mask, keep_prob = placehoders\n",
    "loss, accuracy, train_op = metrics\n",
    "        \n",
    "summary_op = tf.summary.merge_all()\n",
    "init_op = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(max_to_keep=10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Step: 100, loss: 5.73, acc: 0.17\n",
      "INFO:tensorflow:Step: 200, loss: 5.37, acc: 0.17\n",
      "INFO:tensorflow:Step: 300, loss: 5.08, acc: 0.22\n",
      "INFO:tensorflow:Step: 400, loss: 4.90, acc: 0.24\n",
      "INFO:tensorflow:Step: 500, loss: 4.76, acc: 0.24\n",
      "INFO:tensorflow:Step: 600, loss: 4.54, acc: 0.26\n",
      "INFO:tensorflow:Step: 700, loss: 4.43, acc: 0.27\n",
      "INFO:tensorflow:Step: 800, loss: 4.42, acc: 0.29\n",
      "INFO:tensorflow:Step: 900, loss: 4.42, acc: 0.26\n",
      "INFO:tensorflow:Step: 1000, loss: 4.02, acc: 0.30\n",
      "INFO:tensorflow:Step: 1000, model saved \n",
      "INFO:tensorflow:Step: 1100, loss: 4.41, acc: 0.27\n",
      "INFO:tensorflow:Step: 1200, loss: 3.89, acc: 0.30\n",
      "INFO:tensorflow:Step: 1300, loss: 4.26, acc: 0.28\n",
      "INFO:tensorflow:Step: 1400, loss: 4.15, acc: 0.26\n",
      "INFO:tensorflow:Step: 1500, loss: 4.02, acc: 0.31\n",
      "INFO:tensorflow:Step: 1600, loss: 3.88, acc: 0.33\n",
      "INFO:tensorflow:Step: 1700, loss: 4.01, acc: 0.32\n",
      "INFO:tensorflow:Step: 1800, loss: 4.22, acc: 0.28\n",
      "INFO:tensorflow:Step: 1900, loss: 4.01, acc: 0.29\n",
      "INFO:tensorflow:Step: 2000, loss: 4.09, acc: 0.29\n",
      "INFO:tensorflow:Step: 2000, model saved \n",
      "INFO:tensorflow:Step: 2100, loss: 4.11, acc: 0.27\n",
      "INFO:tensorflow:Step: 2200, loss: 4.13, acc: 0.29\n",
      "INFO:tensorflow:Step: 2300, loss: 3.74, acc: 0.31\n",
      "INFO:tensorflow:Step: 2400, loss: 3.72, acc: 0.33\n",
      "INFO:tensorflow:Step: 2500, loss: 4.03, acc: 0.30\n",
      "INFO:tensorflow:Step: 2600, loss: 3.77, acc: 0.30\n",
      "INFO:tensorflow:Step: 2700, loss: 3.87, acc: 0.29\n",
      "INFO:tensorflow:Step: 2800, loss: 3.78, acc: 0.31\n",
      "INFO:tensorflow:Step: 2900, loss: 3.97, acc: 0.30\n",
      "INFO:tensorflow:Step: 3000, loss: 3.92, acc: 0.31\n",
      "INFO:tensorflow:Step: 3000, model saved \n",
      "INFO:tensorflow:Step: 3100, loss: 3.65, acc: 0.31\n",
      "INFO:tensorflow:Step: 3200, loss: 4.04, acc: 0.31\n",
      "INFO:tensorflow:Step: 3300, loss: 3.94, acc: 0.31\n",
      "INFO:tensorflow:Step: 3400, loss: 3.73, acc: 0.34\n",
      "INFO:tensorflow:Step: 3500, loss: 3.83, acc: 0.30\n",
      "INFO:tensorflow:Step: 3600, loss: 3.65, acc: 0.36\n",
      "INFO:tensorflow:Step: 3700, loss: 4.02, acc: 0.29\n",
      "INFO:tensorflow:Step: 3800, loss: 4.01, acc: 0.30\n",
      "INFO:tensorflow:Step: 3900, loss: 3.74, acc: 0.33\n",
      "INFO:tensorflow:Step: 4000, loss: 3.61, acc: 0.35\n",
      "INFO:tensorflow:Step: 4000, model saved \n",
      "INFO:tensorflow:Step: 4100, loss: 3.90, acc: 0.30\n",
      "INFO:tensorflow:Step: 4200, loss: 3.77, acc: 0.31\n",
      "INFO:tensorflow:Step: 4300, loss: 3.74, acc: 0.34\n",
      "INFO:tensorflow:Step: 4400, loss: 3.70, acc: 0.33\n",
      "INFO:tensorflow:Step: 4500, loss: 3.76, acc: 0.32\n",
      "INFO:tensorflow:Step: 4600, loss: 3.58, acc: 0.36\n",
      "INFO:tensorflow:Step: 4700, loss: 3.64, acc: 0.30\n",
      "INFO:tensorflow:Step: 4800, loss: 3.67, acc: 0.35\n",
      "INFO:tensorflow:Step: 4900, loss: 3.53, acc: 0.33\n",
      "INFO:tensorflow:Step: 5000, loss: 3.46, acc: 0.34\n",
      "INFO:tensorflow:Step: 5000, model saved \n",
      "INFO:tensorflow:Step: 5100, loss: 3.73, acc: 0.35\n",
      "INFO:tensorflow:Step: 5200, loss: 3.62, acc: 0.33\n",
      "INFO:tensorflow:Step: 5300, loss: 3.68, acc: 0.32\n",
      "INFO:tensorflow:Step: 5400, loss: 3.70, acc: 0.33\n",
      "INFO:tensorflow:Step: 5500, loss: 3.54, acc: 0.37\n",
      "INFO:tensorflow:Step: 5600, loss: 3.61, acc: 0.32\n",
      "INFO:tensorflow:Step: 5700, loss: 3.63, acc: 0.34\n",
      "INFO:tensorflow:Step: 5800, loss: 3.56, acc: 0.35\n",
      "INFO:tensorflow:Step: 5900, loss: 3.50, acc: 0.36\n",
      "INFO:tensorflow:Step: 6000, loss: 3.38, acc: 0.36\n",
      "INFO:tensorflow:Step: 6000, model saved \n",
      "INFO:tensorflow:Step: 6100, loss: 3.40, acc: 0.37\n",
      "INFO:tensorflow:Step: 6200, loss: 3.81, acc: 0.32\n",
      "INFO:tensorflow:Step: 6300, loss: 3.50, acc: 0.34\n",
      "INFO:tensorflow:Step: 6400, loss: 3.52, acc: 0.33\n",
      "INFO:tensorflow:Step: 6500, loss: 3.50, acc: 0.34\n",
      "INFO:tensorflow:Step: 6600, loss: 3.35, acc: 0.37\n",
      "INFO:tensorflow:Step: 6700, loss: 3.68, acc: 0.34\n",
      "INFO:tensorflow:Step: 6800, loss: 3.43, acc: 0.36\n",
      "INFO:tensorflow:Step: 6900, loss: 3.41, acc: 0.35\n",
      "INFO:tensorflow:Step: 7000, loss: 3.61, acc: 0.31\n",
      "INFO:tensorflow:Step: 7000, model saved \n",
      "INFO:tensorflow:Step: 7100, loss: 3.34, acc: 0.37\n",
      "INFO:tensorflow:Step: 7200, loss: 3.37, acc: 0.38\n",
      "INFO:tensorflow:Step: 7300, loss: 3.55, acc: 0.33\n",
      "INFO:tensorflow:Step: 7400, loss: 3.61, acc: 0.35\n",
      "INFO:tensorflow:Step: 7500, loss: 3.68, acc: 0.33\n",
      "INFO:tensorflow:Step: 7600, loss: 3.60, acc: 0.33\n",
      "INFO:tensorflow:Step: 7700, loss: 3.20, acc: 0.36\n",
      "INFO:tensorflow:Step: 7800, loss: 3.45, acc: 0.36\n",
      "INFO:tensorflow:Step: 7900, loss: 3.61, acc: 0.34\n",
      "INFO:tensorflow:Step: 8000, loss: 3.50, acc: 0.36\n",
      "INFO:tensorflow:Step: 8000, model saved \n",
      "INFO:tensorflow:Step: 8100, loss: 3.53, acc: 0.36\n",
      "INFO:tensorflow:Step: 8200, loss: 3.33, acc: 0.34\n",
      "INFO:tensorflow:Step: 8300, loss: 3.58, acc: 0.36\n",
      "INFO:tensorflow:Step: 8400, loss: 3.48, acc: 0.35\n",
      "INFO:tensorflow:Step: 8500, loss: 3.28, acc: 0.38\n",
      "INFO:tensorflow:Step: 8600, loss: 3.40, acc: 0.33\n",
      "INFO:tensorflow:Step: 8700, loss: 3.73, acc: 0.29\n",
      "INFO:tensorflow:Step: 8800, loss: 3.31, acc: 0.36\n",
      "INFO:tensorflow:Step: 8900, loss: 3.19, acc: 0.39\n",
      "INFO:tensorflow:Step: 9000, loss: 3.49, acc: 0.37\n",
      "INFO:tensorflow:Step: 9000, model saved \n",
      "INFO:tensorflow:Step: 9100, loss: 3.70, acc: 0.31\n",
      "INFO:tensorflow:Step: 9200, loss: 3.50, acc: 0.33\n",
      "INFO:tensorflow:Step: 9300, loss: 3.28, acc: 0.38\n",
      "INFO:tensorflow:Step: 9400, loss: 3.47, acc: 0.35\n",
      "INFO:tensorflow:Step: 9500, loss: 3.32, acc: 0.37\n",
      "INFO:tensorflow:Step: 9600, loss: 3.31, acc: 0.38\n",
      "INFO:tensorflow:Step: 9700, loss: 3.47, acc: 0.34\n",
      "INFO:tensorflow:Step: 9800, loss: 3.11, acc: 0.39\n",
      "INFO:tensorflow:Step: 9900, loss: 3.33, acc: 0.32\n",
      "INFO:tensorflow:Step: 10000, loss: 3.37, acc: 0.36\n",
      "INFO:tensorflow:Step: 10000, model saved \n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "training_steps = 10000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    writer = tf.summary.FileWriter(output_dir, sess.graph)\n",
    "    for i in range(training_steps):\n",
    "        (batch_img_features,\n",
    "         batch_sentence_ids,\n",
    "         batch_weights, _) = caption_data.next_batch(hps.batch_size)\n",
    "        input_vals = (batch_img_features,\n",
    "                      batch_sentence_ids,\n",
    "                      batch_weights,\n",
    "                      hps.keep_prob)\n",
    "        feed_dict = dict(zip(placehoders, input_vals))\n",
    "        fetches = [global_step, loss, accuracy, train_op]\n",
    "        should_log = (i+1) % hps.log_frequent == 0\n",
    "        should_save = (i+1) % hps.save_frequent == 0\n",
    "        \n",
    "        if should_log:\n",
    "            fetches += [summary_op]\n",
    "            \n",
    "        outputs = sess.run(fetches, feed_dict=feed_dict)\n",
    "        global_step_val, loss_val, accuracy_val = outputs[0:3]\n",
    "        if should_log:\n",
    "            summary_str = outputs[-1]\n",
    "            writer.add_summary(summary_str, global_step_val)\n",
    "            logging.info(\"Step: {}, loss: {:.2f}, acc: {:.2f}\".format(global_step_val, loss_val, accuracy_val))\n",
    "            \n",
    "        if should_save:\n",
    "            model_save_file = os.path.join(output_dir, \"image_caption\")\n",
    "            logging.info(\"Step: {}, model saved \".format(global_step_val))\n",
    "            saver.save(sess, model_save_file, global_step=global_step_val)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}